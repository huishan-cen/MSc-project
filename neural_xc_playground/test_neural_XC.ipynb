{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test_neural_XC.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP59pQxM98v5Zu0SPis4BaD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y7cOpXxeUDR1","executionInfo":{"status":"ok","timestamp":1623971463766,"user_tz":-60,"elapsed":15849,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"8f85c1bf-66f1-415b-e349-9e167199df3f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MtsplnKWUZQc","executionInfo":{"status":"ok","timestamp":1623971463766,"user_tz":-60,"elapsed":7,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"a2b59280-fb86-4842-a25a-c3bcf5bb2f4f"},"source":["cd drive/MyDrive/MSc_Project/neural_xc_playground/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/MSc_Project/neural_xc_playground\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xJcJA0zAUp3R","executionInfo":{"status":"ok","timestamp":1623503125521,"user_tz":-60,"elapsed":5217,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"9552e54f-8852-4752-ca73-a37dfe7e3ce0"},"source":["!git clone https://github.com/semodi/neuralxc.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'neuralxc'...\n","remote: Enumerating objects: 3184, done.\u001b[K\n","remote: Counting objects: 100% (1376/1376), done.\u001b[K\n","remote: Compressing objects: 100% (597/597), done.\u001b[K\n","remote: Total 3184 (delta 898), reused 1168 (delta 746), pack-reused 1808\u001b[K\n","Receiving objects: 100% (3184/3184), 19.21 MiB | 10.65 MiB/s, done.\n","Resolving deltas: 100% (1942/1942), done.\n","Checking out files: 100% (248/248), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lrvfoz65Ut5H","executionInfo":{"status":"ok","timestamp":1623971473540,"user_tz":-60,"elapsed":665,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"cee13b95-ef02-4288-bf6e-2acf5f148cca"},"source":["cd neuralxc/"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NadurKfuU_Tk","executionInfo":{"status":"ok","timestamp":1623971677426,"user_tz":-60,"elapsed":200875,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"118d071a-c93a-4171-d830-f6ae08240183"},"source":["!sh install.sh"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/semodi/libnxc.git@9c3b895840a000968866a44841291926fe29aab7 (from -r requirements.txt (line 23))\n","  Cloning https://github.com/semodi/libnxc.git (to revision 9c3b895840a000968866a44841291926fe29aab7) to /tmp/pip-req-build-wpxp7pq1\n","  Running command git clone -q https://github.com/semodi/libnxc.git /tmp/pip-req-build-wpxp7pq1\n","  Running command git checkout -q 9c3b895840a000968866a44841291926fe29aab7\n","Collecting ase==3.17\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/af/ea30928def88748d76946c0af0e6a8f9c2d2e65e3d65da527fe80e9ba06e/ase-3.17.0-py3-none-any.whl (1.8MB)\n","\u001b[K     |████████████████████████████████| 1.8MB 15.7MB/s \n","\u001b[?25hRequirement already satisfied: dask in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2.12.0)\n","Collecting h5py==2.9.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/fd/2ca5c4f4ed33ac4178f9c4d551e3946ab480866e3cd67a65a67a4bb35367/h5py-2.9.0-cp37-cp37m-manylinux1_x86_64.whl (2.8MB)\n","\u001b[K     |████████████████████████████████| 2.8MB 44.6MB/s \n","\u001b[?25hCollecting ipyparallel\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/e9/03a9189eb39276396309faf28bf833b4328befe4513bbf375b811a36a076/ipyparallel-6.3.0-py3-none-any.whl (199kB)\n","\u001b[K     |████████████████████████████████| 204kB 53.2MB/s \n","\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (5.5.0)\n","Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (1.0.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.19.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (1.1.5)\n","Collecting periodictable\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/47/bc639be580ffa41cb859a409c71da2c7ccaf196f8b7c8aa7a0473ba84b9e/periodictable-1.6.0.tar.gz (686kB)\n","\u001b[K     |████████████████████████████████| 696kB 45.8MB/s \n","\u001b[?25hCollecting scikit-learn==0.20.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/cc/a84e1748a2a70d0f3e081f56cefc634f3b57013b16faa6926d3a6f0598df/scikit_learn-0.20.3-cp37-cp37m-manylinux1_x86_64.whl (5.4MB)\n","\u001b[K     |████████████████████████████████| 5.4MB 47.6MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (1.4.1)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (3.6.4)\n","Collecting pytest-cov\n","  Downloading https://files.pythonhosted.org/packages/ba/84/576b071aef9ac9301e5c0ff35d117e12db50b87da6f12e745e9c5f745cc2/pytest_cov-2.12.1-py2.py3-none-any.whl\n","Collecting dill==0.3.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/96/518a8ea959a734b70d2e95fef98bcbfdc7adad1c1e5f5dd9148c835205a5/dill-0.3.2.zip (177kB)\n","\u001b[K     |████████████████████████████████| 184kB 50.0MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 18)) (1.8.1+cu101)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 19)) (3.3.0)\n","Collecting codecov\n","  Downloading https://files.pythonhosted.org/packages/93/9f/bbea5b6231308458963cb5c067bc5643da9949689702fa5a382714b59699/codecov-2.1.11-py2.py3-none-any.whl\n","Collecting pyscf\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/d6/af4ce5035977cb011e4dbe9979bf254129a36d48cb569b86e57b5a72c5b1/pyscf-1.7.6.post1-cp37-cp37m-manylinux1_x86_64.whl (29.7MB)\n","\u001b[K     |████████████████████████████████| 29.7MB 169kB/s \n","\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 22)) (0.8.9)\n","Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from ase==3.17->-r requirements.txt (line 1)) (1.1.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.9.0->-r requirements.txt (line 5)) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (2.8.1)\n","Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (5.1.1)\n","Requirement already satisfied: traitlets>=4.3 in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (5.0.5)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (22.1.0)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (5.3.5)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (0.2.0)\n","Requirement already satisfied: ipykernel>=4.4 in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (4.10.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (4.4.2)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 7)) (1.0.18)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 7)) (0.8.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 7)) (0.7.5)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 7)) (57.0.0)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 7)) (4.8.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 7)) (2.6.1)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 8)) (5.3.1)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 8)) (5.1.0)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 8)) (5.2.0)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 8)) (5.6.1)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 8)) (7.6.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 9)) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 9)) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 9)) (1.3.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 11)) (2018.9)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 15)) (8.8.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 15)) (0.7.1)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 15)) (21.2.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 15)) (1.4.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 15)) (1.10.0)\n","Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from pytest-cov->-r requirements.txt (line 16)) (0.10.2)\n","Collecting coverage>=5.2.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/e0/fc9f7bd9b84e6b41d0aad1a113e36714aac0c0a9b307aca5f9af443bc50f/coverage-5.5-cp37-cp37m-manylinux2010_x86_64.whl (242kB)\n","\u001b[K     |████████████████████████████████| 245kB 52.2MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->-r requirements.txt (line 18)) (3.7.4.3)\n","Requirement already satisfied: requests>=2.7.9 in /usr/local/lib/python3.7/dist-packages (from codecov->-r requirements.txt (line 20)) (2.23.0)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->ase==3.17->-r requirements.txt (line 1)) (7.1.2)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->ase==3.17->-r requirements.txt (line 1)) (1.1.0)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->ase==3.17->-r requirements.txt (line 1)) (2.11.3)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->ase==3.17->-r requirements.txt (line 1)) (1.0.1)\n","Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipyparallel->-r requirements.txt (line 6)) (4.7.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->-r requirements.txt (line 7)) (0.2.5)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->-r requirements.txt (line 7)) (0.7.0)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->-r requirements.txt (line 8)) (5.1.3)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->-r requirements.txt (line 8)) (0.10.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->-r requirements.txt (line 8)) (1.5.0)\n","Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->-r requirements.txt (line 8)) (1.9.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (0.8.4)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (0.5.0)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (0.3)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (1.4.3)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (0.7.1)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (3.3.0)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 8)) (3.5.1)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 8)) (1.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov->-r requirements.txt (line 20)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov->-r requirements.txt (line 20)) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov->-r requirements.txt (line 20)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov->-r requirements.txt (line 20)) (1.24.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->ase==3.17->-r requirements.txt (line 1)) (2.0.1)\n","Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->-r requirements.txt (line 8)) (2.6.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->-r requirements.txt (line 8)) (0.5.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->-r requirements.txt (line 8)) (20.9)\n","Building wheels for collected packages: periodictable, dill, pylibnxc\n","  Building wheel for periodictable (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for periodictable: filename=periodictable-1.6.0-cp37-none-any.whl size=749750 sha256=6007ee82de5597c7603b8d51845188e3c4944715aefb60e22572bebffd63c62c\n","  Stored in directory: /root/.cache/pip/wheels/eb/78/08/4cb95d4ae156e978980596c1f25bb8365d884de1725ef9a306\n","  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for dill: filename=dill-0.3.2-cp37-none-any.whl size=78927 sha256=09202e306d66fcb8d7e0e383f5b1f1dc79994353d836391761e9d974ff0a4f4c\n","  Stored in directory: /root/.cache/pip/wheels/27/4b/a2/34ccdcc2f158742cfe9650675560dea85f78c3f4628f7daad0\n","  Building wheel for pylibnxc (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pylibnxc: filename=pylibnxc-0.1-cp37-none-any.whl size=13977 sha256=525e7e9dbc6b4d3d3f8dc8e9e45d2ad732cf5d4baa49c2e2819529c3bf25d60f\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-eg6fg04f/wheels/b2/6c/b4/59f56fd21ac3b81416fd97d1d8c7016582d9219d7a51104f9c\n","Successfully built periodictable dill pylibnxc\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement h5py~=3.1.0, but you'll have h5py 2.9.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: multiprocess 0.70.11.1 has requirement dill>=0.3.3, but you'll have dill 0.3.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement coverage==3.7.1, but you'll have coverage 5.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: coveralls 0.5 has requirement coverage<3.999,>=3.6, but you'll have coverage 5.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: pytest-cov 2.12.1 has requirement pytest>=4.6, but you'll have pytest 3.6.4 which is incompatible.\u001b[0m\n","Installing collected packages: ase, h5py, ipyparallel, periodictable, scikit-learn, coverage, pytest-cov, dill, codecov, pyscf, pylibnxc\n","  Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","  Found existing installation: coverage 3.7.1\n","    Uninstalling coverage-3.7.1:\n","      Successfully uninstalled coverage-3.7.1\n","  Found existing installation: dill 0.3.3\n","    Uninstalling dill-0.3.3:\n","      Successfully uninstalled dill-0.3.3\n","Successfully installed ase-3.17.0 codecov-2.1.11 coverage-5.5 dill-0.3.2 h5py-2.9.0 ipyparallel-6.3.0 periodictable-1.6.0 pylibnxc-0.1 pyscf-1.7.6.post1 pytest-cov-2.12.1 scikit-learn-0.20.3\n","Obtaining file:///content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc\n","Installing collected packages: neuralxc\n","  Running setup.py develop for neuralxc\n","Successfully installed neuralxc\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fjD2aGSKVFzr","executionInfo":{"status":"ok","timestamp":1623971678142,"user_tz":-60,"elapsed":9,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"624a1903-5b22-4270-e8ef-a2049247c10f"},"source":["cd examples/example_scripts/train_model"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"thIcZYmViP1C"},"source":["### BENZENE NXC"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PGjfgzVHVS8S","executionInfo":{"status":"ok","timestamp":1623504684140,"user_tz":-60,"elapsed":705315,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"d492d87c-4c80-47ef-fde6-fa291aad2142"},"source":["!sh train_benzene.sh"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cp: -r not specified; omitting directory '../sc'\n","cp: -r not specified; omitting directory '../workdir'\n","--2021-06-12 13:11:38--  http://quantum-machine.org/gdml/data/xyz/benzene_ccsd_t.zip\n","Resolving quantum-machine.org (quantum-machine.org)... 130.149.80.145\n","Connecting to quantum-machine.org (quantum-machine.org)|130.149.80.145|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 829159 (810K) [application/zip]\n","Saving to: ‘benzene_ccsd_t.zip’\n","\n","\rbenzene_ccsd_t.zip    0%[                    ]       0  --.-KB/s               \rbenzene_ccsd_t.zip  100%[===================>] 809.73K  --.-KB/s    in 0.07s   \n","\n","2021-06-12 13:11:39 (10.9 MB/s) - ‘benzene_ccsd_t.zip’ saved [829159/829159]\n","\n","Archive:  benzene_ccsd_t.zip\n","  inflating: benzene_ccsd_t-train.xyz  \n","  inflating: benzene_ccsd_t-test.xyz  \n","using unit 0.04336410390059322\n","using unit 0.04336410390059322\n","FILEPATH basis_sgdml_benzene.json\n","Traceback (most recent call last):\n","  File \"../../../scripts/fix_paths.py\", line 13, in <module>\n","    basis['engine_kwargs']['pseudoloc'] = os.path.abspath(basis['engine_kwargs']['pseudoloc'])\n","KeyError: 'pseudoloc'\n","====== Iteration 0 ======\n","converged SCF energy = -231.939704169113\n","converged SCF energy = -231.942599964451\n","converged SCF energy = -231.939874403764\n","converged SCF energy = -231.940169689061\n","converged SCF energy = -231.940661146278\n","converged SCF energy = -231.933947761728\n","converged SCF energy = -231.940447115709\n","converged SCF energy = -231.939636463084\n","converged SCF energy = -231.934608080115\n","converged SCF energy = -231.935355504841\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","10 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","10 systems found, adding energy\n","10 systems found, adding energy\n","{'mae': 0.01378, 'max': 0.03117, 'mean deviation': -0.0, 'rmse': 0.01623}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Epoch 0 ||  Training loss : 1.011722  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.596522  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.243608  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.134624  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.143253  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.092429  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.069888  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.062802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.052207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.045253  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.040652  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 110 ||  Training loss : 0.036594  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.033300  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 130 ||  Training loss : 0.030413  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 140 ||  Training loss : 0.027858  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 150 ||  Training loss : 0.025651  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 160 ||  Training loss : 0.023725  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 170 ||  Training loss : 0.022022  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 180 ||  Training loss : 0.020502  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 190 ||  Training loss : 0.019144  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 0.017930  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 210 ||  Training loss : 0.016843  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 220 ||  Training loss : 0.015867  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 230 ||  Training loss : 0.014990  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 240 ||  Training loss : 0.014200  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 250 ||  Training loss : 0.013489  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 260 ||  Training loss : 0.012850  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 270 ||  Training loss : 0.012274  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 280 ||  Training loss : 0.011757  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 290 ||  Training loss : 0.011293  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 0.010878  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 310 ||  Training loss : 0.010507  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 320 ||  Training loss : 0.010176  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 330 ||  Training loss : 0.009882  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 340 ||  Training loss : 0.009621  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 350 ||  Training loss : 0.009392  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 360 ||  Training loss : 0.009190  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 370 ||  Training loss : 0.009014  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 380 ||  Training loss : 0.008861  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 390 ||  Training loss : 0.008729  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 400 ||  Training loss : 0.008616  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 410 ||  Training loss : 0.008521  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 420 ||  Training loss : 0.008440  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 430 ||  Training loss : 0.008374  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 440 ||  Training loss : 0.008320  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 450 ||  Training loss : 0.008277  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 460 ||  Training loss : 0.008244  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 470 ||  Training loss : 0.008220  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 480 ||  Training loss : 0.008204  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 490 ||  Training loss : 0.008194  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 500 ||  Training loss : 0.008191  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 510 ||  Training loss : 0.008193  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 520 ||  Training loss : 0.008200  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 530 ||  Training loss : 0.008212  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 540 ||  Training loss : 0.008227  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 550 ||  Training loss : 0.008245  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 560 ||  Training loss : 0.008266  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 570 ||  Training loss : 0.008290  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 580 ||  Training loss : 0.008315  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 590 ||  Training loss : 0.008343  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 600 ||  Training loss : 0.008373  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    62: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 610 ||  Training loss : 0.008404  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 620 ||  Training loss : 0.008410  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 630 ||  Training loss : 0.008413  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 640 ||  Training loss : 0.008416  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 650 ||  Training loss : 0.008419  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 660 ||  Training loss : 0.008423  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 670 ||  Training loss : 0.008426  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 680 ||  Training loss : 0.008429  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 690 ||  Training loss : 0.008433  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 700 ||  Training loss : 0.008436  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 710 ||  Training loss : 0.008439  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    73: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 720 ||  Training loss : 0.008443  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 730 ||  Training loss : 0.008444  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 740 ||  Training loss : 0.008444  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 750 ||  Training loss : 0.008444  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 760 ||  Training loss : 0.008445  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 770 ||  Training loss : 0.008445  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 780 ||  Training loss : 0.008445  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 790 ||  Training loss : 0.008446  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 800 ||  Training loss : 0.008446  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 810 ||  Training loss : 0.008447  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 820 ||  Training loss : 0.008447  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    84: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 830 ||  Training loss : 0.008447  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 840 ||  Training loss : 0.008447  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 850 ||  Training loss : 0.008447  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 860 ||  Training loss : 0.008447  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 870 ||  Training loss : 0.008447  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 880 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 890 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 900 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 910 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 920 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 930 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    95: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 940 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 950 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.008448  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 1 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","Overwritten attributes  get_veff  of <class 'pyscf.dft.rks.RKS'>\n","converged SCF energy = -231.94006027206\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.943081213457\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.940261143618\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.94054258753\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.940785574628\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.933390159837\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.940279575509\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.940040590935\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.934439255081\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.934938574187\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","10 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","10 systems found, adding energy\n","{'mae': 0.00702, 'max': 0.01378, 'mean deviation': -0.0, 'rmse': 0.00829}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.00829148818299127\n","Dataset 0 new STD: 0.01642474746397942\n","1\n","Epoch 0 ||  Training loss : 0.008291  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.008715  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.009518  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.008636  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.009044  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.009192  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.009450  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.009735  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.010043  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.010366  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.010710  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.011061  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.011129  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.011167  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.011205  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.011244  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.011283  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.011322  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.011362  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.011401  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.011442  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.011482  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.011523  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.011531  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.011535  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.011539  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.011543  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.011547  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.011552  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.011556  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.011560  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.011565  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.011569  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.011573  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.011574  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.011575  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.011575  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.011576  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.011576  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.011577  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.011577  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.011577  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.011578  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.011578  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.011579  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.011579  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.011579  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.011579  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.011579  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.011579  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.011579  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.011579  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.011579  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.011579  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.011579  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.011579  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.011579  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.011579  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.011579  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.011580  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.011580  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.011580  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.011580  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.011580  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.011580  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.011580  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.011580  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.011580  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.011580  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.011580  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.011580  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.011580  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.011580  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.011580  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.011580  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.011580  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.011580  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.011580  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.011580  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.011581  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.011581  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.011581  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.011581  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.011581  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.011581  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.011581  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.011581  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.011581  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.011581  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.011581  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.011581  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.011581  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.011581  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.011581  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.011581  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.011581  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.011581  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.011581  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.011581  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.011582  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.011582  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.011582  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.011582  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.011582  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.011582  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.011582  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.011582  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.011582  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.011582  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.011582  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.011582  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.011582  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.011582  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.011582  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.011582  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.011582  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.011582  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.011582  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.011583  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.011583  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.011583  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.011583  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.011583  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.011583  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.011583  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.011583  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.011583  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.011583  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.011583  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.011583  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.011583  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.011583  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.011583  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.011583  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.011583  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.011583  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.011583  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.011584  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.011584  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.011584  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.011584  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.011584  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.011584  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.011584  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.011584  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.011584  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.011584  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.011584  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.011584  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.011584  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.011584  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.011584  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.011584  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.011584  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.011584  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.011585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.011585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.011585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.011585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.011585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.011585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.011585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.011585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.011585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.011585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.011585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.011585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.011585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.011585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.011585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.011585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.011585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.011585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.011586  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.011586  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.011586  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.011586  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.011586  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.011586  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.011586  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.011586  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.011586  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.011586  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.011586  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.011586  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.011586  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.011586  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.011586  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.011586  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.011586  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.011586  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.011587  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.011587  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.011587  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.011587  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.011587  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.011587  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.011587  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.011587  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.011587  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.011587  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 2 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.939915525399\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.942990366314\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.93998661425\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.940471597148\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.940857505862\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.933873051145\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.940610776871\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.940102366234\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.934420428603\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.935447603752\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","10 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","10 systems found, adding energy\n","{'mae': 0.01011, 'max': 0.02458, 'mean deviation': -0.0, 'rmse': 0.01178}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.011782080507695756\n","Dataset 0 new STD: 0.01663132319984871\n","1\n","Epoch 0 ||  Training loss : 0.011782  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.011993  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.013020  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.012800  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.013189  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.013574  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.014032  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.014454  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.014870  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015246  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015572  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015848  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015894  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015942  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015967  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015991  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.016014  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.016038  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.016061  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.016084  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.016106  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.016128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.016132  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.016134  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.016136  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.016138  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.016141  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.016143  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.016145  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.016147  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.016150  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.016152  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.016154  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.016155  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.016155  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.016155  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.016156  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.016156  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.016156  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.016156  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.016157  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.016157  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.016157  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.016157  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.016157  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.016157  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.016157  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.016157  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.016157  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.016157  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.016157  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.016157  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.016158  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.016159  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.016160  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.016161  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.016162  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.016163  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.016163  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.016163  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.016163  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.016163  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.016163  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.016163  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 3 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.94000688491\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.942916338373\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.940165359241\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.940478383833\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.940963983615\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.934234302591\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.94074867941\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.939963728826\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.934878326865\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.935650720992\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","10 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","10 systems found, adding energy\n","{'mae': 0.01352, 'max': 0.03079, 'mean deviation': 0.0, 'rmse': 0.01589}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015893379080595526\n","Dataset 0 new STD: 0.016240879893776747\n","1\n","Epoch 0 ||  Training loss : 0.015893  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.016094  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.016455  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.016195  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.016229  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.016230  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.016237  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.016237  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.016242  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.016240  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.016242  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.016242  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.016242  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.016242  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.016241  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 4 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.94000933097\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.942905128087\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.940179564447\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.940474851839\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.940966308144\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.934252920913\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.940752277402\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.939941627762\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.934913237658\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.935660665706\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","10 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","10 systems found, adding energy\n","{'mae': 0.01378, 'max': 0.03117, 'mean deviation': -0.0, 'rmse': 0.01623}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.016230778147898674\n","Dataset 0 new STD: 0.016230824719577444\n","1\n","Epoch 0 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.016267  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.016394  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.016233  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.016237  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.016243  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.016232  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 5 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.94001036443\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.94290615977\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.940180599081\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.940475884378\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.940967341596\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.934253957046\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.940753311026\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.939942658402\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.934914275433\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.93566170016\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","10 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","10 systems found, adding energy\n","{'mae': 0.01378, 'max': 0.03117, 'mean deviation': -0.0, 'rmse': 0.01623}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.016230824722830932\n","Dataset 0 new STD: 0.016230824722835546\n","1\n","Epoch 0 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.016358  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.016279  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.016233  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.016235  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.016234  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.016231  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Testing ======\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.942230754262\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.933328419638\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.93857505532\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.940254587232\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.937539820184\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.935263036255\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.939648147598\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.936396307508\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.942020654827\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -231.933943731427\n","10 systems found, adding energy\n","10 systems found, adding energy\n","{'mae': 0.01311, 'max': 0.02714, 'mean deviation': -0.0, 'rmse': 0.01612}\n","/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OCS8-W59P_rn","executionInfo":{"status":"ok","timestamp":1623971838222,"user_tz":-60,"elapsed":160083,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"487fddbd-e2d5-483c-95c5-39c1906184ec"},"source":["#benzene PBE\n","!neuralxc engine basis_sgdml_benzene.json workdir/testing.traj --workdir ./tmp"],"execution_count":6,"outputs":[{"output_type":"stream","text":["converged SCF energy = -231.941924558944\n","converged SCF energy = -231.93302222432\n","converged SCF energy = -231.938268860002\n","converged SCF energy = -231.939948391914\n","converged SCF energy = -231.937233624867\n","converged SCF energy = -231.934956840936\n","converged SCF energy = -231.939341952281\n","converged SCF energy = -231.936090112191\n","converged SCF energy = -231.941714459509\n","converged SCF energy = -231.933637536111\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tSVJq13kVqR1","executionInfo":{"status":"ok","timestamp":1623504763392,"user_tz":-60,"elapsed":422,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"8842f46c-8687-4a61-d45a-e7bad3553df7"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["basis_sgdml_benzene.json  hyperparameters.json  test_subset.csv\n","\u001b[0m\u001b[01;34mfinal_model\u001b[0m/              README.md             train_benzene.sh\n","\u001b[01;34mfinal_model.jit\u001b[0m/          \u001b[01;34msc\u001b[0m/                   \u001b[01;34mworkdir\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A-8c8hE_bLZq","executionInfo":{"status":"ok","timestamp":1623511160435,"user_tz":-60,"elapsed":386,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"1d282496-de93-4b3e-a4ae-f380109ffe4b"},"source":["cd .."],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HZShVodRxpkg","executionInfo":{"status":"ok","timestamp":1623511162852,"user_tz":-60,"elapsed":475,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"e53686b5-96f0-4f1d-aded-f7cbd5381aa2"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mtrain_model\u001b[0m/  \u001b[01;34mtrain_model_50_20\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8e5u0BWTzlub","executionInfo":{"status":"ok","timestamp":1623511173380,"user_tz":-60,"elapsed":308,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"3e8a7ddc-e9b1-498d-d178-8c8b16b7c32e"},"source":["cd train_model_50_20"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5khAYIXp2wA5"},"source":["ls workdir/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6d1RfKVanzO_"},"source":["## Model train on 50 ethane"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s3HYn1DKzoZZ","outputId":"87d16c14-ad1b-4faf-98ce-dab81b461e02"},"source":["!sh train_50_20.sh"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cp: -r not specified; omitting directory '../workdir'\n","using unit 0.04336410390059322\n","using unit 0.04336410390059322\n","FILEPATH basis_sgdml_benzene.json\n","Traceback (most recent call last):\n","  File \"../../../scripts/fix_paths.py\", line 13, in <module>\n","    basis['engine_kwargs']['pseudoloc'] = os.path.abspath(basis['engine_kwargs']['pseudoloc'])\n","KeyError: 'pseudoloc'\n","====== Iteration 0 ======\n","converged SCF energy = -79.684427113883\n","converged SCF energy = -79.6943500665332\n","converged SCF energy = -79.686889845372\n","converged SCF energy = -79.6922359554811\n","converged SCF energy = -79.6865586039518\n","converged SCF energy = -79.6913095653313\n","converged SCF energy = -79.6937771392767\n","converged SCF energy = -79.688366575151\n","converged SCF energy = -79.690813165124\n","converged SCF energy = -79.6928989718342\n","converged SCF energy = -79.689887874575\n","converged SCF energy = -79.6935007220326\n","converged SCF energy = -79.6914501649868\n","converged SCF energy = -79.6881811353525\n","converged SCF energy = -79.6851843297106\n","converged SCF energy = -79.6846655122535\n","converged SCF energy = -79.69204076459\n","converged SCF energy = -79.6914734714861\n","converged SCF energy = -79.6897241902575\n","converged SCF energy = -79.6882188962683\n","converged SCF energy = -79.6921738455578\n","converged SCF energy = -79.6855847560229\n","converged SCF energy = -79.6875161777398\n","converged SCF energy = -79.6920402683762\n","converged SCF energy = -79.6871181611663\n","converged SCF energy = -79.6830808966932\n","converged SCF energy = -79.679790967371\n","converged SCF energy = -79.6888245411405\n","converged SCF energy = -79.6939602555434\n","converged SCF energy = -79.6924510308239\n","converged SCF energy = -79.6932460467216\n","converged SCF energy = -79.6833029688499\n","converged SCF energy = -79.685599311649\n","converged SCF energy = -79.691462543248\n","converged SCF energy = -79.6944041042538\n","converged SCF energy = -79.6925350681459\n","converged SCF energy = -79.6921868033791\n","converged SCF energy = -79.6889093916453\n","converged SCF energy = -79.6902240755123\n","converged SCF energy = -79.6924157415378\n","converged SCF energy = -79.6951037013669\n","converged SCF energy = -79.6864766988548\n","converged SCF energy = -79.6952561636311\n","converged SCF energy = -79.6870497694797\n","converged SCF energy = -79.6834160934016\n","converged SCF energy = -79.6887380420964\n","converged SCF energy = -79.6848380417077\n","converged SCF energy = -79.692599814125\n","converged SCF energy = -79.6900918264509\n","converged SCF energy = -79.6909261579373\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","50 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","50 systems found, adding energy\n","50 systems found, adding energy\n","{'mae': 0.0337, 'max': 0.0934, 'mean deviation': 0.0, 'rmse': 0.04365}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Epoch 0 ||  Training loss : 2.244588  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 1.935760  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 1.634570  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 1.340187  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 1.053177  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.778608  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.530762  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.340000  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.244885  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.224563  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.214891  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 110 ||  Training loss : 0.200980  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.188686  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 130 ||  Training loss : 0.178882  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 140 ||  Training loss : 0.170064  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 150 ||  Training loss : 0.161720  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 160 ||  Training loss : 0.153899  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 170 ||  Training loss : 0.146585  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 180 ||  Training loss : 0.139708  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 190 ||  Training loss : 0.133231  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 0.127142  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 210 ||  Training loss : 0.121430  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 220 ||  Training loss : 0.116082  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 230 ||  Training loss : 0.111085  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 240 ||  Training loss : 0.106423  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 250 ||  Training loss : 0.102077  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 260 ||  Training loss : 0.098027  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 270 ||  Training loss : 0.094257  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 280 ||  Training loss : 0.090747  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 290 ||  Training loss : 0.087481  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 0.084442  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 310 ||  Training loss : 0.081614  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 320 ||  Training loss : 0.078983  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 330 ||  Training loss : 0.076532  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 340 ||  Training loss : 0.074249  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 350 ||  Training loss : 0.072119  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 360 ||  Training loss : 0.070131  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 370 ||  Training loss : 0.068271  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 380 ||  Training loss : 0.066530  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 390 ||  Training loss : 0.064897  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 400 ||  Training loss : 0.063362  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 410 ||  Training loss : 0.061917  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 420 ||  Training loss : 0.060553  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 430 ||  Training loss : 0.059263  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 440 ||  Training loss : 0.058040  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 450 ||  Training loss : 0.056879  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 460 ||  Training loss : 0.055773  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 470 ||  Training loss : 0.054717  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 480 ||  Training loss : 0.053708  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 490 ||  Training loss : 0.052741  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 500 ||  Training loss : 0.051812  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 510 ||  Training loss : 0.050919  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 520 ||  Training loss : 0.050058  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 530 ||  Training loss : 0.049226  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 540 ||  Training loss : 0.048422  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 550 ||  Training loss : 0.047643  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 560 ||  Training loss : 0.046888  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 570 ||  Training loss : 0.046154  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 580 ||  Training loss : 0.045441  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 590 ||  Training loss : 0.044746  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 600 ||  Training loss : 0.044069  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 610 ||  Training loss : 0.043409  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 620 ||  Training loss : 0.042765  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 630 ||  Training loss : 0.042135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 640 ||  Training loss : 0.041519  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 650 ||  Training loss : 0.040917  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 660 ||  Training loss : 0.040328  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 670 ||  Training loss : 0.039751  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 680 ||  Training loss : 0.039186  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 690 ||  Training loss : 0.038633  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 700 ||  Training loss : 0.038090  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 710 ||  Training loss : 0.037558  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 720 ||  Training loss : 0.037037  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 730 ||  Training loss : 0.036525  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 740 ||  Training loss : 0.036023  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 750 ||  Training loss : 0.035531  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 760 ||  Training loss : 0.035049  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 770 ||  Training loss : 0.034575  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 780 ||  Training loss : 0.034111  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 790 ||  Training loss : 0.033656  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 800 ||  Training loss : 0.033209  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 810 ||  Training loss : 0.032771  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 820 ||  Training loss : 0.032341  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 830 ||  Training loss : 0.031920  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 840 ||  Training loss : 0.031507  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 850 ||  Training loss : 0.031102  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 860 ||  Training loss : 0.030705  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 870 ||  Training loss : 0.030316  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 880 ||  Training loss : 0.029934  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 890 ||  Training loss : 0.029560  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 900 ||  Training loss : 0.029193  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 910 ||  Training loss : 0.028834  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 920 ||  Training loss : 0.028482  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 930 ||  Training loss : 0.028136  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 940 ||  Training loss : 0.027798  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 950 ||  Training loss : 0.027466  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 960 ||  Training loss : 0.027141  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 970 ||  Training loss : 0.026823  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 980 ||  Training loss : 0.026511  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 990 ||  Training loss : 0.026204  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.025904  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1010 ||  Training loss : 0.025610  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1020 ||  Training loss : 0.025322  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1030 ||  Training loss : 0.025039  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1040 ||  Training loss : 0.024761  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1050 ||  Training loss : 0.024489  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1060 ||  Training loss : 0.024223  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1070 ||  Training loss : 0.023961  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1080 ||  Training loss : 0.023704  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1090 ||  Training loss : 0.023452  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1100 ||  Training loss : 0.023205  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1110 ||  Training loss : 0.022963  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1120 ||  Training loss : 0.022724  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1130 ||  Training loss : 0.022491  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1140 ||  Training loss : 0.022261  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1150 ||  Training loss : 0.022036  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1160 ||  Training loss : 0.021815  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1170 ||  Training loss : 0.021598  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1180 ||  Training loss : 0.021384  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1190 ||  Training loss : 0.021175  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1200 ||  Training loss : 0.020969  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1210 ||  Training loss : 0.020767  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1220 ||  Training loss : 0.020568  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1230 ||  Training loss : 0.020373  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1240 ||  Training loss : 0.020181  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1250 ||  Training loss : 0.019993  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1260 ||  Training loss : 0.019807  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1270 ||  Training loss : 0.019625  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1280 ||  Training loss : 0.019446  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1290 ||  Training loss : 0.019270  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1300 ||  Training loss : 0.019097  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1310 ||  Training loss : 0.018927  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1320 ||  Training loss : 0.018760  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1330 ||  Training loss : 0.018596  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1340 ||  Training loss : 0.018434  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1350 ||  Training loss : 0.018275  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1360 ||  Training loss : 0.018119  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1370 ||  Training loss : 0.017966  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1380 ||  Training loss : 0.017815  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1390 ||  Training loss : 0.017666  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1400 ||  Training loss : 0.017520  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1410 ||  Training loss : 0.017376  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1420 ||  Training loss : 0.017235  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1430 ||  Training loss : 0.017096  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1440 ||  Training loss : 0.016959  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1450 ||  Training loss : 0.016825  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1460 ||  Training loss : 0.016693  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1470 ||  Training loss : 0.016563  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1480 ||  Training loss : 0.016435  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1490 ||  Training loss : 0.016309  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1500 ||  Training loss : 0.016186  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1510 ||  Training loss : 0.016064  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1520 ||  Training loss : 0.015944  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1530 ||  Training loss : 0.015827  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1540 ||  Training loss : 0.015711  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1550 ||  Training loss : 0.015597  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1560 ||  Training loss : 0.015485  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1570 ||  Training loss : 0.015375  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1580 ||  Training loss : 0.015267  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1590 ||  Training loss : 0.015160  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1600 ||  Training loss : 0.015055  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1610 ||  Training loss : 0.014952  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1620 ||  Training loss : 0.014851  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1630 ||  Training loss : 0.014751  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1640 ||  Training loss : 0.014652  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1650 ||  Training loss : 0.014556  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1660 ||  Training loss : 0.014461  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1670 ||  Training loss : 0.014367  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1680 ||  Training loss : 0.014275  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1690 ||  Training loss : 0.014184  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1700 ||  Training loss : 0.014095  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1710 ||  Training loss : 0.014007  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1720 ||  Training loss : 0.013921  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1730 ||  Training loss : 0.013836  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1740 ||  Training loss : 0.013752  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1750 ||  Training loss : 0.013669  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1760 ||  Training loss : 0.013588  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1770 ||  Training loss : 0.013508  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1780 ||  Training loss : 0.013430  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1790 ||  Training loss : 0.013352  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1800 ||  Training loss : 0.013276  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1810 ||  Training loss : 0.013201  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1820 ||  Training loss : 0.013127  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1830 ||  Training loss : 0.013054  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1840 ||  Training loss : 0.012982  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1850 ||  Training loss : 0.012911  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1860 ||  Training loss : 0.012841  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1870 ||  Training loss : 0.012773  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1880 ||  Training loss : 0.012705  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1890 ||  Training loss : 0.012639  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1900 ||  Training loss : 0.012573  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1910 ||  Training loss : 0.012508  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1920 ||  Training loss : 0.012445  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1930 ||  Training loss : 0.012382  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1940 ||  Training loss : 0.012320  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1950 ||  Training loss : 0.012259  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1960 ||  Training loss : 0.012199  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1970 ||  Training loss : 0.012140  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1980 ||  Training loss : 0.012082  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1990 ||  Training loss : 0.012025  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.011968  Validation loss : 0.000000  Learning rate: 0.001\n","1\n","====== Iteration 1 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","Overwritten attributes  get_veff  of <class 'pyscf.dft.rks.RKS'>\n","converged SCF energy = -79.6846584781438\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6940377227862\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6884632082328\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6911051538944\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6895758789178\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6910568648799\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.695031869707\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.689701753121\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6884936025017\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6929107969829\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6903861643269\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6911287989813\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6922652450308\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6892092470279\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6844377450602\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868360747621\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6903535599091\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6915463714401\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6899968746268\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6879868362663\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6920238980247\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6865380828501\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.688133001103\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6897639975013\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6903710317942\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851837178721\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.683516749886\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896472204598\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6939167638955\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.693042508276\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.693618348316\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6853782869465\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6859763180383\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.690510700075\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6939466418382\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6953383890566\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919860754123\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6890292615941\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6904473808729\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913802893903\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6945035539559\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881263998935\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6952920702101\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6880993929157\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6863959716084\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6911284580186\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6842878800335\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6909761960828\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6933939850753\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.688936412785\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","50 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","50 systems found, adding energy\n","{'mae': 0.00998, 'max': 0.02213, 'mean deviation': -0.0, 'rmse': 0.0118}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.011797354978482291\n","Dataset 0 new STD: 0.04498658103541565\n","1\n","Epoch 0 ||  Training loss : 0.011797  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.010989  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.011278  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.010413  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.010551  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.010708  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.010980  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.011177  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.011391  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.011559  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.011740  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 110 ||  Training loss : 0.011910  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.012064  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 130 ||  Training loss : 0.012221  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    15: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 140 ||  Training loss : 0.012392  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 150 ||  Training loss : 0.012428  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.012449  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.012472  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.012494  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.012516  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.012538  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.012559  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 220 ||  Training loss : 0.012581  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.012603  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 240 ||  Training loss : 0.012625  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    26: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 250 ||  Training loss : 0.012647  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 260 ||  Training loss : 0.012651  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.012653  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.012656  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.012658  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.012660  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.012663  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.012665  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 330 ||  Training loss : 0.012668  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.012670  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 350 ||  Training loss : 0.012673  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 360 ||  Training loss : 0.012675  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 370 ||  Training loss : 0.012676  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.012676  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.012676  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.012677  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.012677  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.012677  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.012677  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 440 ||  Training loss : 0.012678  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.012678  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 460 ||  Training loss : 0.012678  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 470 ||  Training loss : 0.012678  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 480 ||  Training loss : 0.012678  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.012679  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.012680  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.012681  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.012682  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.012683  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.012683  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.012683  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.012683  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.012683  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.012683  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.012683  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 2 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847812990836\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6946567693489\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887180742943\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6916334015646\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6893304429596\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6915155460011\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6952191057069\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6899483757388\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6891628227976\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6934872817837\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6904664854506\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6918263986978\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6926379700421\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896863661632\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6850450941362\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864079155195\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908955675187\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6916018045459\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901883244366\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6882315210877\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924922445698\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6866235853146\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881857084672\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.69047351428\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6897058422127\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851232495946\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.683101628636\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6895960641276\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6944566760987\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6933581418925\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6939350882801\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855082635574\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864639691526\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907660526792\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943191227685\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6951627295592\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6922856784652\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6892140400261\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907443581368\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6918527634059\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6951170925987\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6880923952269\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6958753403063\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6880985146398\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.686140581964\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6909727075864\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847622696912\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6917513551688\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6930474090177\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896280647356\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","50 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","50 systems found, adding energy\n","{'mae': 0.0101, 'max': 0.03333, 'mean deviation': 0.0, 'rmse': 0.01279}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.012786623793828646\n","Dataset 0 new STD: 0.044043335838077256\n","1\n","Epoch 0 ||  Training loss : 0.012787  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.014012  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.014119  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.014066  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.014304  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.014406  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.014520  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.014629  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.014721  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.014771  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.014820  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.014868  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.014879  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.014888  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.014897  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.014906  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.014915  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.014924  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.014933  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.014942  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.014951  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.014961  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.014970  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.014972  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.014973  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.014974  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.014975  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.014976  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.014977  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.014978  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.014979  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.014981  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.014982  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.014983  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.014983  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.014983  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.014983  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.014984  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.014984  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.014984  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.014984  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.014984  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.014984  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.014984  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.014984  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.014985  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.014986  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.014987  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.014988  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.014988  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.014988  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.014988  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.014988  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.014988  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.014988  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.014988  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.014988  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.014988  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.014988  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.014988  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.014988  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.014988  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.014988  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.014988  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.014988  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.014988  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.014988  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.014988  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.014988  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.014989  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.014989  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.014989  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.014989  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.014989  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.014989  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.014989  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.014989  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.014989  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.014989  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.014989  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.014989  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.014989  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.014989  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.014989  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 3 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847890359951\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6945716627173\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6885502783873\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6916619649901\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6891820997938\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6915575756754\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6951166144463\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898155961401\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6892974689276\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6933814269427\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6904462906289\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919351848753\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6925247202652\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.689661497031\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6850159036649\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864857942463\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6909725135942\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6916315508693\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901760123708\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6882727139313\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.692456069557\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6866730793533\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6882903791574\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.690598008544\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.689615323109\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6853717824461\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6831906342711\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896031847436\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943756816839\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6932665455315\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.693873889668\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.685481269537\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6865023176241\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908686790219\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943214223694\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.694978562605\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6922865199802\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6892176939858\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907082114267\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919082092009\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.695064376805\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6880439575408\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6957677068165\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6880641506359\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6860416578618\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908190358868\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6850004751061\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6917697958977\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928773159733\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6897242000236\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","50 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","50 systems found, adding energy\n","{'mae': 0.01213, 'max': 0.03808, 'mean deviation': 0.0, 'rmse': 0.01499}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.014990626676943512\n","Dataset 0 new STD: 0.04407744307923809\n","1\n","Epoch 0 ||  Training loss : 0.014991  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015029  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015515  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015197  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015197  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015274  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015279  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015355  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015404  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015492  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015571  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015663  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015684  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015695  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015708  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015720  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015732  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015744  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015757  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015770  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015783  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015796  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015809  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015812  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015813  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015815  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015816  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015818  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015819  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015821  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015822  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015824  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015825  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015827  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015827  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015828  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015828  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015828  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015828  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015828  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015828  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015830  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015832  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 4 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.684791398097\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6945461904167\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.688486843623\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6916783803092\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6891146954308\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6915648543969\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950751872889\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6897643280339\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6893528449207\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6933466173017\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6904381605874\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919851164668\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924819196183\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896308178066\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6850109249869\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864881216167\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6910088165735\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6916414629776\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901708782416\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6882860512583\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924433347347\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6866735514661\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883090089537\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6906480489992\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6895704074175\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6853941496061\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6831661467224\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896013621352\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943492875227\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6932348963361\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6938519297661\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6854454101177\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864967811954\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6909069157398\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943253733546\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6949034813094\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6922881829303\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6892187893163\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6906945056528\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919308314258\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950513340812\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6880162015118\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6957326302553\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6880480364507\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6859873573612\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907571084281\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6850505850149\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6917843068277\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928042546454\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6897644590608\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","50 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","50 systems found, adding energy\n","{'mae': 0.01282, 'max': 0.03985, 'mean deviation': 0.0, 'rmse': 0.01583}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015830514703735084\n","Dataset 0 new STD: 0.04409080431210056\n","1\n","Epoch 0 ||  Training loss : 0.015831  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.016222  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.016435  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.016188  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.016232  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.016252  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.016182  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.016198  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.016167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.016202  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.016289  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.016418  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.016445  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.016459  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.016475  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.016491  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.016507  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.016523  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.016540  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.016557  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.016574  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.016591  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.016609  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.016612  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.016614  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.016616  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.016618  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.016620  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.016622  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.016624  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.016626  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.016628  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.016630  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.016632  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.016633  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.016633  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.016633  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.016634  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.016634  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.016634  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.016634  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.016634  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.016635  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.016635  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.016635  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.016635  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.016635  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.016635  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.016635  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.016635  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.016635  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.016635  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.016635  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.016635  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.016635  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.016636  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.016637  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.016638  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.016640  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.016640  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.016640  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.016640  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.016640  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.016640  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.016640  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.016640  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.016640  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.016640  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.016640  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.016640  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.016640  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.016640  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.016640  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.016640  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.016640  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.016640  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.016640  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 5 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.684787404615\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6945436126904\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6884403671528\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6917031716748\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6890450663014\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6915710955219\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950402284869\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6897245696635\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.68941343045\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6933353603426\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6904270432016\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6920397493781\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.692453560639\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896107462992\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6850214938874\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864610050292\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6910498484106\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6916518078136\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901627144043\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6882969479962\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924403553351\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6866639580129\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883136748273\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907093729961\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6895026109167\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6853974829012\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6831199637047\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6895870474474\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943407771312\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6932140217725\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6938385687502\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6854012998139\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864956184655\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6909381338396\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943352603959\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6948345791341\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6922925061255\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.689218257287\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6906851932246\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919531385079\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950577944344\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6879791716955\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6957209258615\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6880236930444\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6859206320858\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6906976424776\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.685097290178\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6918152137697\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6927263889918\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898158531117\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","50 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","50 systems found, adding energy\n","{'mae': 0.01343, 'max': 0.04151, 'mean deviation': -0.0, 'rmse': 0.01664}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.01663882206725846\n","Dataset 0 new STD: 0.04408699459139337\n","1\n","Epoch 0 ||  Training loss : 0.016639  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.016579  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.017060  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.016893  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.017054  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.017135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.017247  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.017381  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.017532  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.017678  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.017841  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 110 ||  Training loss : 0.018007  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    13: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 120 ||  Training loss : 0.018180  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 130 ||  Training loss : 0.018214  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.018233  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.018253  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.018273  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.018293  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.018315  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.018336  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.018359  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.018381  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 220 ||  Training loss : 0.018405  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    24: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 230 ||  Training loss : 0.018428  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 240 ||  Training loss : 0.018433  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.018435  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.018438  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.018441  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.018443  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.018446  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.018448  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.018451  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.018454  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 330 ||  Training loss : 0.018457  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    35: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 340 ||  Training loss : 0.018460  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 350 ||  Training loss : 0.018460  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.018461  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.018461  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.018461  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.018462  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.018462  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.018462  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.018462  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.018463  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 440 ||  Training loss : 0.018463  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    46: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 450 ||  Training loss : 0.018463  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 460 ||  Training loss : 0.018463  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.018464  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.018465  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.018466  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.018467  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.018467  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.018467  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.018467  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.018467  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.018467  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.018467  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.018467  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.018467  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.018467  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.018467  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.018467  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.018467  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.018467  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.018467  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.018467  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.018467  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.018467  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.018467  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.018467  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.018467  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.018468  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.018468  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.018468  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.018468  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.018468  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.018468  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.018468  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.018468  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.018468  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.018468  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.018468  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.018468  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.018468  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.018468  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.018468  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.018468  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.018468  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.018468  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.018468  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.018468  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.018469  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.018469  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.018469  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.018469  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.018469  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.018469  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.018469  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.018469  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.018469  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.018469  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.018469  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.018469  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.018469  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.018469  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.018469  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.018469  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.018469  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.018469  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.018469  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.018470  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.018470  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.018470  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.018470  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.018470  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.018470  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.018470  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.018470  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.018470  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.018470  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.018470  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.018470  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.018470  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.018470  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.018470  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.018470  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.018470  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.018470  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.018470  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.018470  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Testing ======\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.955670394155\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941863594178\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942195506176\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936256268652\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.933428486386\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.935974684513\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937003661533\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945051691903\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939662909127\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945864667348\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.934755118142\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.935094550083\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937598815316\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937861389666\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VsKw4wm_obyf"},"source":["## Model then trained on 20 propane"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r96HxyhLCmVI","executionInfo":{"status":"ok","timestamp":1623602666612,"user_tz":-60,"elapsed":622432,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"afb2d739-e572-46c1-c5c7-7ad579c7724d"},"source":["!sh train_50_20.sh "],"execution_count":null,"outputs":[{"output_type":"stream","text":["cp: -r not specified; omitting directory '../workdir'\n","using unit 0.04336410390059322\n","using unit 0.04336410390059322\n","FILEPATH basis_sgdml_benzene.json\n","Traceback (most recent call last):\n","  File \"../../../scripts/fix_paths.py\", line 13, in <module>\n","    basis['engine_kwargs']['pseudoloc'] = os.path.abspath(basis['engine_kwargs']['pseudoloc'])\n","KeyError: 'pseudoloc'\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","====== Iteration 0 ======\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/model0.jit\n","NeuralXC: Model successfully loaded\n","Overwritten attributes  get_veff  of <class 'pyscf.dft.rks.RKS'>\n","converged SCF energy = -118.941969114032\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/model0.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944305391491\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/model0.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.93983317644\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/model0.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.951894798676\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/model0.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94409407734\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/model0.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945798236578\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/model0.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946617353246\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/model0.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943485145321\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/model0.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948320913431\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/model0.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941682256646\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/model0.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946314588459\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/model0.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950839473922\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/model0.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937162560917\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/model0.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947621121688\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/model0.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950826109031\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/model0.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943477531574\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/model0.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938558911919\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/model0.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948771405917\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/model0.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943346163293\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/model0.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94717238969\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","20 systems found, adding energy\n","{'mae': 0.01421, 'max': 0.02941, 'mean deviation': -0.0, 'rmse': 0.01723}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.017228065577353263\n","Dataset 0 new STD: 0.04462116550787632\n","1\n","Epoch 0 ||  Training loss : 0.017228  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.017007  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.016873  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.016238  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.016093  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.016194  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.016273  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.016368  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.016438  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.016500  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.016572  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 110 ||  Training loss : 0.016650  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.016732  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 130 ||  Training loss : 0.016818  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 140 ||  Training loss : 0.016905  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 150 ||  Training loss : 0.016996  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 160 ||  Training loss : 0.017013  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.017023  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.017033  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.017043  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.017053  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.017064  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 220 ||  Training loss : 0.017075  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.017086  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 240 ||  Training loss : 0.017097  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 250 ||  Training loss : 0.017109  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    27: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 260 ||  Training loss : 0.017121  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 270 ||  Training loss : 0.017123  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.017124  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.017126  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.017127  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.017128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.017130  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 330 ||  Training loss : 0.017131  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.017132  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 350 ||  Training loss : 0.017134  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 360 ||  Training loss : 0.017135  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    38: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 370 ||  Training loss : 0.017136  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 380 ||  Training loss : 0.017137  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.017137  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.017137  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.017137  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.017137  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.017137  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 440 ||  Training loss : 0.017138  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.017138  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 460 ||  Training loss : 0.017138  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 470 ||  Training loss : 0.017138  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 480 ||  Training loss : 0.017138  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 490 ||  Training loss : 0.017138  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.017138  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.017138  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.017138  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.017138  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.017138  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.017138  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.017138  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.017138  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.017138  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.017138  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.017138  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.017138  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.017139  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.017140  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.017141  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.017142  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.017142  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.017142  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.017142  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.017142  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.017142  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.017142  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.017142  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.017142  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.017142  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 1 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941781315231\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944113038104\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939673529193\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.952050189855\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944135267433\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945796163868\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946729738487\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943535459586\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948305337114\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941605910154\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946394500233\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950983209913\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937177487195\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947655927737\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950863840449\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94339285155\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938548736408\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948767890243\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943339144377\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947162553122\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01399, 'max': 0.02972, 'mean deviation': -0.0, 'rmse': 0.01714}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.017136979008589823\n","Dataset 0 new STD: 0.044477956279030034\n","1\n","Epoch 0 ||  Training loss : 0.017137  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.017296  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.017555  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.017312  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.017422  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.017494  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.017507  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.017573  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.017637  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.017708  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.017784  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.017861  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.017876  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.017885  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.017893  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.017902  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.017912  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.017922  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.017932  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.017942  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.017952  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.017963  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.017974  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.017977  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.017978  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.017979  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.017980  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.017981  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.017983  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.017984  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.017985  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.017987  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.017988  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.017989  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.017990  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.017990  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.017990  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.017990  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.017990  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.017990  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.017990  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.017991  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.017992  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.017993  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.017994  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 2 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941793151389\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944043846395\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939654976746\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.952072382228\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944107512332\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945789594839\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946785821889\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943496646787\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948271099974\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94162592865\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946434886212\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950953981487\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937177405905\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947673196318\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950831411079\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943395820339\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938628721223\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948757163786\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943322423852\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947116189589\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01471, 'max': 0.03057, 'mean deviation': -0.0, 'rmse': 0.018}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.017995289419667392\n","Dataset 0 new STD: 0.04448254008572297\n","1\n","Epoch 0 ||  Training loss : 0.017995  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.018137  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.018365  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.018107  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.018219  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.018313  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.018312  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.018370  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.018426  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.018501  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.018571  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.018644  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.018659  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.018668  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.018676  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.018685  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.018695  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.018704  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.018714  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.018724  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.018734  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.018745  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.018756  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.018758  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.018759  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.018760  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.018761  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.018763  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.018764  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.018765  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.018766  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.018768  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.018769  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.018770  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.018771  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.018771  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.018771  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.018771  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.018771  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.018771  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.018772  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.018772  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.018772  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.018772  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.018772  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.018772  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.018772  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.018772  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.018772  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.018772  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.018772  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.018772  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.018772  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.018772  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.018772  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.018772  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.018772  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.018772  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.018772  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.018772  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.018772  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.018773  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.018774  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.018775  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.018776  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.018777  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.018777  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.018777  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.018777  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 3 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941804195746\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943982398118\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939639049055\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.952092376234\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944082661212\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945784060601\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94683617443\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943461884919\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948240669844\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941644188911\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94647169873\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.95092739997\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937177558452\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94768880974\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950802452517\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943399053752\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938701116206\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948747805423\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943307332268\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947074695572\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01536, 'max': 0.03263, 'mean deviation': -0.0, 'rmse': 0.01878}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.018776695880265745\n","Dataset 0 new STD: 0.04448845919686398\n","1\n","Epoch 0 ||  Training loss : 0.018777  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.018948  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.019116  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.018930  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.018986  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.019065  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.019094  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.019128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.019190  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.019260  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.019325  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.019396  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.019410  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.019418  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.019427  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.019435  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.019444  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.019453  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.019463  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.019472  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.019482  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.019493  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.019503  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.019505  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.019506  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.019507  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.019508  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.019510  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.019511  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.019512  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.019513  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.019514  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.019516  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.019517  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.019517  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.019517  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 4 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941812616464\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943923650402\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.93962313516\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.952112974885\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944059748864\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945779297667\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946884946145\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943429772358\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948212420982\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941661191233\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946508001263\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950903432448\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937178313139\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947704362263\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950775883327\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943402135203\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.93877013461\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948739359704\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943293245823\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947035771366\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01597, 'max': 0.03459, 'mean deviation': -0.0, 'rmse': 0.01952}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.019523389254705176\n","Dataset 0 new STD: 0.04449232361955598\n","1\n","Epoch 0 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.019752  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.019647  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.019747  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.019691  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.019749  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.019804  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.019846  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.019891  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.019958  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.020019  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.020083  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.020095  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.020102  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.020109  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.020117  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.020125  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.020133  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.020141  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.020149  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.020158  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.020167  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.020176  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.020178  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.020179  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.020180  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.020181  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.020182  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.020183  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.020184  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.020185  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.020186  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.020188  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.020189  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.020189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.020189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.020189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.020189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.020189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.020190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.020191  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.020192  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.020193  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 5 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941818540295\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943871704966\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939608748699\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.952131352852\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944038954498\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945775214532\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946928512748\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943401111169\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948187436955\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941676347497\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946540854092\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950881744987\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937179175189\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947718525776\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950752286686\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.9434050346\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938832196943\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948731861577\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943280840786\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947001147127\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01652, 'max': 0.03634, 'mean deviation': -0.0, 'rmse': 0.02019}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.02019440406446647\n","Dataset 0 new STD: 0.04449532646931608\n","1\n","Epoch 0 ||  Training loss : 0.020194  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.020485  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.020250  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.020366  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.020385  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.020397  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.020452  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.020506  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.020542  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.020601  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.020658  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.020718  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.020728  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.020735  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.020741  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.020748  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.020755  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.020762  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.020770  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.020777  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.020785  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.020793  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.020802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.020803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.020804  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.020805  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.020806  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.020807  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.020808  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.020809  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.020810  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.020811  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.020812  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.020813  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.020813  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.020813  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.020813  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.020813  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.020814  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.020814  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.020814  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.020814  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.020814  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.020814  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.020814  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.020814  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.020814  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.020814  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.020814  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.020814  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.020814  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.020814  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.020814  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.020814  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.020814  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.020814  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.020814  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.020814  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.020814  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.020815  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.020816  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.020817  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.020818  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.020818  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.020818  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.020818  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.020818  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.020818  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.020818  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.020818  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.020818  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.020818  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.020818  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.020818  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.020818  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.020818  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.020818  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.020818  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.020818  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.020818  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Testing ======\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.955915782422\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941898077214\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942137250517\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936116912323\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.93318073598\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.935828506438\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936920627246\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944957874275\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939784906634\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945776296322\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.934933207134\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.934836703009\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937602998323\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937801111982\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939498348784\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939243384553\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936890980769\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.93606983068\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942910595796\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.935480943928\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945249925581\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939834540659\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940243484565\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940903014382\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941926144752\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940618451105\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939136919011\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946855557694\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940141282434\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947518007259\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942128674867\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945967605091\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945134000515\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940565603521\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945294689752\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940037257819\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943855491923\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941426115424\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940340215479\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943958550566\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944475299921\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948388859073\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939183135796\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945856712439\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940302448834\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937092873565\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942536229578\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941843234958\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944248772736\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942248395089\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938215871625\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936820236165\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937474112009\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939237065528\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937404068971\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945158950893\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94761413697\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.9457122791\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942399890505\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943636873433\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939198106205\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.935993816487\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941319770729\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94279858486\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942545809202\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94171220789\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938876989197\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936596273668\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943410675546\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.935875877355\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940822031874\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941733874058\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946807578653\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942263915311\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937761560583\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942895524756\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.93774500954\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.935886636616\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.935363161055\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94415012673\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.933892340876\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938445547289\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939989669951\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.932617091688\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.935314411752\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.933629237363\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941676347497\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.934010032285\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937952901539\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939985966163\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94053746616\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94369141006\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941832406783\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946010118938\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939706196858\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945921839133\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939723221005\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943490401786\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944961879713\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941639998772\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941943265166\n","101 systems found, adding energy\n","101 systems found, adding energy\n","{'mae': 0.01664, 'max': 0.08024, 'mean deviation': 0.0, 'rmse': 0.02138}\n","/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"UDMJ6QQtF5GI","executionInfo":{"status":"ok","timestamp":1623602952086,"user_tz":-60,"elapsed":202,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"c020f32d-e1ef-461e-e570-021f5d9a5ca9"},"source":["pwd"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20'"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"W4qvMLpPGLqV"},"source":["cp -r ../train_model_50/final_model/ workdir/50_final_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WKbm76asHG6P","executionInfo":{"status":"ok","timestamp":1623600166996,"user_tz":-60,"elapsed":203,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"3ab7f5f2-e664-493d-b64a-5f116f4408f3"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["basis_sgdml_benzene.json  README.txt       train_50_20.sh    \u001b[0m\u001b[01;34mworkdir\u001b[0m/\n","hyperparameters.json      test_subset.csv  train_subset.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3YesCVHDHHZ0","executionInfo":{"status":"ok","timestamp":1623600171448,"user_tz":-60,"elapsed":200,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"1d4ee806-bb14-4522-86e5-3bec53d43b10"},"source":["ls workdir/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34m50_final_model\u001b[0m/  butane.xyz  propane.xyz\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"uKmMv3AxHIfX","executionInfo":{"status":"ok","timestamp":1623603524717,"user_tz":-60,"elapsed":210,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"4d7eb814-9c9b-4be8-d6fc-c49a38ba8d1a"},"source":["pwd"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20'"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_-y-zV3DT7CL","executionInfo":{"status":"ok","timestamp":1623603527540,"user_tz":-60,"elapsed":420,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"28acccc0-f78a-4c0d-9f24-5b6edffeebfd"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["basis_sgdml_benzene.json  hyperparameters.json  train_50_20.sh\n","\u001b[0m\u001b[01;34mfinal_model\u001b[0m/              README.txt            train_subset.csv\n","\u001b[01;34mfinal_model.jit\u001b[0m/          test_subset.csv       \u001b[01;34mworkdir\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iBcRZj1qUFsp","executionInfo":{"status":"ok","timestamp":1623604184669,"user_tz":-60,"elapsed":2933,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"f0897976-0ab0-4a91-c9a4-036b1cdeb522"},"source":["!neuralxc eval --model final_model --hdf5 workdir/testing/data.hdf5 workdir/sc/data.hdf5 workdir/testing.traj --plot --savefig plot.pdf"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/usr/local/bin/neuralxc\", line 7, in <module>\n","    exec(compile(f.read(), __file__, 'exec'))\n","  File \"/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/bin/neuralxc\", line 267, in <module>\n","    func(**args_dict)\n","  File \"/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/drivers/model.py\", line 420, in eval_driver\n","    data = load_sets(datafile, hdf5[1], hdf5[2], basis_key, cutoff)\n","  File \"/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/ml/utils.py\", line 123, in load_sets\n","    load_data(datafile, bl, ref, basis_key, perc)\n","  File \"/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/ml/utils.py\", line 178, in load_data\n","    E0_base = find_attr_in_tree(datafile, baseline, 'E0')\n","  File \"/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/ml/utils.py\", line 74, in find_attr_in_tree\n","    if attr in file[tree].attrs:\n","  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n","  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n","  File \"/usr/local/lib/python3.7/dist-packages/h5py/_hl/group.py\", line 262, in __getitem__\n","    oid = h5o.open(self.id, self._e(name), lapl=self._lapl)\n","  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n","  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n","  File \"h5py/h5o.pyx\", line 190, in h5py.h5o.open\n","KeyError: 'Unable to open object (component not found)'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"-eJ6yV77WFmn","executionInfo":{"status":"ok","timestamp":1623604093875,"user_tz":-60,"elapsed":8,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"e6a8538c-4d91-4a84-956f-e2d6e7bd2fcd"},"source":["pwd"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20'"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"HS7h_AtRY4aR"},"source":[""],"execution_count":null,"outputs":[]}]}