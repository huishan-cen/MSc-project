{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test_neural_XC_2.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNnKs0vPWNuRhb4WAGAziUJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"if85Sc0jOBmq","executionInfo":{"status":"ok","timestamp":1624805869483,"user_tz":-60,"elapsed":28602,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"9c4d8da2-7fb8-4451-d1d4-e25fe3e0b3ba"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U3BJKgV4SLkQ","executionInfo":{"status":"ok","timestamp":1624805869486,"user_tz":-60,"elapsed":9,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}}},"source":["import numpy as np"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HeCIdApVqzlt","executionInfo":{"status":"ok","timestamp":1624805870435,"user_tz":-60,"elapsed":956,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"61dc582e-98c3-4217-bd00-891897aa3242"},"source":["cd drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WKdPFb4j9R6P"},"source":["## Install"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iCOKexN3P9Bo","executionInfo":{"status":"ok","timestamp":1624806111458,"user_tz":-60,"elapsed":241026,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"c64336d2-607e-4b80-bfc2-f1e2a860b480"},"source":["!sh install.sh"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/semodi/libnxc.git@9c3b895840a000968866a44841291926fe29aab7 (from -r requirements.txt (line 23))\n","  Cloning https://github.com/semodi/libnxc.git (to revision 9c3b895840a000968866a44841291926fe29aab7) to /tmp/pip-req-build-y1tgaqfp\n","  Running command git clone -q https://github.com/semodi/libnxc.git /tmp/pip-req-build-y1tgaqfp\n","  Running command git checkout -q 9c3b895840a000968866a44841291926fe29aab7\n","Collecting ase==3.17\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/af/ea30928def88748d76946c0af0e6a8f9c2d2e65e3d65da527fe80e9ba06e/ase-3.17.0-py3-none-any.whl (1.8MB)\n","\u001b[K     |████████████████████████████████| 1.8MB 14.9MB/s \n","\u001b[?25hRequirement already satisfied: dask in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2.12.0)\n","Collecting h5py==2.9.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/fd/2ca5c4f4ed33ac4178f9c4d551e3946ab480866e3cd67a65a67a4bb35367/h5py-2.9.0-cp37-cp37m-manylinux1_x86_64.whl (2.8MB)\n","\u001b[K     |████████████████████████████████| 2.8MB 51.9MB/s \n","\u001b[?25hCollecting ipyparallel\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/e9/03a9189eb39276396309faf28bf833b4328befe4513bbf375b811a36a076/ipyparallel-6.3.0-py3-none-any.whl (199kB)\n","\u001b[K     |████████████████████████████████| 204kB 54.0MB/s \n","\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (5.5.0)\n","Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (1.0.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.19.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (1.1.5)\n","Collecting periodictable\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/47/bc639be580ffa41cb859a409c71da2c7ccaf196f8b7c8aa7a0473ba84b9e/periodictable-1.6.0.tar.gz (686kB)\n","\u001b[K     |████████████████████████████████| 696kB 43.6MB/s \n","\u001b[?25hCollecting scikit-learn==0.20.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/cc/a84e1748a2a70d0f3e081f56cefc634f3b57013b16faa6926d3a6f0598df/scikit_learn-0.20.3-cp37-cp37m-manylinux1_x86_64.whl (5.4MB)\n","\u001b[K     |████████████████████████████████| 5.4MB 46.3MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (1.4.1)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (3.6.4)\n","Collecting pytest-cov\n","  Downloading https://files.pythonhosted.org/packages/ba/84/576b071aef9ac9301e5c0ff35d117e12db50b87da6f12e745e9c5f745cc2/pytest_cov-2.12.1-py2.py3-none-any.whl\n","Collecting dill==0.3.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/96/518a8ea959a734b70d2e95fef98bcbfdc7adad1c1e5f5dd9148c835205a5/dill-0.3.2.zip (177kB)\n","\u001b[K     |████████████████████████████████| 184kB 51.3MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 18)) (1.9.0+cu102)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 19)) (3.3.0)\n","Collecting codecov\n","  Downloading https://files.pythonhosted.org/packages/93/9f/bbea5b6231308458963cb5c067bc5643da9949689702fa5a382714b59699/codecov-2.1.11-py2.py3-none-any.whl\n","Collecting pyscf\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/d6/af4ce5035977cb011e4dbe9979bf254129a36d48cb569b86e57b5a72c5b1/pyscf-1.7.6.post1-cp37-cp37m-manylinux1_x86_64.whl (29.7MB)\n","\u001b[K     |████████████████████████████████| 29.7MB 172kB/s \n","\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 22)) (0.8.9)\n","Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from ase==3.17->-r requirements.txt (line 1)) (1.1.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.9.0->-r requirements.txt (line 5)) (1.15.0)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (5.3.5)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (22.1.0)\n","Requirement already satisfied: ipykernel>=4.4 in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (4.10.1)\n","Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (5.1.1)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (0.2.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (4.4.2)\n","Requirement already satisfied: traitlets>=4.3 in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (5.0.5)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (2.8.1)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 7)) (4.8.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 7)) (57.0.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 7)) (0.7.5)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 7)) (1.0.18)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 7)) (0.8.1)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 7)) (2.6.1)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 8)) (5.2.0)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 8)) (5.1.0)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 8)) (5.6.1)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 8)) (7.6.3)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 8)) (5.3.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 9)) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 9)) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 9)) (2.4.7)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 11)) (2018.9)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 15)) (8.8.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 15)) (1.10.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 15)) (21.2.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 15)) (1.4.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 15)) (0.7.1)\n","Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from pytest-cov->-r requirements.txt (line 16)) (0.10.2)\n","Collecting coverage>=5.2.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/e0/fc9f7bd9b84e6b41d0aad1a113e36714aac0c0a9b307aca5f9af443bc50f/coverage-5.5-cp37-cp37m-manylinux2010_x86_64.whl (242kB)\n","\u001b[K     |████████████████████████████████| 245kB 57.6MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->-r requirements.txt (line 18)) (3.7.4.3)\n","Requirement already satisfied: requests>=2.7.9 in /usr/local/lib/python3.7/dist-packages (from codecov->-r requirements.txt (line 20)) (2.23.0)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->ase==3.17->-r requirements.txt (line 1)) (7.1.2)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->ase==3.17->-r requirements.txt (line 1)) (2.11.3)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->ase==3.17->-r requirements.txt (line 1)) (1.1.0)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->ase==3.17->-r requirements.txt (line 1)) (1.0.1)\n","Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipyparallel->-r requirements.txt (line 6)) (4.7.1)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->-r requirements.txt (line 7)) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->-r requirements.txt (line 7)) (0.2.5)\n","Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->-r requirements.txt (line 8)) (1.9.0)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (0.3)\n","Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (5.1.3)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (0.7.1)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (1.4.3)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (0.8.4)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (0.5.0)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (3.3.0)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 8)) (3.5.1)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 8)) (1.0.0)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->-r requirements.txt (line 8)) (0.10.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->-r requirements.txt (line 8)) (1.5.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov->-r requirements.txt (line 20)) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov->-r requirements.txt (line 20)) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov->-r requirements.txt (line 20)) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov->-r requirements.txt (line 20)) (2.10)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->ase==3.17->-r requirements.txt (line 1)) (2.0.1)\n","Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.4->nbconvert->jupyter->-r requirements.txt (line 8)) (2.6.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->-r requirements.txt (line 8)) (0.5.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->-r requirements.txt (line 8)) (20.9)\n","Building wheels for collected packages: periodictable, dill, pylibnxc\n","  Building wheel for periodictable (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for periodictable: filename=periodictable-1.6.0-cp37-none-any.whl size=749750 sha256=d41ef64bed39c84b5326ae6f298ee1992ff0cbb4ffd88ec89085d1dc1f13357b\n","  Stored in directory: /root/.cache/pip/wheels/eb/78/08/4cb95d4ae156e978980596c1f25bb8365d884de1725ef9a306\n","  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for dill: filename=dill-0.3.2-cp37-none-any.whl size=78927 sha256=9bd10d46fc5fa3b69fd1ff91cff9d96934c8c2f9362bd0c817f239e0724054b0\n","  Stored in directory: /root/.cache/pip/wheels/27/4b/a2/34ccdcc2f158742cfe9650675560dea85f78c3f4628f7daad0\n","  Building wheel for pylibnxc (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pylibnxc: filename=pylibnxc-0.1-cp37-none-any.whl size=13977 sha256=9449d1c8dd85870d1d65e683c03a620e11c523d6474a6b9535c160a5975b7e51\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-vhv_8mar/wheels/b2/6c/b4/59f56fd21ac3b81416fd97d1d8c7016582d9219d7a51104f9c\n","Successfully built periodictable dill pylibnxc\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement h5py~=3.1.0, but you'll have h5py 2.9.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: multiprocess 0.70.12.2 has requirement dill>=0.3.4, but you'll have dill 0.3.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement coverage==3.7.1, but you'll have coverage 5.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: coveralls 0.5 has requirement coverage<3.999,>=3.6, but you'll have coverage 5.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: pytest-cov 2.12.1 has requirement pytest>=4.6, but you'll have pytest 3.6.4 which is incompatible.\u001b[0m\n","Installing collected packages: ase, h5py, ipyparallel, periodictable, scikit-learn, coverage, pytest-cov, dill, codecov, pyscf, pylibnxc\n","  Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","  Found existing installation: coverage 3.7.1\n","    Uninstalling coverage-3.7.1:\n","      Successfully uninstalled coverage-3.7.1\n","  Found existing installation: dill 0.3.4\n","    Uninstalling dill-0.3.4:\n","      Successfully uninstalled dill-0.3.4\n","Successfully installed ase-3.17.0 codecov-2.1.11 coverage-5.5 dill-0.3.2 h5py-2.9.0 ipyparallel-6.3.0 periodictable-1.6.0 pylibnxc-0.1 pyscf-1.7.6.post1 pytest-cov-2.12.1 scikit-learn-0.20.3\n","Obtaining file:///content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc\n","Installing collected packages: neuralxc\n","  Running setup.py develop for neuralxc\n","Successfully installed neuralxc\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GV0j19J1i4CG"},"source":["##Command - neuralxc help"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MgyBs28Inlkx","executionInfo":{"status":"ok","timestamp":1623977787203,"user_tz":-60,"elapsed":7580,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"1d82e340-2ccf-4adb-e3b6-c681c2ccaca7"},"source":["!neuralxc --help"],"execution_count":null,"outputs":[{"output_type":"stream","text":["usage: neuralxc [-h]\n","                {basis,data,fit,sc,eval,predict,serialize,pre,default,engine}\n","                ...\n","\n","Add data to hdf5 file\n","\n","positional arguments:\n","  {basis,data,fit,sc,eval,predict,serialize,pre,default,engine}\n","\n","optional arguments:\n","  -h, --help            show this help message and exit\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZdxmbqVYmgAR"},"source":["### neuralxc sc"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bECv5gJwtCB9","executionInfo":{"status":"ok","timestamp":1623794681577,"user_tz":-60,"elapsed":2429,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"08a3b3d8-8c7e-4c40-a22b-8091613fcbbc"},"source":["!neuralxc sc -h"],"execution_count":null,"outputs":[{"output_type":"stream","text":["usage: neuralxc sc [-h] [--data data] [--maxit maxit] [--tol tol]\n","                   [--sets sets] [--nozero] [--model0 model0] [--hyperopt]\n","                   xyz preprocessor hyper\n","\n","Fit a NeuralXC model selfconsistently\n","\n","positional arguments:\n","  xyz              Path to .xyz/.traj file containing structures and reference\n","                   data\n","  preprocessor     Path to configuration file for preprocessor\n","  hyper            Path to .json configuration file setting hyperparameters\n","\n","optional arguments:\n","  -h, --help       show this help message and exit\n","  --data data      Start from this dataset instead of computing iteration 0\n","  --maxit maxit    Maximum number of iterations (default: 5)\n","  --tol tol        Tolerance in energy defining whether iterative training\n","                   converged (default: 0.0005 eV)\n","  --sets sets      Path to file defining sets\n","  --nozero         Do not automatically set energy origins for every dataset\n","                   by using min\n","  --model0 model0  Build new model on top of model0 as a stacked estimator\n","  --hyperopt       Do a hyperparameter optimzation\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lWjjZ9R6mnVZ"},"source":["### neuralxc eval"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"byWSDcGRRCJg","executionInfo":{"status":"ok","timestamp":1623794678196,"user_tz":-60,"elapsed":31747,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"c9416ff9-df54-4caf-c06c-5eb51e71ce6e"},"source":["!neuralxc eval -h"],"execution_count":null,"outputs":[{"output_type":"stream","text":["usage: neuralxc eval [-h] [--model model] [--hdf5 hdf5 hdf5 hdf5] [--plot]\n","                     [--savefig SAVEFIG] [--cutoff cutoff] [--sample sample]\n","                     [--invert_sample] [--keep_mean] [--hashkey HASHKEY]\n","\n","Evaluate a NeuralXC model\n","\n","optional arguments:\n","  -h, --help            show this help message and exit\n","  --model model         Path to NeuralXC model\n","  --hdf5 hdf5 hdf5 hdf5\n","                        Path to hdf5 file, baseline data, reference data\n","  --plot                Create scatterplot?\n","  --savefig SAVEFIG     Save scatterplot?\n","  --cutoff cutoff       Cut off extreme datapoints\n","  --sample sample       Evaluate on sample. Path to sample file\n","  --invert_sample       Invert the sample provided (evaluate on datapoints not\n","                        in sample)\n","  --keep_mean           If set, don't subract parallelity error from MAE and\n","                        RMSE\n","  --hashkey HASHKEY     Manually choose which basis hash key to apply model to\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KOBWMDz7mu6L"},"source":["### neuralxc data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0dovX3nEwCRF","executionInfo":{"status":"ok","timestamp":1623795598256,"user_tz":-60,"elapsed":2627,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"2aab2255-3b3a-4797-bf85-678e3c82b1b5"},"source":["!neuralxc data -h"],"execution_count":null,"outputs":[{"output_type":"stream","text":["usage: neuralxc data [-h] {add,inspect,split,delete,sample,merge} ...\n","\n","Routines to manipulate datasets\n","\n","positional arguments:\n","  {add,inspect,split,delete,sample,merge}\n","\n","optional arguments:\n","  -h, --help            show this help message and exit\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GhT0x2E6mxes"},"source":["### neuralxc engine\n","calculate dft energy"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xYb8R2_kwujf","executionInfo":{"status":"ok","timestamp":1623795790922,"user_tz":-60,"elapsed":3131,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"4082737b-60d0-47db-dd71-3133911fbdd2"},"source":["!neuralxc engine -h"],"execution_count":null,"outputs":[{"output_type":"stream","text":["usage: neuralxc engine [-h] [--workdir workdir] preprocessor xyz\n","\n","Run engine for structures stored in .xyz/.traj file\n","\n","positional arguments:\n","  preprocessor       Config file for preprocessor\n","  xyz                .xyz or .traj file containing structures\n","\n","optional arguments:\n","  -h, --help         show this help message and exit\n","  --workdir workdir  Specify work-directory. If not specified uses .tmp/ and\n","                     deletes after calculation has finished\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"J3P_D-8Bdk6k"},"source":["PBE energy of butane"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"20RYuZXixbT9","executionInfo":{"status":"ok","timestamp":1623960737932,"user_tz":-60,"elapsed":1194701,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"b4639dd1-336d-475a-cd4a-e3a2f9f03e90"},"source":["!neuralxc engine basis_sgdml_benzene.json workdir/testing.traj --workdir ./tmp"],"execution_count":null,"outputs":[{"output_type":"stream","text":["converged SCF energy = -158.213511336514\n","converged SCF energy = -158.201412578238\n","converged SCF energy = -158.189950939404\n","converged SCF energy = -158.199422033359\n","converged SCF energy = -158.196163848141\n","converged SCF energy = -158.19820283554\n","converged SCF energy = -158.201586477319\n","converged SCF energy = -158.200987213881\n","converged SCF energy = -158.192222003618\n","converged SCF energy = -158.196165114757\n","converged SCF energy = -158.194574022852\n","converged SCF energy = -158.187506634185\n","converged SCF energy = -158.193918785934\n","converged SCF energy = -158.196533994202\n","converged SCF energy = -158.197999954619\n","converged SCF energy = -158.199055561256\n","converged SCF energy = -158.200512444359\n","converged SCF energy = -158.199887597465\n","converged SCF energy = -158.199554184651\n","converged SCF energy = -158.200125696983\n","converged SCF energy = -158.201718873854\n","converged SCF energy = -158.199072895897\n","converged SCF energy = -158.188315751662\n","converged SCF energy = -158.204416758261\n","converged SCF energy = -158.197442225669\n","converged SCF energy = -158.196572555745\n","converged SCF energy = -158.195829983587\n","converged SCF energy = -158.202545291629\n","converged SCF energy = -158.198711164268\n","converged SCF energy = -158.189625074732\n","converged SCF energy = -158.202347151204\n","converged SCF energy = -158.195896849628\n","converged SCF energy = -158.196439811267\n","converged SCF energy = -158.197123599712\n","converged SCF energy = -158.193856716796\n","converged SCF energy = -158.190963436891\n","converged SCF energy = -158.196626980464\n","converged SCF energy = -158.195323740469\n","converged SCF energy = -158.2021477247\n","converged SCF energy = -158.196296604415\n","converged SCF energy = -158.19617059226\n","converged SCF energy = -158.197776863921\n","converged SCF energy = -158.204391488155\n","converged SCF energy = -158.19797442575\n","converged SCF energy = -158.201108058592\n","converged SCF energy = -158.191804198037\n","converged SCF energy = -158.188915853119\n","converged SCF energy = -158.197258802255\n","converged SCF energy = -158.197646896123\n","converged SCF energy = -158.189998618247\n","converged SCF energy = -158.183200178043\n","converged SCF energy = -158.191088611585\n","converged SCF energy = -158.199447849018\n","converged SCF energy = -158.198132248269\n","converged SCF energy = -158.198590254919\n","converged SCF energy = -158.197820619781\n","converged SCF energy = -158.19643057072\n","converged SCF energy = -158.194138887755\n","converged SCF energy = -158.200134715225\n","converged SCF energy = -158.187670854982\n","converged SCF energy = -158.193953169322\n","converged SCF energy = -158.194561280886\n","converged SCF energy = -158.200630577236\n","converged SCF energy = -158.195945998497\n","converged SCF energy = -158.195556256226\n","converged SCF energy = -158.196174755678\n","converged SCF energy = -158.196540379447\n","converged SCF energy = -158.189226337019\n","converged SCF energy = -158.194762675464\n","converged SCF energy = -158.183138803878\n","converged SCF energy = -158.189092317817\n","converged SCF energy = -158.193687071001\n","converged SCF energy = -158.192373684391\n","converged SCF energy = -158.193778383987\n","converged SCF energy = -158.18668783064\n","converged SCF energy = -158.195638573593\n","converged SCF energy = -158.190174596119\n","converged SCF energy = -158.197517322584\n","converged SCF energy = -158.194605122443\n","converged SCF energy = -158.19110883718\n","converged SCF energy = -158.195141896326\n","converged SCF energy = -158.191276430937\n","converged SCF energy = -158.195347868696\n","converged SCF energy = -158.192451006582\n","converged SCF energy = -158.187395486196\n","converged SCF energy = -158.197360954057\n","converged SCF energy = -158.202276411038\n","converged SCF energy = -158.198159432943\n","converged SCF energy = -158.196154973029\n","converged SCF energy = -158.199542604162\n","converged SCF energy = -158.191884380276\n","converged SCF energy = -158.202105320968\n","converged SCF energy = -158.189708680537\n","converged SCF energy = -158.198959618506\n","converged SCF energy = -158.192462325722\n","converged SCF energy = -158.194268318542\n","converged SCF energy = -158.200208094108\n","converged SCF energy = -158.194758505404\n","converged SCF energy = -158.198364141549\n","converged SCF energy = -158.200215981915\n","converged SCF energy = -158.197225585557\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CrYqystjnKQj"},"source":["### neuralxc pre"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xeoKnuAlq_X2","executionInfo":{"status":"ok","timestamp":1624723572852,"user_tz":-60,"elapsed":12230,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"860bde38-da6e-4dd1-f7a5-43255d97cd7d"},"source":["!neuralxc pre -h"],"execution_count":5,"outputs":[{"output_type":"stream","text":["^C\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cS2wuWGfvzCY"},"source":["Running `neuralxc pre ...` gives a numpy array, does not give hdf5 files"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-oJMkAwutHQW","executionInfo":{"status":"ok","timestamp":1624723575633,"user_tz":-60,"elapsed":2786,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"05fbbd59-2fb3-412b-afbf-5b3fe2b2722a"},"source":["!neuralxc pre basis_sgdml_benzene.json --srcdir workdir/testing/workdir --xyz workdir/butane.xyz --dest ./tmp"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/usr/local/bin/neuralxc\", line 7, in <module>\n","    exec(compile(f.read(), __file__, 'exec'))\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/bin/neuralxc\", line 4, in <module>\n","    from neuralxc.drivers import *\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/__init__.py\", line 17, in <module>\n","    from . import (base, config, constants, datastructures, drivers, ml, projector, pyscf, symmetrizer, utils)\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/datastructures/__init__.py\", line 1, in <module>\n","    from . import hdf5\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/datastructures/hdf5.py\", line 7, in <module>\n","    import neuralxc.ml.utils\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/ml/__init__.py\", line 1, in <module>\n","    from . import transformer, utils\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/ml/utils.py\", line 15, in <module>\n","    from neuralxc.preprocessor import Preprocessor\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/preprocessor/__init__.py\", line 1, in <module>\n","    from .driver import driver\n","  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n","  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n","  File \"<frozen importlib._bootstrap_external>\", line 818, in get_code\n","  File \"<frozen importlib._bootstrap_external>\", line 917, in get_data\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FhbkNAYAEnrO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Csr23PKdEpb_"},"source":["## train on 20 methane test on 10 "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jbQcVA8LE0Do","executionInfo":{"status":"ok","timestamp":1624806431913,"user_tz":-60,"elapsed":328,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"4dadce5c-7890-4fec-cc0f-6f508c6fa83c"},"source":["cd examples/example_scripts"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1KA4w-rYG68i","executionInfo":{"status":"ok","timestamp":1624806434568,"user_tz":-60,"elapsed":582,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"7ab34971-b6fb-43c2-bdac-db205de336ec"},"source":["cd train_model_20methane_Cheng_ccsdt/"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lg-oHU3uG-OP","executionInfo":{"status":"ok","timestamp":1624808445039,"user_tz":-60,"elapsed":1949813,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"b4bc069d-d48a-4af2-ce63-d8517443bcca"},"source":["!sh train_20_CH4.sh"],"execution_count":8,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 5 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588698872543\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603699120466\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596787136638\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564476942995\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566705237157\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607279372583\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577914114363\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609873964805\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586262398065\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584930966727\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585530568103\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611119520178\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.460511777901\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614139074502\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580038083432\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612598856678\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598803741656\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574136949215\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591920245789\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586167209241\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285579288\n","Dataset 0 new STD: 0.015165952285579288\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015178  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015181  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015177  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 6 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588700201044\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603700448967\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596788465139\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564478271495\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566706565658\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607280701084\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577915442863\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609875293306\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586263726566\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584932295227\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585531896604\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611120848678\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605119107511\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614140403003\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580039411933\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612600185178\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598805070157\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574138277715\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459192157429\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586168537742\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285590503\n","Dataset 0 new STD: 0.015165952285590503\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015179  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015181  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015177  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 7 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588700550046\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603700797969\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596788814141\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564478620497\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.456670691466\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607281050086\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577915791865\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609875642308\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586264075568\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584932644229\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585532245606\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461112119768\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605119456513\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614140752005\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580039760935\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461260053418\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598805419159\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574138626717\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591921923292\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586168886744\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285583133\n","Dataset 0 new STD: 0.015165952285583132\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015179  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015181  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015177  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 8 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588700351765\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603700599688\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459678861586\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564478422216\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566706716379\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607280851805\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577915593584\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609875444026\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586263877287\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584932445948\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585532047324\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611120999399\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605119258232\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614140553724\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580039562654\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612600335899\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598805220878\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574138428436\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591921725011\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586168688463\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285601551\n","Dataset 0 new STD: 0.015165952285601551\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015179  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015181  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015177  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 9 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588699863695\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603700111618\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459678812779\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564477934147\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566706228309\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607280363735\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577915105515\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609874955957\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586263389217\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584931957879\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585531559255\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461112051133\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605118770162\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614140065654\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580039074585\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461259984783\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598804732808\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574137940367\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591921236942\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586168200393\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.01516595228558604\n","Dataset 0 new STD: 0.01516595228558604\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015385  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015206  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 10 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588699433294\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603699681217\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596787697389\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564477503746\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566705797908\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607279933334\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577914675114\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609874525556\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586262958816\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584931527478\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585531128854\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611120080929\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605118339761\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614139635253\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580038644183\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612599417429\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598804302407\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574137509966\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459192080654\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586167769992\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285584754\n","Dataset 0 new STD: 0.015165952285584754\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015386  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 11 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588699040864\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603699288786\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596787304959\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564477111315\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566705405477\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607279540904\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577914282683\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609874133125\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586262566385\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584931135047\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585530736423\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611119688498\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605117947331\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614139242823\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580038251753\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612599024998\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598803909976\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574137117535\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459192041411\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586167377561\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285588409\n","Dataset 0 new STD: 0.015165952285588409\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015385  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 12 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588698673436\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603698921359\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596786937531\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564476743887\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.456670503805\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607279173476\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577913915255\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609873765698\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586262198958\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584930767619\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585530368996\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461111932107\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605117579903\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614138875395\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580037884325\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461259865757\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598803542549\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574136750107\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591920046682\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586167010134\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285600666\n","Dataset 0 new STD: 0.015165952285600666\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015386  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 13 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588698322436\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603698570359\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596786586531\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564476392887\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566704687049\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607278822476\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577913564255\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609873414697\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586261847957\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584930416619\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585530017995\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461111897007\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605117228903\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614138524395\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580037533325\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461259830657\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598803191548\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574136399107\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591919695682\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586166659133\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285588636\n","Dataset 0 new STD: 0.015165952285588636\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015385  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 14 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588697982216\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603698230139\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596786246311\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564476052668\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.456670434683\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607278482256\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577913224036\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609873074478\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586261507738\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.45849300764\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585529677776\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611118629851\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605116888683\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614138184175\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580037193105\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612597966351\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598802851329\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574136058888\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591919355463\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586166318914\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.01516595228560363\n","Dataset 0 new STD: 0.01516595228560363\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015386  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 15 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588697649059\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603697896982\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596785913154\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.456447571951\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566704013673\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607278149099\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577912890878\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.460987274132\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586261174581\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584929743242\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585529344618\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611118296693\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605116555526\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614137851018\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580036859948\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612597633193\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598802518172\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.457413572573\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591919022305\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586165985757\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285571126\n","Dataset 0 new STD: 0.015165952285571126\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015386  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 16 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588697320525\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603697568448\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459678558462\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564475390977\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566703685139\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607277820565\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577912562345\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609872412787\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586260846047\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584929414709\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585529016085\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461111796816\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605116226992\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614137522484\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580036531414\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461259730466\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598802189638\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574135397197\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591918693772\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586165657223\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285587208\n","Dataset 0 new STD: 0.015165952285587208\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015385  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 17 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588696995023\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603697242946\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596785259118\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564475065474\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566703359637\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607277495063\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577912236842\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609872087284\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586260520545\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584929089206\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585528690582\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611117642657\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.460511590149\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614137196982\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580036205912\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612596979157\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598801864136\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574135071694\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591918368269\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.458616533172\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285578599\n","Dataset 0 new STD: 0.015165952285578599\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015385  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 18 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588696671514\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603696919437\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596784935609\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564474741965\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566703036128\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607277171554\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577911913333\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609871763775\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586260197036\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584928765697\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585528367073\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611117319148\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605115577981\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614136873473\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580035882403\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612596655648\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598801540626\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574134748185\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459191804476\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586165008211\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': -0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285613323\n","Dataset 0 new STD: 0.015165952285613323\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015385  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 19 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588696349325\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603696597248\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459678461342\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564474419776\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566702713939\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607276849365\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577911591144\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609871441587\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586259874847\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584928443508\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585528044885\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611116996959\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605115255792\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614136551284\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580035560214\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612596333459\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598801218438\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574134425996\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591917722571\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586164686023\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.01516595228560993\n","Dataset 0 new STD: 0.015165952285609928\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015385  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 20 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.458869602802\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603696275943\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596784292115\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564474098472\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566702392634\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.460727652806\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577911269839\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609871120282\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586259553542\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584928122204\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.458552772358\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611116675655\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605114934487\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614136229979\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580035238909\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612596012155\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598800897133\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574134104692\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591917401266\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586164364718\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285613564\n","Dataset 0 new STD: 0.015165952285613564\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015385  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Testing ======\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602720670981\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4561183206506\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580035238909\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611412144713\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4579827069849\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585051459794\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4593660200192\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4615559515409\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4568776855781\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4600042564116\n","10 systems found, adding energy\n","10 systems found, adding energy\n","{'mae': 0.00852, 'max': 0.02302, 'mean deviation': 0.0, 'rmse': 0.01112}\n","/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J9f6RekYHk5G","executionInfo":{"status":"ok","timestamp":1624727008314,"user_tz":-60,"elapsed":175,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"ac32a425-c872-4170-c797-88c2b39dee30"},"source":["cd ../train_model_20methane_pyscf_ccsdt/"],"execution_count":22,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3G_VwQTCHsqE","executionInfo":{"status":"ok","timestamp":1624729042587,"user_tz":-60,"elapsed":2034294,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"909ed143-8c70-4903-9a30-25c41a424761"},"source":["!sh train_20_CH4.sh"],"execution_count":23,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 5 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538582927\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.460253883085\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595626847022\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316653379\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565544947541\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119082967\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576753824747\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713675189\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102108449\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770677111\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370278487\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959230562\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957489394\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612978784887\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578877793816\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438567062\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459764345204\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976659599\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590759956174\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585006919625\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206088512\n","Dataset 0 new STD: 0.022127678206088516\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022137  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022138  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 6 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538475045\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602538722968\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459562673914\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316545496\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565544839658\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606118975085\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576753716864\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713567306\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102000567\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770569228\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370170604\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959122679\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957381512\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612978677004\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578877685934\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438459179\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643344158\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976551716\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590759848291\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585006811742\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.02212767820606867\n","Dataset 0 new STD: 0.02212767820606867\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022138  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 7 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538660396\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602538908319\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595626924491\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316730848\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.456554502501\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119160436\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576753902216\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713752658\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102185918\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.458377075458\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370355956\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959308031\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957566863\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612978862355\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578877871285\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438644531\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643529509\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976737068\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590760033642\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585006997094\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206094806\n","Dataset 0 new STD: 0.022127678206094806\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022140  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022136  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 8 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538537641\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602538785564\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595626801736\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316608092\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565544902255\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119037681\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.457675377946\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713629903\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102063163\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770631824\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370233201\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959185275\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957444108\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.46129787396\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.457887774853\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438521775\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643406754\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976614312\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590759910887\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585006874339\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206097283\n","Dataset 0 new STD: 0.022127678206097283\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022138  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 9 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538715246\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602538963169\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595626979341\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316785698\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.456554507986\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119215286\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576753957066\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713807508\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102240768\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.458377080943\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370410806\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959362881\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957621713\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612978917205\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578877926136\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438699381\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643584359\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976791918\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590760088493\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585007051944\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206087752\n","Dataset 0 new STD: 0.022127678206087752\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022140  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022136  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 10 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538588708\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602538836631\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595626852803\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316659159\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565544953321\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119088747\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576753830527\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713680969\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102114229\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770682891\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370284267\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959236342\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957495175\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612978790667\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578877799597\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438572842\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459764345782\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976665379\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590759961954\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585006925405\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206082493\n","Dataset 0 new STD: 0.022127678206082493\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022138  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 11 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538764424\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602539012347\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595627028519\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316834875\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565545129038\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119264464\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576754006243\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713856686\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102289946\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770858607\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370459984\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959412058\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957670891\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612978966383\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578877975313\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438748558\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643633537\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976841095\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459076013767\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585007101122\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206098334\n","Dataset 0 new STD: 0.022127678206098334\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022140  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022136  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 12 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538636846\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602538884769\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595626900941\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316707297\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.456554500146\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119136886\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576753878665\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713729108\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102162368\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770731029\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370332406\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.460995928448\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957543313\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612978838805\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578877847735\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461143862098\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643505959\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976713517\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590760010092\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585006973544\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206082118\n","Dataset 0 new STD: 0.022127678206082118\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022138  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 13 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538811989\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602539059912\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595627076085\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316882441\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565545176603\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119312029\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576754053809\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713904251\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102337511\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770906173\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370507549\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959459624\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957718457\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612979013949\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578878022879\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438796124\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643681102\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976888661\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590760185236\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585007148687\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206113447\n","Dataset 0 new STD: 0.022127678206113447\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022140  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022136  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 14 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538684172\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602538932095\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595626948267\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316754623\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565545048786\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119184212\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576753925991\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713776434\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102209694\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770778355\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370379732\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959331806\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957590639\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612978886131\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578877895061\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438668306\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643553285\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976760843\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590760057418\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.458500702087\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.02212767820607841\n","Dataset 0 new STD: 0.02212767820607841\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022138  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 15 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538859199\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602539107122\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595627123294\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.456331692965\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565545223813\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119359239\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576754101018\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.460871395146\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102384721\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770953382\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370554758\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959506833\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957765666\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612979061158\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578878070088\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438843333\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643728312\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.457297693587\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590760232445\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585007195897\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.02212767820606717\n","Dataset 0 new STD: 0.02212767820606717\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022140  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022136  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 16 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538731344\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602538979267\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595626995439\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316801795\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565545095958\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119231384\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576753973163\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713823605\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102256866\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770825527\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370426903\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959378978\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957637811\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612978933303\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578877942233\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438715478\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643600457\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976808015\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459076010459\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585007068042\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206079738\n","Dataset 0 new STD: 0.02212767820607974\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022138  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 17 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538906322\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602539154245\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595627170417\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316976773\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565545270935\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119406362\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576754148141\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713998583\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102431843\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583771000505\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370601881\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959553956\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957812789\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612979108281\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578878117211\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438890456\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643775435\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976982993\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590760279568\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585007243019\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206084772\n","Dataset 0 new STD: 0.022127678206084772\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022140  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022136  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 18 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538778413\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602539026336\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595627042508\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316848864\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565545143026\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119278452\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576754020232\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713870674\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102303934\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770872596\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370473972\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959426047\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.460395768488\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612978980372\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578877989302\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438762547\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643647525\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976855084\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590760151659\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.458500711511\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206097207\n","Dataset 0 new STD: 0.022127678206097207\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022138  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 19 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538953333\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602539201256\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595627217428\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563317023784\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565545317947\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119453373\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576754195152\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608714045595\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102478855\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583771047516\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370648893\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959600967\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.46039578598\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612979155292\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578878164222\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438937467\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643822446\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572977030004\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590760326579\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585007290031\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206113447\n","Dataset 0 new STD: 0.022127678206113444\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022140  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022136  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 20 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538825414\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602539073337\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595627089509\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316895866\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565545190028\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119325454\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576754067234\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713917676\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102350936\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770919598\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370520974\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959473049\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957731881\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612979027373\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578878036303\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438809549\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643694527\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976902086\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590760198661\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585007162112\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.02212767820610274\n","Dataset 0 new STD: 0.02212767820610274\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022138  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Testing ======\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4601563468375\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.45600260039\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578878036303\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4610254942107\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578669867243\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583894257188\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4592502997586\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614402312804\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4567619653176\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459888536151\n","10 systems found, adding energy\n","10 systems found, adding energy\n","{'mae': 0.01221, 'max': 0.03256, 'mean deviation': 0.0, 'rmse': 0.01608}\n","/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wALta_uxJz8R"},"source":["## Train model_50_20_merge\n","5 iter"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1l0hvC6MQAgI","executionInfo":{"status":"ok","timestamp":1623969537858,"user_tz":-60,"elapsed":347,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"a09b2da8-aca8-437f-844c-29064770efcd"},"source":["cd examples/example_scripts/train_model_50_20_merge/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"da2CESLSQ_Ty","executionInfo":{"status":"ok","timestamp":1623955240209,"user_tz":-60,"elapsed":193,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"c2bf8b66-933a-4e8d-ca05-bbce9af57a96"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["basis_sgdml_benzene.json  test_subset.csv          train_subset_propane.csv\n","hyperparameters.json      train_50_20.sh           \u001b[0m\u001b[01;34mworkdir\u001b[0m/\n","README.txt                train_subset_ethane.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6UYSjhAuQ__o","executionInfo":{"status":"ok","timestamp":1623959120903,"user_tz":-60,"elapsed":3877182,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"5fbce264-1df8-4a23-a781-7acc5e942787"},"source":["!sh train_50_20.sh "],"execution_count":null,"outputs":[{"output_type":"stream","text":["cp: -r not specified; omitting directory '../workdir'\n","using unit 0.04336410390059322\n","using unit 0.04336410390059322\n","/usr/local/lib/python3.7/dist-packages/ase/io/jsonio.py:58: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  a = np.array(obj)\n","using unit 0.04336410390059322\n","FILEPATH basis_sgdml_benzene.json\n","Traceback (most recent call last):\n","  File \"../../../scripts/fix_paths.py\", line 13, in <module>\n","    basis['engine_kwargs']['pseudoloc'] = os.path.abspath(basis['engine_kwargs']['pseudoloc'])\n","KeyError: 'pseudoloc'\n","====== Iteration 0 ======\n","converged SCF energy = -79.6844271138828\n","converged SCF energy = -79.6943500665332\n","converged SCF energy = -79.6868898453716\n","converged SCF energy = -79.6922359554811\n","converged SCF energy = -79.6865586039518\n","converged SCF energy = -79.6913095653311\n","converged SCF energy = -79.6937771392765\n","converged SCF energy = -79.6883665751506\n","converged SCF energy = -79.6908131651242\n","converged SCF energy = -79.6928989718341\n","converged SCF energy = -79.689887874575\n","converged SCF energy = -79.6935007220325\n","converged SCF energy = -79.6914501649869\n","converged SCF energy = -79.6881811353526\n","converged SCF energy = -79.6851843297105\n","converged SCF energy = -79.6846655122534\n","converged SCF energy = -79.69204076459\n","converged SCF energy = -79.6914734714862\n","converged SCF energy = -79.6897241902576\n","converged SCF energy = -79.6882188962681\n","converged SCF energy = -79.692173845558\n","converged SCF energy = -79.6855847560228\n","converged SCF energy = -79.6875161777395\n","converged SCF energy = -79.6920402683761\n","converged SCF energy = -79.6871181611662\n","converged SCF energy = -79.6830808966929\n","converged SCF energy = -79.6797909673712\n","converged SCF energy = -79.6888245411406\n","converged SCF energy = -79.6939602555435\n","converged SCF energy = -79.6924510308243\n","converged SCF energy = -79.6932460467215\n","converged SCF energy = -79.6833029688501\n","converged SCF energy = -79.685599311649\n","converged SCF energy = -79.6914625432481\n","converged SCF energy = -79.6944041042538\n","converged SCF energy = -79.692535068146\n","converged SCF energy = -79.6921868033789\n","converged SCF energy = -79.6889093916453\n","converged SCF energy = -79.6902240755121\n","converged SCF energy = -79.6924157415376\n","converged SCF energy = -79.6951037013665\n","converged SCF energy = -79.6864766988545\n","converged SCF energy = -79.6952561636311\n","converged SCF energy = -79.6870497694799\n","converged SCF energy = -79.6834160934014\n","converged SCF energy = -79.6887380420963\n","converged SCF energy = -79.6848380417075\n","converged SCF energy = -79.692599814125\n","converged SCF energy = -79.690091826451\n","converged SCF energy = -79.6909261579376\n","converged SCF energy = -118.940210536202\n","converged SCF energy = -118.940596594795\n","converged SCF energy = -118.937541724306\n","converged SCF energy = -118.952538068452\n","converged SCF energy = -118.942789383792\n","converged SCF energy = -118.94475157552\n","converged SCF energy = -118.947818500794\n","converged SCF energy = -118.941952877314\n","converged SCF energy = -118.946537584784\n","converged SCF energy = -118.940833957381\n","converged SCF energy = -118.946983174052\n","converged SCF energy = -118.950053476769\n","converged SCF energy = -118.936409631262\n","converged SCF energy = -118.947404363513\n","converged SCF energy = -118.949410728218\n","converged SCF energy = -118.942215468016\n","converged SCF energy = -118.939608592427\n","converged SCF energy = -118.947629121785\n","converged SCF energy = -118.941922685014\n","converged SCF energy = -118.945064132137\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.0934, 'mean deviation': 0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Epoch 0 ||  Training loss : 2.680926  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 2.270776  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 1.872122  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 1.483722  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 1.124191  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.826411  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.602909  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.425758  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.275676  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.157964  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.083823  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 110 ||  Training loss : 0.059576  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.057761  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 130 ||  Training loss : 0.057190  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 140 ||  Training loss : 0.055697  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 150 ||  Training loss : 0.054788  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 160 ||  Training loss : 0.054461  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 170 ||  Training loss : 0.054116  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 180 ||  Training loss : 0.053690  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 190 ||  Training loss : 0.053257  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 0.052861  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 210 ||  Training loss : 0.052506  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 220 ||  Training loss : 0.052187  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 230 ||  Training loss : 0.051896  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 240 ||  Training loss : 0.051626  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 250 ||  Training loss : 0.051372  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 260 ||  Training loss : 0.051132  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 270 ||  Training loss : 0.050907  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 280 ||  Training loss : 0.050695  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 290 ||  Training loss : 0.050497  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 0.050312  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 310 ||  Training loss : 0.050138  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 320 ||  Training loss : 0.049974  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 330 ||  Training loss : 0.049820  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 340 ||  Training loss : 0.049674  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 350 ||  Training loss : 0.049537  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 360 ||  Training loss : 0.049407  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 370 ||  Training loss : 0.049283  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 380 ||  Training loss : 0.049166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 390 ||  Training loss : 0.049055  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 400 ||  Training loss : 0.048949  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 410 ||  Training loss : 0.048848  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 420 ||  Training loss : 0.048751  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 430 ||  Training loss : 0.048659  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 440 ||  Training loss : 0.048570  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 450 ||  Training loss : 0.048485  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 460 ||  Training loss : 0.048404  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 470 ||  Training loss : 0.048325  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 480 ||  Training loss : 0.048250  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 490 ||  Training loss : 0.048177  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 500 ||  Training loss : 0.048107  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 510 ||  Training loss : 0.048039  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 520 ||  Training loss : 0.047973  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 530 ||  Training loss : 0.047910  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 540 ||  Training loss : 0.047848  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 550 ||  Training loss : 0.047789  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 560 ||  Training loss : 0.047731  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 570 ||  Training loss : 0.047675  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 580 ||  Training loss : 0.047621  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 590 ||  Training loss : 0.047569  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 600 ||  Training loss : 0.047518  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 610 ||  Training loss : 0.047468  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 620 ||  Training loss : 0.047420  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 630 ||  Training loss : 0.047373  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 640 ||  Training loss : 0.047327  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 650 ||  Training loss : 0.047283  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 660 ||  Training loss : 0.047240  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 670 ||  Training loss : 0.047198  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 680 ||  Training loss : 0.047158  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 690 ||  Training loss : 0.047118  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 700 ||  Training loss : 0.047079  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 710 ||  Training loss : 0.047042  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 720 ||  Training loss : 0.047005  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 730 ||  Training loss : 0.046970  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 740 ||  Training loss : 0.046935  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 750 ||  Training loss : 0.046901  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 760 ||  Training loss : 0.046869  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 770 ||  Training loss : 0.046837  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 780 ||  Training loss : 0.046806  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 790 ||  Training loss : 0.046776  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 800 ||  Training loss : 0.046746  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 810 ||  Training loss : 0.046717  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 820 ||  Training loss : 0.046690  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 830 ||  Training loss : 0.046662  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 840 ||  Training loss : 0.046636  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 850 ||  Training loss : 0.046610  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 860 ||  Training loss : 0.046585  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 870 ||  Training loss : 0.046560  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 880 ||  Training loss : 0.046537  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 890 ||  Training loss : 0.046513  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 900 ||  Training loss : 0.046491  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 910 ||  Training loss : 0.046468  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 920 ||  Training loss : 0.046447  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 930 ||  Training loss : 0.046426  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 940 ||  Training loss : 0.046405  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 950 ||  Training loss : 0.046385  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 960 ||  Training loss : 0.046365  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 970 ||  Training loss : 0.046345  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 980 ||  Training loss : 0.046326  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 990 ||  Training loss : 0.046308  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.046290  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1010 ||  Training loss : 0.046272  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1020 ||  Training loss : 0.046254  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1030 ||  Training loss : 0.046237  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1040 ||  Training loss : 0.046220  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1050 ||  Training loss : 0.046203  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1060 ||  Training loss : 0.046187  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1070 ||  Training loss : 0.046170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1080 ||  Training loss : 0.046154  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1090 ||  Training loss : 0.046139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1100 ||  Training loss : 0.046123  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1110 ||  Training loss : 0.046108  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1120 ||  Training loss : 0.046093  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1130 ||  Training loss : 0.046078  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1140 ||  Training loss : 0.046063  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1150 ||  Training loss : 0.046048  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1160 ||  Training loss : 0.046033  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1170 ||  Training loss : 0.046019  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1180 ||  Training loss : 0.046004  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1190 ||  Training loss : 0.045990  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1200 ||  Training loss : 0.045976  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1210 ||  Training loss : 0.045962  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1220 ||  Training loss : 0.045948  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1230 ||  Training loss : 0.045934  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1240 ||  Training loss : 0.045920  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1250 ||  Training loss : 0.045907  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1260 ||  Training loss : 0.045893  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1270 ||  Training loss : 0.045879  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1280 ||  Training loss : 0.045866  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1290 ||  Training loss : 0.045852  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1300 ||  Training loss : 0.045839  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1310 ||  Training loss : 0.045825  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1320 ||  Training loss : 0.045812  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1330 ||  Training loss : 0.045798  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1340 ||  Training loss : 0.045785  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1350 ||  Training loss : 0.045772  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1360 ||  Training loss : 0.045759  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1370 ||  Training loss : 0.045745  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1380 ||  Training loss : 0.045732  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1390 ||  Training loss : 0.045719  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1400 ||  Training loss : 0.045706  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1410 ||  Training loss : 0.045693  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1420 ||  Training loss : 0.045680  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1430 ||  Training loss : 0.045667  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1440 ||  Training loss : 0.045654  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1450 ||  Training loss : 0.045641  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1460 ||  Training loss : 0.045628  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1470 ||  Training loss : 0.045615  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1480 ||  Training loss : 0.045603  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1490 ||  Training loss : 0.045590  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1500 ||  Training loss : 0.045577  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1510 ||  Training loss : 0.045565  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1520 ||  Training loss : 0.045552  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1530 ||  Training loss : 0.045539  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1540 ||  Training loss : 0.045527  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1550 ||  Training loss : 0.045515  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1560 ||  Training loss : 0.045502  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1570 ||  Training loss : 0.045490  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1580 ||  Training loss : 0.045478  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1590 ||  Training loss : 0.045466  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1600 ||  Training loss : 0.045454  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1610 ||  Training loss : 0.045442  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1620 ||  Training loss : 0.045430  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1630 ||  Training loss : 0.045418  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1640 ||  Training loss : 0.045406  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1650 ||  Training loss : 0.045395  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1660 ||  Training loss : 0.045383  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1670 ||  Training loss : 0.045372  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1680 ||  Training loss : 0.045360  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1690 ||  Training loss : 0.045349  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1700 ||  Training loss : 0.045338  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1710 ||  Training loss : 0.045327  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1720 ||  Training loss : 0.045316  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1730 ||  Training loss : 0.045306  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1740 ||  Training loss : 0.045295  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1750 ||  Training loss : 0.045284  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1760 ||  Training loss : 0.045274  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1770 ||  Training loss : 0.045264  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1780 ||  Training loss : 0.045254  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1790 ||  Training loss : 0.045244  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1800 ||  Training loss : 0.045234  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1810 ||  Training loss : 0.045224  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1820 ||  Training loss : 0.045215  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1830 ||  Training loss : 0.045205  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1840 ||  Training loss : 0.045196  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1850 ||  Training loss : 0.045187  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1860 ||  Training loss : 0.045178  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1870 ||  Training loss : 0.045169  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1880 ||  Training loss : 0.045160  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1890 ||  Training loss : 0.045152  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1900 ||  Training loss : 0.045143  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1910 ||  Training loss : 0.045135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1920 ||  Training loss : 0.045127  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1930 ||  Training loss : 0.045118  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1940 ||  Training loss : 0.045110  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1950 ||  Training loss : 0.045103  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1960 ||  Training loss : 0.045095  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1970 ||  Training loss : 0.045087  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1980 ||  Training loss : 0.045080  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1990 ||  Training loss : 0.045073  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.045065  Validation loss : 0.000000  Learning rate: 0.001\n","1\n","====== Iteration 1 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","Overwritten attributes  get_veff  of <class 'pyscf.dft.rks.RKS'>\n","converged SCF energy = -79.6812716297853\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912453311934\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6835007105281\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6892065022271\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6831946134441\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6882167869532\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6904986810002\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6850628909231\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.687975960365\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896813734741\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6867016753223\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6906238506668\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881421902778\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6849496530413\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6820993544199\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6814414344865\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6891030732183\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883700992507\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.686534013248\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851722817418\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6889675525757\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6824298006197\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6844150072878\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.689132682872\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6837488293911\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6798272020352\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6764514408521\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.685651980716\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908166485835\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6893171669734\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900806008642\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6799096885021\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6824146482864\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6884614891927\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913134027128\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6891062991422\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6890782579071\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6858008147691\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.687017572717\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6893848748087\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.691998385335\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832521816968\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921153040746\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6839240098357\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6800836716774\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6853480775726\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6818242520907\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6895843793963\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6867241942039\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6880250166799\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937793189321\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938002598851\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.935035931238\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950375228961\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940295823969\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942324305585\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945733129722\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939350739032\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944021044295\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938543584423\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944814397965\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947581631453\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.93393384405\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945116377885\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946955571101\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939762935281\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937423680917\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945316150339\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.93948981043\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942616317023\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03685, 'max': 0.10222, 'mean deviation': 0.0, 'rmse': 0.04758}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.047577617986875875\n","Dataset 0 new STD: 0.047592881410371546\n","1\n","Epoch 0 ||  Training loss : 0.047578  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.047681  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.048225  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.048018  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.048166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.048277  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.048298  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.048313  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.048275  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.048228  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.048166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.048099  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.048088  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.048080  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.048074  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.048067  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.048060  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.048053  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.048046  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.048039  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.048032  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.048025  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.048017  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.048016  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.048015  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.048014  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.048014  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.048013  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.048012  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.048011  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.048010  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.048010  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.048009  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.048008  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.048008  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.048008  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.048008  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 2 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6838610371986\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6937919104537\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6862986023629\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.691700386665\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6859407662889\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907649983368\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931884879377\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6877817696719\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6903113563159\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923305284307\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6893295641027\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6929916080063\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908752257897\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6875923680701\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.684642445676\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6840839012952\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6915173463633\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6909280924784\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6891606534753\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.687672402861\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6916139749216\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6850202111854\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6869581571817\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6915365268246\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6865165432497\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6824953889566\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.679170474316\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6882516942286\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6933988984345\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6918735984291\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6926828250517\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.682703006381\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6850201721267\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6909429595503\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6938580705919\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919200123835\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6916350903333\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883568372254\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896646412958\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6918864554105\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6945435710425\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6858811905096\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6946890664729\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864767144849\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6828023071983\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881356896947\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843049661659\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6920772374419\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6894727923508\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6904106504746\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940873555473\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941189502372\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938179255322\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.953241152291\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943432652327\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945412299793\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948547034577\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942596647224\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947173510802\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941511074496\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947697770492\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950702499396\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937082290615\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948086067491\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.95005955284\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942877787396\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940339252094\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948292879731\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942558709363\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945690321054\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03451, 'max': 0.0951, 'mean deviation': 0.0, 'rmse': 0.04464}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.04464456723144677\n","Dataset 0 new STD: 0.04418924376315862\n","1\n","Epoch 0 ||  Training loss : 0.044645  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.044687  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.044578  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.044403  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.044352  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.044260  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.044245  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.044210  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.044204  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.044193  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.044187  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 110 ||  Training loss : 0.044186  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.044185  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 130 ||  Training loss : 0.044185  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 140 ||  Training loss : 0.044185  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 150 ||  Training loss : 0.044186  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 160 ||  Training loss : 0.044187  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 170 ||  Training loss : 0.044187  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 180 ||  Training loss : 0.044188  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 190 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    22: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 210 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 220 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 240 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 250 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 260 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 270 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 280 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 290 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 300 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 310 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 320 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 330 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 350 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 360 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 370 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 380 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 390 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 400 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 410 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 420 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 430 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 440 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 460 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 470 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 480 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 490 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 500 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 510 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 520 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 530 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 540 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 550 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 3 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843527217307\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6942756810433\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868155816844\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921614886416\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864844096036\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912351615584\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6937028394271\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6882922899657\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907386071671\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928246096418\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898135016208\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6934261682907\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913758485509\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881068375634\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851099065263\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6845912132908\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919662515777\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913990527313\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896498089138\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881444726569\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6920994567401\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855104110628\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6874417987186\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919657155956\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870439439208\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6830066121196\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.679716791104\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887501781028\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6938858839873\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923766912729\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931716868966\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832287223916\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855249570799\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913880599623\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943296780822\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924608634939\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921123923741\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888349951019\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901497014926\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923412773779\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950292887635\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.68640240799\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6951818035442\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.686975435542\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6833418925339\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6886638065288\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847635958803\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6925253325161\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900176466665\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908516295634\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940983366698\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94136963213\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938314622556\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.953310745286\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943562229995\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945524384449\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948591107389\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942725749194\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947310467137\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941606728946\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947755833759\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950826290365\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937182416595\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948177096843\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950183585538\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942988282781\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940381187574\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948401941081\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942695529171\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945837047405\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.09339, 'mean deviation': 0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.04379871968073516\n","Dataset 0 new STD: 0.043800787548092815\n","1\n","Epoch 0 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.043816  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043976  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043806  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043806  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043804  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043804  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043805  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 4 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843425405033\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6942654936122\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868052917855\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921513691859\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864740626569\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912249887229\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6936925807988\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6882820181401\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907285634617\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928144030304\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898033033999\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.693416121423\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913656029291\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6880965771552\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6850997500279\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6845809531861\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919561707501\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913888928329\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896396181834\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881343168628\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6920892724293\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855001892197\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6874316057927\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919556676134\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870336152473\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6829963401081\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.679706428648\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.688739972233\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6938756848418\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923664657487\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931614778374\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832184185544\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855147442988\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913779536117\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943195246437\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924505251822\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921022263729\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888248165893\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901395042718\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923311552469\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950191245711\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6863921420245\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6951715949283\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.686965204935\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6833315507048\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6886534936271\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847534583177\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6925152249014\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900072872224\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908415613827\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940780936352\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941167029872\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938112136233\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.95310844416\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943359787618\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945321973102\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.9483888647\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942523285173\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947107994666\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941404348414\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947553546452\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950623876004\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936980024494\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947974748732\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949981133509\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942785866042\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940178954367\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948199520684\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942493089392\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945634547488\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.09339, 'mean deviation': 0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.04380096252265617\n","Dataset 0 new STD: 0.043801291336320713\n","1\n","Epoch 0 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.043820  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043908  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043816  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043811  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043804  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043805  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 5 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843357053692\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6942586580617\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6867984387121\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.692144545755\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864671984466\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912181565113\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6936857321601\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6882751681697\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907217539555\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928075637496\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6897964662663\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6934093109639\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913587575333\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6880897282621\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6850929206032\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6845741050784\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919493541563\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913820624755\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896327818648\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881274871869\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.692082437068\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6854933481233\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.687424769358\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919488572917\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870267552247\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6829894897506\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6796995621043\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887331330453\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6938688472803\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923596230888\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931546386284\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832115624991\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855079036996\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913711332064\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943126951545\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924436624843\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6920953945244\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888179829723\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901326671974\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923243318111\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950122925331\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6863852918923\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6951647555554\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.686958361793\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6833246877637\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6886466359172\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847466322506\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6925084041234\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900004211386\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908347472486\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940718725206\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941104787077\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938049914415\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.953046255166\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943297573144\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945259764286\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.9483266864\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942461067045\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947045774705\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94134214553\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947491360454\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950561665692\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.93691781962\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947912551119\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949918917707\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942723656822\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940116777847\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948137310673\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942430874417\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945572322571\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.0934, 'mean deviation': 0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.04380136512623971\n","Dataset 0 new STD: 0.04380139585992881\n","1\n","Epoch 0 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.043880  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043914  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043805  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043813  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043805  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Testing ======\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.214619119776\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.202520364397\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.19105872461\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200529817863\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197271634115\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199310621856\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.202694264046\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.202094996676\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.193329792283\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197272899444\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195681803165\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.188614424961\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195026568081\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197641779024\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199107742761\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200163344057\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.201620230101\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200995380841\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200661970638\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.201233482256\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.202826658385\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200180680842\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.189423537622\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.205524543323\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198550010241\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197680342397\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.19693776976\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.203653076091\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199818949394\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.190732860174\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.203454937304\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197004634714\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197547595131\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198231385276\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.194964500666\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.192071222673\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197734765683\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196431524428\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.203255511211\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197404387413\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197278382517\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198884645603\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.205499275971\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.19908221208\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.202215841394\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.192911986343\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.190023637083\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198366587003\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198754681553\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.191106406327\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.184307966878\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.192196394547\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200555633809\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199240034957\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199698038306\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198928406349\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197538355604\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195246672638\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.201242499341\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.188778641125\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195060954008\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195669063547\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.20173836338\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197053782564\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196664039262\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197282542818\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197648161019\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.190334125712\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195870456866\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.184246590679\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.19020010394\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.194794855907\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.193481471028\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.194886168076\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.187795619613\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196746356824\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.191282383891\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198625106181\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195712906278\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.192216623157\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196249680239\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.192384217237\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196455654053\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.193558790354\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.188503273039\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198468738459\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.203384195472\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199267219296\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197262758823\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200650388121\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.192992166159\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.203213104738\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.190816465164\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200067404755\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.193570108451\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.19537610633\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.20131587921\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195866288769\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199471928795\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.201323765668\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198333370854\n","101 systems found, adding energy\n","101 systems found, adding energy\n","{'mae': 0.04258, 'max': 0.15613, 'mean deviation': 0.0, 'rmse': 0.05406}\n","/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wISksOGPJn8E"},"source":["## Train model_50_20_merge_2\n","10 iter"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WPZrXlpEHBXM","executionInfo":{"status":"ok","timestamp":1623969543893,"user_tz":-60,"elapsed":265,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"82d2e727-1c5a-4806-ad48-2670f76d64b0"},"source":["cd .."],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dNbOgwnUILL2","executionInfo":{"status":"ok","timestamp":1623969546566,"user_tz":-60,"elapsed":299,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"8ab10295-d6ac-477f-8c1f-7ad47be40941"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mtrain_model\u001b[0m/  \u001b[01;34mtrain_model_50\u001b[0m/  \u001b[01;34mtrain_model_50_20\u001b[0m/  \u001b[01;34mtrain_model_50_20_merge\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0K7EZV5lIL63"},"source":["cp -r train_model_50_20_merge train_model_50_20_merge_2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"db_NEX_jIPZu","executionInfo":{"status":"ok","timestamp":1623969726423,"user_tz":-60,"elapsed":5,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"bc567b93-9078-4773-d1c7-066e1e853849"},"source":["cd train_model_50_20_merge_2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C8jbpOVeISAz","executionInfo":{"status":"ok","timestamp":1623969902036,"user_tz":-60,"elapsed":277,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"7d21ff19-b6fd-4754-fb30-0b5999ba16bf"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["basis_sgdml_benzene.json  test_subset.csv          train_subset_propane.csv\n","hyperparameters.json      train_50_20.sh           \u001b[0m\u001b[01;34mworkdir\u001b[0m/\n","README.txt                train_subset_ethane.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qntl_k0jJitQ","executionInfo":{"status":"ok","timestamp":1623976608935,"user_tz":-60,"elapsed":3677671,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"a8b6ad66-5a23-470b-db37-ce2ba5fe2626"},"source":["!sh train_50_20.sh"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843901786287\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943131325923\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868532710639\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921987393529\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6865223187686\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912725252443\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6937404928121\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883299368556\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907756327691\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928621282724\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898509542565\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6934632192027\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914134244329\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881445014032\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851472517751\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6846288156667\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6920033939253\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914363885627\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.689687263217\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881818094088\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921369109771\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.685547919239\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.687479250341\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6920027454098\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870817407899\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6830442441916\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6797547016271\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887876708721\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6939233587864\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924142639647\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6932091832304\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832664689377\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855624766575\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914252401629\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943670191201\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924987323248\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921497808108\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888723971613\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901871591921\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923785154879\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950666877945\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864400962013\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6952193118395\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870130125466\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.683379774478\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887015875177\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.684800856978\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.692562537359\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900555679169\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908887267184\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940443021329\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940829834491\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937774480583\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.95277011829\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943021998709\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944984060124\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948050305589\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942185584775\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94677033015\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941066314145\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94721514576\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950286033143\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936642023051\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947636604258\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949643364796\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94244794503\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939840346829\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947861626816\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94215535105\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945296995054\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03385, 'max': 0.09338, 'mean deviation': 0.0, 'rmse': 0.04379}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.04379236657704381\n","Dataset 0 new STD: 0.04379834998622868\n","1\n","Epoch 0 ||  Training loss : 0.043792  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.044228  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043807  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043814  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043812  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043812  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 3 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843789474098\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943019000558\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868416790647\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921877888843\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6865104377652\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912613988159\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6937289729294\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883184088129\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907649983892\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928508053954\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898397081111\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.693452555308\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914019986025\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881329690129\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851361631744\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6846173459018\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919925979266\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914253049543\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896760237894\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881707297336\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921256790802\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855365896017\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6874680112759\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919921016464\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870699949289\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6830327303649\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6797428012101\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887763747026\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6939120890837\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924028644222\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931978802791\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832548025782\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855511452283\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.691414376616\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943559377146\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924869019372\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921386368664\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888612251497\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901759090478\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923675749375\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950555348575\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864285325305\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6952079971911\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870016030847\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6833679272039\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6886898758386\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847898751363\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6925516474984\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900436602786\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908779912483\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940333856778\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940719915692\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937665044993\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.952661388811\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942912704411\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944874896078\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947941821048\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942076197967\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946660905455\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940957277885\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94710649438\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950176797346\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936532951782\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947527683962\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949534048842\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942338788578\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939731912666\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947752442352\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942046005648\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94518745286\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.0934, 'mean deviation': 0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.043801402783190405\n","Dataset 0 new STD: 0.04380140593726139\n","1\n","Epoch 0 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.043825  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043811  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043831  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043808  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043804  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 4 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843762545217\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6942992071719\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868389860112\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921850961195\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6865077445917\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912587059698\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6937262799159\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883157157902\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907623057623\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928481124728\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898370152142\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6934498626705\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913993056262\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881302759917\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851334703494\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6846146528923\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919899052282\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914226121255\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896733308966\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881680369067\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921229861967\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855338966621\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6874653183786\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919894090137\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870673018058\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6830300373322\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.679740108011\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887736817794\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6939093961824\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924001714629\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931951873609\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832521094896\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855484522878\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914116838864\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943532448925\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924842087858\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921359440176\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888585322842\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901732161507\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923648821761\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950528420055\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864258394936\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6952053042701\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6869989101186\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6833652340411\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6886871827362\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847871823455\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6925489547634\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900409670909\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908752985753\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940443701232\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940829759825\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937774889335\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.95277123348\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943022548821\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944984740549\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948051665822\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942186042345\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946770749813\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94106712241\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94721633908\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950286641797\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936642796291\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947637528542\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949643893247\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942448633046\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939841757454\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947862286814\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942155850043\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945297297167\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.0934, 'mean deviation': 0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.043801406788502875\n","Dataset 0 new STD: 0.04380140680001133\n","1\n","Epoch 0 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.044389  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.044017  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043888  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043833  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043809  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 5 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843731560174\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6942961086678\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868358875062\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921819976154\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6865046460864\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912556074656\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.693723181411\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.688312617285\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907592072582\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928450139687\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898339167095\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.693446764167\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913962071213\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881271774872\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851303718451\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6846115543878\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919868067242\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914195136206\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896702323918\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881649384025\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921198876924\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855307981574\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6874622198743\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919863105105\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870642033006\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6830269388274\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6797370095056\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887705832749\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6939062976778\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923970729584\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931920888559\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832490109844\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855453537834\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914085853822\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943501463885\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924811102803\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921328455131\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888554337798\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901701176466\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923617836721\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950497435013\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.686422740989\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6952022057657\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6869958116141\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6833621355361\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6886840842308\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847840838418\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6925458562593\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900378685855\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908722000717\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940525919888\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940911978481\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937857107992\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.952853452138\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943104767477\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945066959206\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948133884479\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942268261001\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94685296847\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941149341066\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947298557738\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950368860456\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936725014949\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947719747199\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949726111904\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942530851703\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939923976113\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94794450547\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942238068699\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945379515823\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.0934, 'mean deviation': -0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.04380140680070295\n","Dataset 0 new STD: 0.04380140680177187\n","1\n","Epoch 0 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.044043  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043830  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043806  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 6 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843715574711\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6942945101214\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868342889599\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921803990695\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.68650304754\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912540089194\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6937215828648\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883110187388\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907576087125\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928434154224\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898323181634\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6934451656206\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913946085749\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881255789406\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851287732987\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6846099558416\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919852081782\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914179150744\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896686338457\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881633398563\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921182891461\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855291996112\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6874606213277\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919847119643\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870626047541\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.683025340281\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6797354109591\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887689847288\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6939046991314\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923954744123\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931904903099\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832474124381\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855437552371\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914069868363\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943485478421\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924795117339\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.692131246967\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888538352336\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901685191002\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923601851259\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950481449549\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.686421142443\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.695200607219\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6869942130679\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6833605369896\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6886824856844\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847824852957\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6925442577132\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900362700391\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908706015257\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940596230529\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940982289121\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937927418632\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.95292376278\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943175078118\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945137269846\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948204195121\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942338571641\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946923279111\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941219651708\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947368868379\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950439171096\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936795325589\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94779005784\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949796422546\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942601162344\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939994286753\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948014816112\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94230837934\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945449826464\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.0934, 'mean deviation': -0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.043801406804574235\n","Dataset 0 new STD: 0.043801406804577774\n","1\n","Epoch 0 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.043865  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043818  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043815  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043804  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 7 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843709125548\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.694293865205\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868336440438\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921797541529\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6865024026236\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912533640031\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6937209379485\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883103738228\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907569637958\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.692842770506\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898316732469\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6934445207044\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913939636586\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881249340243\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851281283825\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6846093109253\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919845632619\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914172701579\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896679889293\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.68816269494\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921176442296\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855285546947\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6874599764116\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.691984067048\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870619598381\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.683024695365\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6797347660428\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887683398123\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6939040542152\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923948294957\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931898453934\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832467675218\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855431103208\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914063419198\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943479029257\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924788668177\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921306020509\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888531903171\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901678741842\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923595402096\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950475000387\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864204975266\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6951999623029\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6869935681515\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6833598920734\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6886818407682\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847818403795\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6925436127969\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900356251227\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908699566091\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940613181815\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940999240407\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937944369918\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.952940714065\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943192029404\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945154221132\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948221146407\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942355522927\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946940230396\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941236602993\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947385819664\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950456122381\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936812276875\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947807009125\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949813373831\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94261811363\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940011238039\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948031767397\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942325330626\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945466777749\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.0934, 'mean deviation': -0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.043801406802590724\n","Dataset 0 new STD: 0.043801406802590724\n","1\n","Epoch 0 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.043894  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043806  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043811  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043805  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 8 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843706528489\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6942936054991\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868333843379\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.692179494447\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6865021429177\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912531042972\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6937206782426\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883101141169\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907567040899\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928425108001\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.689831413541\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6934442609985\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913937039527\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881246743184\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851278686765\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6846090512194\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.691984303556\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.691417010452\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896677292234\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881624352341\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921173845237\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855282949888\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6874597167057\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919838073421\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870617001322\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6830244356591\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6797345063369\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887680801064\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6939037945093\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923945697898\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931895856875\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832465078159\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855428506149\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914060822139\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943476432198\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924786071118\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.692130342345\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888529306112\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901676144783\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923592805037\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950472403328\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864202378207\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.695199702597\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6869933084456\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6833596323675\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6886815810623\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847815806736\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.692543353091\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900353654168\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908696969032\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940567389546\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940953448138\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937898577648\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.952894921796\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943146237135\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945108428863\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948175354137\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942309730658\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946894438127\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941190810724\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947340027395\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950410330112\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936766484606\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947761216856\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949767581561\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94257232136\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.93996544577\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947985975128\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942279538357\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94542098548\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.0934, 'mean deviation': 0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.04380140680248595\n","Dataset 0 new STD: 0.04380140680248595\n","1\n","Epoch 0 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.043922  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043827  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043807  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 9 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843704535956\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6942934062458\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868331850845\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921792951937\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6865019436644\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912529050439\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6937204789893\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883099148635\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907565048365\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928423115467\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898312142876\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6934440617452\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913935046993\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881244750651\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851276694231\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6846088519661\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919841043026\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914168111987\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896675299701\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881622359808\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921171852704\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855280957355\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6874595174524\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919836080888\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870615008788\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6830242364058\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6797343070835\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887678808531\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.693903595256\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923943705364\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931893864342\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832463085625\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855426513615\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914058829605\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943474439664\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924784078585\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921301430917\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888527313578\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901674152249\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923590812503\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950470410795\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864200385674\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6951995033436\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6869931091922\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6833594331141\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.688681381809\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847813814203\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6925431538376\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900351661635\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908694976499\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940509275589\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940895334182\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937840463692\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.952836807839\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943088123178\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945050314907\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948117240181\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942251616701\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94683632417\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941132696767\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947281913439\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950352216155\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936708370649\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947703102899\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949709467605\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942514207404\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939907331813\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947927861171\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.9422214244\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945362871523\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.0934, 'mean deviation': -0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.043801406802415316\n","Dataset 0 new STD: 0.043801406802415316\n","1\n","Epoch 0 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.043916  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043833  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043807  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 10 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843702813014\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6942932339516\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868330127903\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921791228995\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6865017713702\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912527327497\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6937203066951\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883097425694\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907563325424\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928421392526\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898310419934\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.693443889451\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913933324052\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881243027709\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.685127497129\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6846086796719\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919839320084\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914166389045\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896673576759\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881620636866\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921170129762\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855279234413\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6874593451582\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919834357946\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870613285847\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6830240641116\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6797341347894\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887677085589\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6939034229618\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923941982423\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.69318921414\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832461362683\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855424790674\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914057106664\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943472716723\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924782355643\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921299707975\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888525590636\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901672429308\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923589089562\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950468687853\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864198662732\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6951993310495\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6869929368981\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.68335926082\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6886812095148\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847812091261\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6925429815434\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900349938693\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908693253557\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940456491952\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940842550545\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937787680055\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.952784024202\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943035339542\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94499753127\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948064456544\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942198833064\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946783540534\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941079913131\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947229129802\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950299432518\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936655587012\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947650319262\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949656683968\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942461423767\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939854548176\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947875077535\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942168640763\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945310087887\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.0934, 'mean deviation': 0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.04380140680248962\n","Dataset 0 new STD: 0.04380140680248962\n","1\n","Epoch 0 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.043916  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043832  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043807  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Testing ======\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.214060080596\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.201961322321\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.190499683485\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199970777441\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196712592222\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198751579621\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.2021352214\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.201535957962\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.1927707477\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196713858838\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195122766934\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.188055378265\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.194467530015\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197082738283\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.1985486987\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199604305337\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.20106118844\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200436341545\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200102928732\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200674441064\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.202267617935\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199621639979\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.188864495744\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.204965502341\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.19799096975\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197121299826\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196378727668\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.203094035709\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199259908349\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.190173818813\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.202895895286\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196445593709\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196988555348\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197672343793\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.194405460877\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.191512180973\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197175724545\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.19587248455\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.202696468782\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196845348495\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196719336342\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198325608003\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.204940232235\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198523169831\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.201656802673\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.19235294212\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.1894645972\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197807546336\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198195640204\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.190547362328\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.183748922124\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.191637355666\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199996593098\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198680992352\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199138999001\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198369363862\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.1969793148\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.194687631836\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200683459305\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.188219599063\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.194501913404\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195110024966\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.201179321317\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196494742577\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196105000307\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196723499759\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.19708912353\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.1897750811\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195311419545\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.183687547957\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.189641061897\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.194235815082\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.192922428473\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.194327128069\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.18723657472\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196187317675\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.190723340199\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198066066666\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195153866524\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.191657581261\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195690640406\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.191825175017\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195896612778\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.192999750663\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.187944230277\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197909698137\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.202825155119\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198708177025\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196703717109\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200091348243\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.192433124356\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.202654065048\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.190257424619\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199508362587\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.193011069803\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.194817062622\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200756838189\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195307249485\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.19891288563\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200764725996\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197774329638\n","101 systems found, adding energy\n","101 systems found, adding energy\n","{'mae': 0.04258, 'max': 0.15613, 'mean deviation': 0.0, 'rmse': 0.05406}\n","/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hr50rTK3Jmno"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"33tiXCx3R30Q"},"source":["## Train on 20 propane and test on the same 20 propanes"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ua1sOAS6SDkd","executionInfo":{"status":"ok","timestamp":1624402916402,"user_tz":-60,"elapsed":266,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"0ce8da54-d207-4d18-a894-b1f7009d7b2d"},"source":["cd examples/example_scripts"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[Errno 2] No such file or directory: 'examples/example_scripts/'\n","/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qw9NU6r8Taj8","executionInfo":{"status":"ok","timestamp":1624402926952,"user_tz":-60,"elapsed":281,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"8c896052-c847-4af5-f093-044918035f08"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mtrain_model\u001b[0m/             \u001b[01;34mtrain_model_50\u001b[0m/     \u001b[01;34mtrain_model_50_20_merge\u001b[0m/\n","\u001b[01;34mtrain_model_20_propane\u001b[0m/  \u001b[01;34mtrain_model_50_20\u001b[0m/  \u001b[01;34mtrain_model_50_20_merge_2\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aslsYBBYTlOZ","executionInfo":{"status":"ok","timestamp":1624402938623,"user_tz":-60,"elapsed":274,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"5dd4f0c3-51a1-4ce5-97ab-a394c4e035d1"},"source":["cd train_model_20_propane/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-xQ772NiU0aO","executionInfo":{"status":"ok","timestamp":1624310086906,"user_tz":-60,"elapsed":1636099,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"5b50bb0a-37b1-4c25-83da-31008aa97d29"},"source":["!sh train_20_propane.sh"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cp: -r not specified; omitting directory '../workdir'\n","using unit 0.04336410390059322\n","using unit 0.04336410390059322\n","FILEPATH basis_sgdml_benzene.json\n","Traceback (most recent call last):\n","  File \"../../../scripts/fix_paths.py\", line 13, in <module>\n","    basis['engine_kwargs']['pseudoloc'] = os.path.abspath(basis['engine_kwargs']['pseudoloc'])\n","KeyError: 'pseudoloc'\n","====== Iteration 0 ======\n","converged SCF energy = -118.940210536202\n","converged SCF energy = -118.940596594795\n","converged SCF energy = -118.937541724306\n","converged SCF energy = -118.952538068452\n","converged SCF energy = -118.942789383792\n","converged SCF energy = -118.94475157552\n","converged SCF energy = -118.947818500794\n","converged SCF energy = -118.941952877314\n","converged SCF energy = -118.946537584784\n","converged SCF energy = -118.940833957381\n","converged SCF energy = -118.946983174052\n","converged SCF energy = -118.950053476769\n","converged SCF energy = -118.936409631262\n","converged SCF energy = -118.947404363513\n","converged SCF energy = -118.949410728218\n","converged SCF energy = -118.942215468016\n","converged SCF energy = -118.939608592427\n","converged SCF energy = -118.947629121785\n","converged SCF energy = -118.941922685014\n","converged SCF energy = -118.945064132137\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","20 systems found, adding energy\n","{'mae': 0.03427, 'max': 0.08926, 'mean deviation': -0.0, 'rmse': 0.04418}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","Fitting 3 folds for each of 1 candidates, totalling 3 fits\n","[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","1\n","Epoch 0 ||  Training loss : 0.956606  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.599758  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.262772  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.059898  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.114419  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.066847  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.033646  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.037237  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.027246  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022222  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.020715  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 110 ||  Training loss : 0.019002  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.018209  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 130 ||  Training loss : 0.017395  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 140 ||  Training loss : 0.016769  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 150 ||  Training loss : 0.016386  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 160 ||  Training loss : 0.016143  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 170 ||  Training loss : 0.015968  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 180 ||  Training loss : 0.015836  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 190 ||  Training loss : 0.015752  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 0.015711  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 210 ||  Training loss : 0.015697  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 220 ||  Training loss : 0.015702  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 230 ||  Training loss : 0.015724  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 240 ||  Training loss : 0.015761  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 250 ||  Training loss : 0.015809  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 260 ||  Training loss : 0.015865  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 270 ||  Training loss : 0.015929  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 280 ||  Training loss : 0.015998  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 290 ||  Training loss : 0.016072  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 0.016148  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 310 ||  Training loss : 0.016228  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 320 ||  Training loss : 0.016308  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 330 ||  Training loss : 0.016323  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 340 ||  Training loss : 0.016331  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 350 ||  Training loss : 0.016339  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 360 ||  Training loss : 0.016347  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 370 ||  Training loss : 0.016355  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 380 ||  Training loss : 0.016364  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 390 ||  Training loss : 0.016372  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 400 ||  Training loss : 0.016381  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 410 ||  Training loss : 0.016390  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 420 ||  Training loss : 0.016399  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 430 ||  Training loss : 0.016407  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 440 ||  Training loss : 0.016409  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 450 ||  Training loss : 0.016410  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 460 ||  Training loss : 0.016411  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 470 ||  Training loss : 0.016412  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 480 ||  Training loss : 0.016413  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 490 ||  Training loss : 0.016414  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 500 ||  Training loss : 0.016415  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 510 ||  Training loss : 0.016416  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 520 ||  Training loss : 0.016417  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 530 ||  Training loss : 0.016418  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    55: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 540 ||  Training loss : 0.016419  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 550 ||  Training loss : 0.016419  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 560 ||  Training loss : 0.016419  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 570 ||  Training loss : 0.016419  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 580 ||  Training loss : 0.016419  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 590 ||  Training loss : 0.016419  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 600 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 610 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 620 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 630 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 640 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    66: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 650 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 660 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.016420  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.016421  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.016422  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","1\n","1\n","Epoch 0 ||  Training loss : 0.135266  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.069636  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043223  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.030373  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.025084  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022527  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.021083  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.020164  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.019380  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.018598  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.017934  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 110 ||  Training loss : 0.017475  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.017204  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 130 ||  Training loss : 0.017080  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 140 ||  Training loss : 0.017058  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 150 ||  Training loss : 0.017107  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 160 ||  Training loss : 0.017210  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 170 ||  Training loss : 0.017356  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 180 ||  Training loss : 0.017535  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 190 ||  Training loss : 0.017741  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 0.017965  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 210 ||  Training loss : 0.018203  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 220 ||  Training loss : 0.018449  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 230 ||  Training loss : 0.018698  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 240 ||  Training loss : 0.018946  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 250 ||  Training loss : 0.019190  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 260 ||  Training loss : 0.019235  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 270 ||  Training loss : 0.019258  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 280 ||  Training loss : 0.019282  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 290 ||  Training loss : 0.019305  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 300 ||  Training loss : 0.019329  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 310 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 320 ||  Training loss : 0.019378  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 330 ||  Training loss : 0.019403  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 340 ||  Training loss : 0.019429  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 350 ||  Training loss : 0.019454  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 360 ||  Training loss : 0.019480  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 370 ||  Training loss : 0.019485  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 380 ||  Training loss : 0.019488  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 390 ||  Training loss : 0.019491  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 400 ||  Training loss : 0.019494  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 410 ||  Training loss : 0.019496  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 420 ||  Training loss : 0.019499  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 430 ||  Training loss : 0.019502  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 440 ||  Training loss : 0.019505  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 450 ||  Training loss : 0.019508  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 460 ||  Training loss : 0.019511  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 470 ||  Training loss : 0.019514  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 480 ||  Training loss : 0.019515  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 490 ||  Training loss : 0.019515  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 500 ||  Training loss : 0.019515  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 510 ||  Training loss : 0.019516  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 520 ||  Training loss : 0.019516  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 530 ||  Training loss : 0.019516  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 540 ||  Training loss : 0.019517  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 550 ||  Training loss : 0.019517  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 560 ||  Training loss : 0.019517  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 570 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 580 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 590 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.019518  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.019519  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.019520  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.019521  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.019522  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.019523  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.019524  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.019524  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.019524  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.019524  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.019524  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.019524  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.019524  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.019524  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.019524  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.019524  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.019524  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.019524  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.019524  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.019524  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.019525  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.019525  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.019525  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.019525  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.019525  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.019525  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.019525  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.019525  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.019525  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.019525  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.019525  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.019525  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.019525  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.019526  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.019526  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.019526  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.019526  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.019526  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.019526  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.019526  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.019526  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.019526  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.019526  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.019526  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.019526  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.019526  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","1\n","1\n","Epoch 0 ||  Training loss : 0.694479  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.301112  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.197117  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.194931  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.145363  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.132692  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.115288  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.102132  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.091210  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.081155  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.072326  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 110 ||  Training loss : 0.064254  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.057054  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 130 ||  Training loss : 0.050589  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 140 ||  Training loss : 0.044825  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 150 ||  Training loss : 0.039721  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 160 ||  Training loss : 0.035259  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 170 ||  Training loss : 0.031398  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 180 ||  Training loss : 0.028091  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 190 ||  Training loss : 0.025289  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 0.022938  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 210 ||  Training loss : 0.020979  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 220 ||  Training loss : 0.019358  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 230 ||  Training loss : 0.018021  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 240 ||  Training loss : 0.016922  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 250 ||  Training loss : 0.016018  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 260 ||  Training loss : 0.015275  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 270 ||  Training loss : 0.014662  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 280 ||  Training loss : 0.014154  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 290 ||  Training loss : 0.013733  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 0.013380  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 310 ||  Training loss : 0.013084  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 320 ||  Training loss : 0.012834  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 330 ||  Training loss : 0.012621  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 340 ||  Training loss : 0.012438  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 350 ||  Training loss : 0.012281  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 360 ||  Training loss : 0.012144  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 370 ||  Training loss : 0.012024  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 380 ||  Training loss : 0.011918  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 390 ||  Training loss : 0.011825  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 400 ||  Training loss : 0.011742  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 410 ||  Training loss : 0.011668  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 420 ||  Training loss : 0.011602  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 430 ||  Training loss : 0.011542  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 440 ||  Training loss : 0.011488  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 450 ||  Training loss : 0.011439  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 460 ||  Training loss : 0.011396  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 470 ||  Training loss : 0.011356  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 480 ||  Training loss : 0.011321  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 490 ||  Training loss : 0.011290  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 500 ||  Training loss : 0.011262  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 510 ||  Training loss : 0.011237  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 520 ||  Training loss : 0.011216  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 530 ||  Training loss : 0.011198  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 540 ||  Training loss : 0.011183  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 550 ||  Training loss : 0.011171  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 560 ||  Training loss : 0.011162  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 570 ||  Training loss : 0.011156  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 580 ||  Training loss : 0.011152  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 590 ||  Training loss : 0.011151  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 600 ||  Training loss : 0.011154  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 610 ||  Training loss : 0.011158  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 620 ||  Training loss : 0.011166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 630 ||  Training loss : 0.011176  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 640 ||  Training loss : 0.011189  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 650 ||  Training loss : 0.011204  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 660 ||  Training loss : 0.011222  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 670 ||  Training loss : 0.011243  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 680 ||  Training loss : 0.011266  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 690 ||  Training loss : 0.011292  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    71: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 700 ||  Training loss : 0.011320  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 710 ||  Training loss : 0.011326  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 720 ||  Training loss : 0.011329  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 730 ||  Training loss : 0.011332  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 740 ||  Training loss : 0.011336  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 750 ||  Training loss : 0.011339  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 760 ||  Training loss : 0.011342  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 770 ||  Training loss : 0.011346  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 780 ||  Training loss : 0.011349  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 790 ||  Training loss : 0.011353  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 800 ||  Training loss : 0.011356  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    82: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 810 ||  Training loss : 0.011360  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 820 ||  Training loss : 0.011360  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 830 ||  Training loss : 0.011361  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 840 ||  Training loss : 0.011361  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 850 ||  Training loss : 0.011362  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 860 ||  Training loss : 0.011362  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 870 ||  Training loss : 0.011362  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 880 ||  Training loss : 0.011363  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 890 ||  Training loss : 0.011363  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 900 ||  Training loss : 0.011364  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 910 ||  Training loss : 0.011364  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    93: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 920 ||  Training loss : 0.011364  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 930 ||  Training loss : 0.011364  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 940 ||  Training loss : 0.011364  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 950 ||  Training loss : 0.011364  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 960 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 970 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 980 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 990 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 1000 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 1010 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 1020 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch   104: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 1030 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 1040 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.011365  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","1\n","[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   13.9s finished\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n","  DeprecationWarning)\n","1\n","Epoch 0 ||  Training loss : 0.234174  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.072265  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.039384  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.042320  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.023116  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.018572  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.018183  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.016074  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015158  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.014632  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.014328  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 110 ||  Training loss : 0.014155  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.014013  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 130 ||  Training loss : 0.013975  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 140 ||  Training loss : 0.013982  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 150 ||  Training loss : 0.014051  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 160 ||  Training loss : 0.014159  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 170 ||  Training loss : 0.014302  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 180 ||  Training loss : 0.014471  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 190 ||  Training loss : 0.014658  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 0.014856  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 210 ||  Training loss : 0.015059  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 220 ||  Training loss : 0.015264  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 230 ||  Training loss : 0.015466  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 240 ||  Training loss : 0.015662  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 250 ||  Training loss : 0.015698  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 260 ||  Training loss : 0.015716  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 270 ||  Training loss : 0.015735  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 280 ||  Training loss : 0.015753  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 290 ||  Training loss : 0.015772  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 300 ||  Training loss : 0.015791  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 310 ||  Training loss : 0.015810  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 320 ||  Training loss : 0.015829  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 330 ||  Training loss : 0.015849  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 340 ||  Training loss : 0.015868  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 350 ||  Training loss : 0.015888  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 360 ||  Training loss : 0.015891  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 370 ||  Training loss : 0.015893  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 380 ||  Training loss : 0.015895  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 390 ||  Training loss : 0.015897  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 400 ||  Training loss : 0.015899  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 410 ||  Training loss : 0.015901  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 420 ||  Training loss : 0.015904  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 430 ||  Training loss : 0.015906  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 440 ||  Training loss : 0.015908  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 450 ||  Training loss : 0.015910  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 460 ||  Training loss : 0.015912  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 470 ||  Training loss : 0.015912  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 480 ||  Training loss : 0.015913  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 490 ||  Training loss : 0.015913  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 500 ||  Training loss : 0.015913  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 510 ||  Training loss : 0.015913  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 520 ||  Training loss : 0.015914  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 530 ||  Training loss : 0.015914  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 540 ||  Training loss : 0.015914  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 550 ||  Training loss : 0.015914  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 560 ||  Training loss : 0.015914  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    58: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 570 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 580 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015915  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015916  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015917  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015918  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 1 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","Overwritten attributes  get_veff  of <class 'pyscf.dft.rks.RKS'>\n","converged SCF energy = -118.940753833464\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943264562286\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938711857679\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.9511665906\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943305965084\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944997381534\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945821941146\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942780877485\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947513725755\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940710549637\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945466540973\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950234972304\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936309413513\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946817681861\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.95005842135\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942499488347\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937696948564\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947866921067\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.9426538722\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946338828659\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01258, 'max': 0.0283, 'mean deviation': -0.0, 'rmse': 0.01586}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.0158629382744933\n","Dataset 0 new STD: 0.044239833451589945\n","1\n","Epoch 0 ||  Training loss : 0.015863  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.016371  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.017170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.017400  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.017728  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.018077  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.018332  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.018572  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.018778  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.018945  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.019085  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.019205  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.019226  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.019236  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.019247  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.019258  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.019269  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.019280  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.019291  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.019302  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.019313  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.019325  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.019336  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.019338  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.019339  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.019340  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.019342  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.019343  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.019344  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.019345  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.019347  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.019348  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.019349  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.019351  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.019351  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.019351  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.019351  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.019351  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.019352  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.019352  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.019352  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.019352  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.019352  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.019352  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.019353  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.019354  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.019355  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.019356  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.019356  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.019356  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.019356  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.019356  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 2 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940798083151\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94307075063\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938727957346\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.951622866228\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943447332684\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945092335303\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946289469913\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94284856289\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947543918789\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940848734196\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94592457165\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950397239429\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936521931432\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947051013104\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950187850145\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942624824873\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938000874121\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94806577305\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942581603244\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946373748462\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01559, 'max': 0.03526, 'mean deviation': -0.0, 'rmse': 0.01927}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.019273082117061757\n","Dataset 0 new STD: 0.04424348508623354\n","1\n","Epoch 0 ||  Training loss : 0.019273  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.019745  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.019791  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.019636  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.019508  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.019501  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.019578  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.019685  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.019796  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.019872  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.019916  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.019937  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.019940  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.019943  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.019946  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.019948  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.019948  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.019948  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.019946  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.019944  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.019941  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.019938  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.019935  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.019934  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.019934  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.019933  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.019933  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.019933  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.019932  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.019932  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.019932  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.019931  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.019931  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.019931  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.019931  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.019931  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.019931  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.019930  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 3 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940608629268\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942821275001\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938533305449\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.951853763023\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943564111546\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945086673707\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946409295099\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942938951144\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947554870426\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940720920032\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946008816618\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950639324459\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936533577078\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947080566904\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950260208651\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942516003539\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937903553988\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948065181689\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942542672419\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946368288878\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01591, 'max': 0.03853, 'mean deviation': -0.0, 'rmse': 0.01993}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.019926933941706727\n","Dataset 0 new STD: 0.044101179954803345\n","1\n","Epoch 0 ||  Training loss : 0.019927  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.020283  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.020128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.019891  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.019724  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.019558  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.019416  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.019274  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.019152  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.019058  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.018983  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 110 ||  Training loss : 0.018923  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.018880  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 130 ||  Training loss : 0.018849  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 140 ||  Training loss : 0.018827  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 150 ||  Training loss : 0.018809  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 160 ||  Training loss : 0.018795  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 170 ||  Training loss : 0.018782  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 180 ||  Training loss : 0.018771  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 190 ||  Training loss : 0.018760  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 0.018750  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 210 ||  Training loss : 0.018742  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 220 ||  Training loss : 0.018734  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 230 ||  Training loss : 0.018726  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 240 ||  Training loss : 0.018720  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 250 ||  Training loss : 0.018713  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 260 ||  Training loss : 0.018708  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 270 ||  Training loss : 0.018702  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 280 ||  Training loss : 0.018697  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 290 ||  Training loss : 0.018693  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 0.018688  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 310 ||  Training loss : 0.018684  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 320 ||  Training loss : 0.018680  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 330 ||  Training loss : 0.018676  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 340 ||  Training loss : 0.018672  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 350 ||  Training loss : 0.018668  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 360 ||  Training loss : 0.018664  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 370 ||  Training loss : 0.018660  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 380 ||  Training loss : 0.018656  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 390 ||  Training loss : 0.018653  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 400 ||  Training loss : 0.018649  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 410 ||  Training loss : 0.018646  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 420 ||  Training loss : 0.018642  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 430 ||  Training loss : 0.018639  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 440 ||  Training loss : 0.018635  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 450 ||  Training loss : 0.018632  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 460 ||  Training loss : 0.018629  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 470 ||  Training loss : 0.018626  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 480 ||  Training loss : 0.018623  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 490 ||  Training loss : 0.018621  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 500 ||  Training loss : 0.018618  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 510 ||  Training loss : 0.018615  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 520 ||  Training loss : 0.018613  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 530 ||  Training loss : 0.018611  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 540 ||  Training loss : 0.018609  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 550 ||  Training loss : 0.018607  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 560 ||  Training loss : 0.018605  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 570 ||  Training loss : 0.018603  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 580 ||  Training loss : 0.018602  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 590 ||  Training loss : 0.018600  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 600 ||  Training loss : 0.018599  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 610 ||  Training loss : 0.018598  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 620 ||  Training loss : 0.018596  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 630 ||  Training loss : 0.018595  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 640 ||  Training loss : 0.018594  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 650 ||  Training loss : 0.018594  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 660 ||  Training loss : 0.018593  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 670 ||  Training loss : 0.018592  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 680 ||  Training loss : 0.018591  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 690 ||  Training loss : 0.018591  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 700 ||  Training loss : 0.018590  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 710 ||  Training loss : 0.018590  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 720 ||  Training loss : 0.018589  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 730 ||  Training loss : 0.018589  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 740 ||  Training loss : 0.018589  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 750 ||  Training loss : 0.018589  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 760 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 770 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 780 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 790 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 800 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 810 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 820 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 830 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 840 ||  Training loss : 0.018587  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 850 ||  Training loss : 0.018587  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 860 ||  Training loss : 0.018587  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 870 ||  Training loss : 0.018587  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    89: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 880 ||  Training loss : 0.018587  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 890 ||  Training loss : 0.018587  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 900 ||  Training loss : 0.018587  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 910 ||  Training loss : 0.018587  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 920 ||  Training loss : 0.018587  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 930 ||  Training loss : 0.018587  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 940 ||  Training loss : 0.018587  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 950 ||  Training loss : 0.018587  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 960 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 970 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 980 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch   100: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 990 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 1000 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 1010 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 1020 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 1030 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 1040 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 1050 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 1060 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 1070 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 1080 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 1090 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch   111: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 1100 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 1110 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 1120 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 1130 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 1140 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 1150 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 1160 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 1170 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 1180 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 1190 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 1200 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch   122: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 1210 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 1220 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 4 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940640160329\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942942509333\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938594951462\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.951792314662\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943596966783\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945088347932\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946297115803\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942967989516\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947596031313\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940705867562\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945937951946\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950641923272\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936513898581\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947033543671\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950281516739\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942534145896\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937794917045\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948071578085\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942555998171\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946419991343\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01492, 'max': 0.03548, 'mean deviation': -0.0, 'rmse': 0.01859}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.01858676339615229\n","Dataset 0 new STD: 0.04410376736473545\n","1\n","Epoch 0 ||  Training loss : 0.018587  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.018719  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.018654  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.018633  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.018587  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.018592  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.018591  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.018589  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.018591  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.018589  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.018589  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.018589  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.018589  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.018589  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.018589  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.018589  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.018589  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 5 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940641760093\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942943844644\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938596403745\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.951789939994\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943595013227\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945088245624\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94629654326\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942966416668\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947595371314\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940706997109\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945937319804\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950639014614\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936513805237\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947033493009\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950280225119\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942534701034\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937796610278\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948071165162\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942555903326\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946419246684\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01493, 'max': 0.03547, 'mean deviation': -0.0, 'rmse': 0.01859}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.01858814047334687\n","Dataset 0 new STD: 0.0441071457192786\n","1\n","Epoch 0 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.019014  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.018680  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.018594  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.018600  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.018607  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.018589  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.018584  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.018590  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.018591  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.018589  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 110 ||  Training loss : 0.018589  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 130 ||  Training loss : 0.018588  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 140 ||  Training loss : 0.018587  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 150 ||  Training loss : 0.018587  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 160 ||  Training loss : 0.018586  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 170 ||  Training loss : 0.018586  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 180 ||  Training loss : 0.018586  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 190 ||  Training loss : 0.018586  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.018586  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.018586  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 220 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 240 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 250 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 260 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 270 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 280 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    30: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 290 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 300 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 330 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 350 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 360 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 370 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 380 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 390 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 400 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 410 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 440 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 460 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 470 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 480 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 490 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 500 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 510 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 520 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.018585  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Testing ======\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940641760093\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942943844644\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938596403745\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.951789939994\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943595013227\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945088245624\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94629654326\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942966416668\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947595371314\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940706997109\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945937319804\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950639014614\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936513805237\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947033493009\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950280225119\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942534701034\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937796610278\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948071165162\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942555903326\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946419246684\n","20 systems found, adding energy\n","20 systems found, adding energy\n","{'mae': 0.01493, 'max': 0.03547, 'mean deviation': -0.0, 'rmse': 0.01859}\n","/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MjnLCRpvVATK","executionInfo":{"status":"ok","timestamp":1624311091143,"user_tz":-60,"elapsed":191576,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"980b66b6-0e0f-41ac-f282-475a970f6fce"},"source":["!neuralxc engine basis_sgdml_benzene.json workdir/testing.traj --workdir ./tmp"],"execution_count":null,"outputs":[{"output_type":"stream","text":["converged SCF energy = -118.940210536202\n","converged SCF energy = -118.940596594795\n","converged SCF energy = -118.937541724306\n","converged SCF energy = -118.952538068452\n","converged SCF energy = -118.942789383792\n","converged SCF energy = -118.94475157552\n","converged SCF energy = -118.947818500794\n","converged SCF energy = -118.941952877314\n","converged SCF energy = -118.946537584784\n","converged SCF energy = -118.940833957381\n","converged SCF energy = -118.946983174052\n","converged SCF energy = -118.950053476769\n","converged SCF energy = -118.936409631262\n","converged SCF energy = -118.947404363513\n","converged SCF energy = -118.949410728218\n","converged SCF energy = -118.942215468016\n","converged SCF energy = -118.939608592427\n","converged SCF energy = -118.947629121785\n","converged SCF energy = -118.941922685014\n","converged SCF energy = -118.945064132137\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CjZX2qB7-mSW"},"source":["## Task: get projection out"]},{"cell_type":"markdown","metadata":{"id":"IxIJKo55-rcJ"},"source":["### `neural xc pre ...` seems to do the job"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6aKa0V3GeWHq","executionInfo":{"status":"ok","timestamp":1624405750491,"user_tz":-60,"elapsed":9777,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"fcfef875-46dc-4e34-fcfe-8163581e072d"},"source":["!neuralxc pre basis_sgdml_benzene.json --srcdir ./tmp --dest ./tmp --xyz workdir/testing.traj"],"execution_count":null,"outputs":[{"output_type":"stream","text":["======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","./tmp/0/pyscf.chkpt\n","./tmp/1/pyscf.chkpt\n","./tmp/2/pyscf.chkpt\n","./tmp/3/pyscf.chkpt\n","./tmp/4/pyscf.chkpt\n","./tmp/5/pyscf.chkpt\n","./tmp/6/pyscf.chkpt\n","./tmp/7/pyscf.chkpt\n","./tmp/8/pyscf.chkpt\n","./tmp/9/pyscf.chkpt\n","./tmp/10/pyscf.chkpt\n","./tmp/11/pyscf.chkpt\n","./tmp/12/pyscf.chkpt\n","./tmp/13/pyscf.chkpt\n","./tmp/14/pyscf.chkpt\n","./tmp/15/pyscf.chkpt\n","./tmp/16/pyscf.chkpt\n","./tmp/17/pyscf.chkpt\n","./tmp/18/pyscf.chkpt\n","./tmp/19/pyscf.chkpt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mA-ULovFBkH7"},"source":["test_pre = np.load(\".tmp/540fffd922b6f0cbee81e4c1c80c7dfa.npy\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oMs4hRCDQFGl","executionInfo":{"status":"ok","timestamp":1624458178995,"user_tz":-60,"elapsed":279,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"36b075ea-42b6-4ea2-c0d2-45a342c61f00"},"source":["test_pre"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1.75093776e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","        -1.06849315e-04,  2.35582319e-03, -5.02244948e-02],\n","       [ 1.75063416e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","        -1.12710913e-02, -3.75379073e-02, -3.70123750e-02],\n","       [ 1.75087629e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","         3.32885384e-02,  8.95267630e-02,  5.28164416e-02],\n","       ...,\n","       [ 1.75051993e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","        -3.81710922e-02,  3.76717721e-02,  2.99026083e-02],\n","       [ 1.75092086e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","        -6.50714538e-02, -7.12436218e-03,  4.84704325e-02],\n","       [ 1.75051101e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","        -7.98489089e-02, -9.68339061e-02, -4.01309898e-02]])"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EyyPAaI7CD-5","executionInfo":{"status":"ok","timestamp":1624404156301,"user_tz":-60,"elapsed":380,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"36862813-8405-4b22-e47d-bd46181e7741"},"source":["test_pre.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(20, 1262)"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"4mhEHUGw-0-u"},"source":["#### TODO: figure out why 1262?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KtMr9aJZCbsK","executionInfo":{"status":"ok","timestamp":1624405414720,"user_tz":-60,"elapsed":260,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"89a15a95-a2de-4141-c56d-941a3705f4e0"},"source":["(1+4+9+16+25+25+25+25+25+25)*3 + (1+4+9+16)*8"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["780"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"markdown","metadata":{"id":"yatjk2zsCPqV"},"source":["propane C3H8 "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TM_rIalVCG5I","executionInfo":{"status":"ok","timestamp":1624404168844,"user_tz":-60,"elapsed":261,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"e40e8aa2-c0a7-4286-8f83-cd2bc5a39154"},"source":["test_pre[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 1.75093776e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","       -1.06849315e-04,  2.35582319e-03, -5.02244948e-02])"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"VWxhFwmI_Awv"},"source":["### import drivers from neuralxc.drivers\n","`pre_driver()` function does the same job as `nerualxc pre ...`"]},{"cell_type":"code","metadata":{"id":"biI5uUyUKdKI"},"source":["from neuralxc.drivers.other import pre_driver"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"No4h3jhFKze8","executionInfo":{"status":"ok","timestamp":1624406830483,"user_tz":-60,"elapsed":8111,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"468f77ec-e6bb-4032-98f4-82a8c895b943"},"source":["pre_driver(\"examples/example_scripts/train_model_20_propane/workdir/testing.traj\",\n","           \"examples/example_scripts/train_model_20_propane/tmp\",\n","           \"examples/example_scripts/train_model_20_propane/basis_sgdml_benzene.json\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","examples/example_scripts/train_model_20_propane/tmp/0/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/1/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/2/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/3/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/4/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/5/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/6/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/7/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/8/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/9/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/10/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/11/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/12/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/13/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/14/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/15/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/16/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/17/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/18/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/19/pyscf.chkpt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZpSdG0GILXvW"},"source":["test_pre2 = np.load(\".tmp/540fffd922b6f0cbee81e4c1c80c7dfa.npy\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SewRHKSVMmG5","executionInfo":{"status":"ok","timestamp":1624407103461,"user_tz":-60,"elapsed":289,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"5d8bc7d0-d14c-45a3-d1d4-c8de3a425ba0"},"source":["np.array_equal(test_pre,test_pre2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":85}]},{"cell_type":"markdown","metadata":{"id":"ICDO3V3vBPU3"},"source":["### tried to change n and l \n","though it doesn't seem to work. After ruuning `pre_driver()`, \"pre.json\" get overwritten"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-dwlN9UHN3cU","executionInfo":{"status":"ok","timestamp":1624407795800,"user_tz":-60,"elapsed":8858,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"d0197ac0-cd65-4c44-c1b6-47b9a8744968"},"source":["#changed n=4,l=4 for C\n","pre_driver(\"examples/example_scripts/train_model_20_propane/workdir/testing.traj\",\n","           \"examples/example_scripts/train_model_20_propane/tmp\",\n","           \"../pre.json\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 4, 'l': 4}, 'H': {'n': 4, 'l': 4}}\n","examples/example_scripts/train_model_20_propane/tmp/0/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/1/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/2/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/3/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/4/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/5/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/6/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/7/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/8/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/9/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/10/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/11/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/12/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/13/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/14/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/15/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/16/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/17/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/18/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/19/pyscf.chkpt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ci5mrXVpMmJZ"},"source":["test_pre3 = np.load(\".tmp/540fffd922b6f0cbee81e4c1c80c7dfa.npy\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B6sqGAdRMmMA","executionInfo":{"status":"ok","timestamp":1624407812789,"user_tz":-60,"elapsed":4,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"8824c281-a9b8-4af9-eadb-1c8ba2d28462"},"source":["np.array_equal(test_pre,test_pre3)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"kneW23z1QuXc","executionInfo":{"status":"ok","timestamp":1624407994741,"user_tz":-60,"elapsed":273,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"ede31a85-45c6-4d70-e41b-e452a834532c"},"source":["pwd"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc'"]},"metadata":{"tags":[]},"execution_count":101}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Bhla_pIPQP0","executionInfo":{"status":"ok","timestamp":1624408031890,"user_tz":-60,"elapsed":8744,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"443f3fed-ef15-41a0-8079-0e0b313ffc7a"},"source":["#changed n=1,l=1 for C and H\n","pre_driver(\"examples/example_scripts/train_model_20_propane/workdir/testing.traj\",\n","           \"examples/example_scripts/train_model_20_propane/tmp\",\n","           \"../pre.json\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 1, 'l': 1}, 'H': {'n': 1, 'l': 1}}\n","examples/example_scripts/train_model_20_propane/tmp/0/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/1/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/2/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/3/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/4/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/5/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/6/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/7/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/8/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/9/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/10/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/11/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/12/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/13/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/14/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/15/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/16/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/17/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/18/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/19/pyscf.chkpt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hwCMkIwbQ5ZS"},"source":["test_pre4 = np.load(\".tmp/540fffd922b6f0cbee81e4c1c80c7dfa.npy\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e_WKMgOdRARq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624408075987,"user_tz":-60,"elapsed":316,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"665d7bed-0808-43c1-e9f1-d43ea184619c"},"source":["np.array_equal(test_pre,test_pre3)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":104}]},{"cell_type":"markdown","metadata":{"id":"psyDamXdBifo"},"source":["#### go through `pre_driver()` line by line"]},{"cell_type":"code","metadata":{"id":"VAyuxQFDBfsf"},"source":["preprocessor_path = \"../pre.json\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y0g9LVDJCNne"},"source":["`make_nested_absolute()` is in neuralxc.formatter"]},{"cell_type":"code","metadata":{"id":"uVvIbyu8CJkP"},"source":["from neuralxc.formatter import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G8l_r7suCVZp"},"source":["import json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cRn9GtdGBfu_"},"source":["pre = make_nested_absolute(json.loads(open(\"../pre.json\", 'r').read()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PHpxd1QyBfxb","executionInfo":{"status":"ok","timestamp":1624454627565,"user_tz":-60,"elapsed":310,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"5b38f7f3-2dea-49bb-a76e-a8cc0f9dd3f7"},"source":["pre"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'engine_kwargs': {'basis': 'ccpvdz', 'xc': 'PBE'},\n"," 'n_workers': 1,\n"," 'preprocessor': {'C': {'l': 5, 'n': 10},\n","  'H': {'l': 4, 'n': 4},\n","  'application': 'pyscf',\n","  'basis': 'ccpvtz-jkfit',\n","  'extension': 'chkpt',\n","  'spec_agnostic': False}}"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"2C1FC2ApC3uA"},"source":["from ase.io import read"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UIGaa3lwBfzw"},"source":["atoms = read(\"examples/example_scripts/train_model_20_propane/workdir/testing.traj\", ':')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y8Tv3wSqBf2S","executionInfo":{"status":"ok","timestamp":1624454773387,"user_tz":-60,"elapsed":302,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"5b4e12fc-2959-417b-b251-6ad5d726e68f"},"source":["atoms"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...))]"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xz2G3NDMBf5I","executionInfo":{"status":"ok","timestamp":1624454763874,"user_tz":-60,"elapsed":290,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"dd3bb128-e033-45f7-82f8-697a99d40827"},"source":["atoms[0].positions"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1.25991742, -1.19954342,  1.41354126],\n","       [ 2.13037387, -0.57736755,  0.21461351],\n","       [ 0.86798119, -0.41870608,  2.04738686],\n","       [ 1.74387902, -1.90588774,  1.96012759],\n","       [ 0.3011863 , -1.61970725,  1.05809014],\n","       [ 2.90968884,  0.0927169 ,  0.59594325],\n","       [ 1.47191602,  0.10321607, -0.42808757],\n","       [ 2.72913521, -1.59525051, -0.75344542],\n","       [ 3.54174583, -2.16964612, -0.17017267],\n","       [ 3.23082184, -1.04979412, -1.52984522],\n","       [ 2.00499249, -2.2991183 , -1.25418812]])"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"ekhGuznZC_Om"},"source":["from neuralxc.ml.utils import get_preprocessor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"grxYNj35C_Q8"},"source":["preprocessor = get_preprocessor(pre, atoms, \"examples/example_scripts/train_model_20_propane/tmp\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WC_Wnjp0C_Ta","executionInfo":{"status":"ok","timestamp":1624454898014,"user_tz":-60,"elapsed":311,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"fb858789-7c15-4158-8e03-19fb4ad9bfbc"},"source":["preprocessor.basis_instructions"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'C': {'l': 5, 'n': 10},\n"," 'H': {'l': 4, 'n': 4},\n"," 'application': 'pyscf',\n"," 'basis': 'ccpvtz-jkfit',\n"," 'extension': 'chkpt',\n"," 'spec_agnostic': False}"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"zIF0rOoIC_Vo"},"source":["from neuralxc.ml.utils import get_basis_grid"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2JTYh9NaC_YN"},"source":["basis_grid = get_basis_grid(pre)['preprocessor__basis_instructions']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YtFTUkj4EE7f"},"source":["???  basis_grid is pretty much the same as preprocessor.basis_instructions "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QsjRnH_rC_am","executionInfo":{"status":"ok","timestamp":1624454985843,"user_tz":-60,"elapsed":4,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"f05d7240-880c-4bbf-fabf-eff77486ccfb"},"source":["basis_grid"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'C': {'l': 5, 'n': 10},\n","  'H': {'l': 4, 'n': 4},\n","  'application': 'pyscf',\n","  'basis': 'ccpvtz-jkfit',\n","  'extension': 'chkpt',\n","  'spec_agnostic': False}]"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"w_ElQhFwC_c0"},"source":["basis_instr = basis_grid[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pyHiwCzuE-_K"},"source":["preprocessor.basis_instructions = basis_instr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wYWqBWZME_Bs","executionInfo":{"status":"ok","timestamp":1624455334666,"user_tz":-60,"elapsed":307,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"b0c07178-83c6-4f1d-fd5f-35ce69a061f7"},"source":[" print('BI', basis_instr)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"5GxFzpPVFSKq","executionInfo":{"status":"ok","timestamp":1624455356577,"user_tz":-60,"elapsed":291,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"07b3793f-d628-4689-f176-268ef4fd47e1"},"source":["basis_instr.get('application', 'siesta')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'pyscf'"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"eMOjDIwSFwfR"},"source":["from neuralxc.drivers.other import get_real_basis"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jizFN6NAFSNB"},"source":["real_basis = get_real_basis(atoms,basis_instr['basis'],spec_agnostic=basis_instr.get('spec_agnostic', False))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FgvCJEfKFSS_","executionInfo":{"status":"ok","timestamp":1624455502966,"user_tz":-60,"elapsed":403,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"49d7f566-d0ee-4ca9-ca27-66478549e8d4"},"source":["real_basis"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'C': {'l': 5, 'n': 10}, 'H': {'l': 4, 'n': 4}}"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3_r03NiHE_EJ","executionInfo":{"status":"ok","timestamp":1624455572144,"user_tz":-60,"elapsed":357,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"a15ad882-04f5-4b4c-e0fb-f7cffd4a6177"},"source":["real_basis.keys()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['C', 'H'])"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"markdown","metadata":{"id":"Wx8mpoBEGa93"},"source":["#### The line below overwrites `basis_instr` whic is read from user-provided input"]},{"cell_type":"code","metadata":{"id":"4957PFl2GPEd"},"source":["for key in real_basis:\n","    basis_instr[key] = real_basis[key]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tkjSe2jKGrWi"},"source":["pre.update({'preprocessor': basis_instr})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TEGze91RGrY5","executionInfo":{"status":"ok","timestamp":1624455745960,"user_tz":-60,"elapsed":262,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"213566a8-e99c-4b84-fc9f-1b7637152113"},"source":["pre"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'engine_kwargs': {'basis': 'ccpvdz', 'xc': 'PBE'},\n"," 'n_workers': 1,\n"," 'preprocessor': {'C': {'l': 5, 'n': 10},\n","  'H': {'l': 4, 'n': 4},\n","  'application': 'pyscf',\n","  'basis': 'ccpvtz-jkfit',\n","  'extension': 'chkpt',\n","  'spec_agnostic': False}}"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"markdown","metadata":{"id":"7TKBkpBEHTMA"},"source":["#### `preprocessor.fit_transform(None)` seems to be the method that we need to figure out next!!!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"klZuCaOMGra4","executionInfo":{"status":"ok","timestamp":1624455800787,"user_tz":-60,"elapsed":16882,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"daa8a7bf-e3d3-4225-8819-97350b88e642"},"source":["data = preprocessor.fit_transform(None)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["examples/example_scripts/train_model_20_propane/tmp/0/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/1/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/2/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/3/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/4/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/5/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/6/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/7/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/8/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/9/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/10/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/11/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/12/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/13/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/14/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/15/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/16/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/17/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/18/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/19/pyscf.chkpt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"46Jw0dIuHPqq","executionInfo":{"status":"ok","timestamp":1624455842442,"user_tz":-60,"elapsed":260,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"7d0e6177-1570-4187-8caa-89cceb4579e9"},"source":["data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1.75093776e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","        -1.06849315e-04,  2.35582319e-03, -5.02244948e-02],\n","       [ 1.75063416e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","        -1.12710913e-02, -3.75379073e-02, -3.70123750e-02],\n","       [ 1.75087629e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","         3.32885384e-02,  8.95267630e-02,  5.28164416e-02],\n","       ...,\n","       [ 1.75051993e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","        -3.81710922e-02,  3.76717721e-02,  2.99026083e-02],\n","       [ 1.75092086e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","        -6.50714538e-02, -7.12436218e-03,  4.84704325e-02],\n","       [ 1.75051101e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","        -7.98489089e-02, -9.68339061e-02, -4.01309898e-02]])"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6sq8n1kuGPG-","executionInfo":{"status":"ok","timestamp":1624455833777,"user_tz":-60,"elapsed":270,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"221ee7f2-ab5d-4c13-cb8e-a83e96433c89"},"source":["data.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(20, 1262)"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"markdown","metadata":{"id":"s1DowF_zKnK5"},"source":["Investiaging `preprocessor` object leads to:"]},{"cell_type":"markdown","metadata":{"id":"MC_1T77QKOYK"},"source":["`get_preprocessor()` in `neuralxc/ml/utils.py`"]},{"cell_type":"markdown","metadata":{"id":"ZQ5KjAxiKrnO"},"source":["which then leads to "]},{"cell_type":"markdown","metadata":{"id":"v3Fv-sbXKYkB"},"source":["`Preprocessor` class in `neuralxc/preprocessor/preprocessor.py`"]},{"cell_type":"markdown","metadata":{"id":"OyNDVH4OKxMC"},"source":["which leads to\n"]},{"cell_type":"markdown","metadata":{"id":"qfuCC8_hKzak"},"source":["`DensityProjector()` in `neuralxc/projector/projector.py`"]},{"cell_type":"code","metadata":{"id":"8C7m4jnvKN5o"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BepKIudVKN8D"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tcFivjIWKN-v"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rH_TtnlBKOBK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PSEgqJXkGPJD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"se-6TNBTGPLU"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YVztqbgQGPNt"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LcZ4sas99ggj"},"source":["import h5py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XSLWLGaZ-ivf"},"source":["f = h5py.File(\"workdir/testing/data.hdf5\", \"r\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RWxNboTz-1vE","executionInfo":{"status":"ok","timestamp":1624403515514,"user_tz":-60,"elapsed":264,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"7e80dd46-1075-4ddd-8b4e-424aefe94971"},"source":["f.keys()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<KeysViewHDF5 ['system']>"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iVs_w1yC-2oV","executionInfo":{"status":"ok","timestamp":1624403537259,"user_tz":-60,"elapsed":266,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"0a5806ec-c14b-4b09-f374-aeda97ef3c12"},"source":["list(f['system'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['it0', 'ref', 'testing']"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4DICrl6qAeGG","executionInfo":{"status":"ok","timestamp":1624403743079,"user_tz":-60,"elapsed":262,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"c56e5899-e00b-4664-fd01-97ce53c1b943"},"source":["list(f['system']['it0']['density'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['540fffd922b6f0cbee81e4c1c80c7dfa']"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E4-JGMJm_5Ba","executionInfo":{"status":"ok","timestamp":1624403586498,"user_tz":-60,"elapsed":268,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"ecba242d-bf74-4984-e0ef-2cdcd167ebd3"},"source":["list(f['system']['it0']['energy'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.10226966514346714,\n"," 0.039626753764423484,\n"," 0.15792664630362196,\n"," -0.20108776160122943,\n"," 0.021907554097651882,\n"," -0.018725369080584642,\n"," -0.051604822480840085,\n"," 0.03901253773119606,\n"," -0.08694773404067746,\n"," 0.10049447554138169,\n"," -0.04182985434999864,\n"," -0.16976948681258364,\n"," 0.21459703824666576,\n"," -0.07165824658704878,\n"," -0.16000632735449472,\n"," 0.050760118499965756,\n"," 0.1796901350612643,\n"," -0.09989474409803734,\n"," 0.050183174743324344,\n"," -0.05494375272974139]"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cAxiKij2_vjn","executionInfo":{"status":"ok","timestamp":1624403565527,"user_tz":-60,"elapsed":310,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"07a70b36-967f-4d61-ae48-91d1a057c798"},"source":["list(f['system']['ref']['energy'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.10550690699938059,\n"," 0.006440664167257637,\n"," 0.14910719306271858,\n"," -0.17694527238290902,\n"," 0.025851356377643242,\n"," -0.0311490537451391,\n"," -0.016132461960751243,\n"," 0.018089817258442054,\n"," -0.1169276492832978,\n"," 0.11217311358223014,\n"," -0.011145042170483066,\n"," -0.18532556012951318,\n"," 0.21620274688575591,\n"," -0.06451687403523465,\n"," -0.17053774719624926,\n"," 0.04993273920445063,\n"," 0.2065744226656534,\n"," -0.0954033824173166,\n"," 0.041813624193309806,\n"," -0.06360954108640726]"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H2Sj9XDM-4B8","executionInfo":{"status":"ok","timestamp":1624403622853,"user_tz":-60,"elapsed":355,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"c205c896-a829-479e-96ea-1bc800ae63eb"},"source":["list(f['system']['testing']['nxc']['energy'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.10226966514346714,\n"," 0.039626753764423484,\n"," 0.15792664630362196,\n"," -0.20108776160122943,\n"," 0.021907554097651882,\n"," -0.018725369080584642,\n"," -0.051604822480840085,\n"," 0.03901253773119606,\n"," -0.08694773404067746,\n"," 0.10049447554138169,\n"," -0.04182985434999864,\n"," -0.16976948681258364,\n"," 0.21459703824666576,\n"," -0.07165824658704878,\n"," -0.16000632735449472,\n"," 0.050760118499965756,\n"," 0.1796901350612643,\n"," -0.09989474409803734,\n"," 0.050183174743324344,\n"," -0.05494375272974139]"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UafMhu4H_ZxX","executionInfo":{"status":"ok","timestamp":1624403693120,"user_tz":-60,"elapsed":411,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"c815038a-8f23-4583-b589-f22e424375d7"},"source":["list(f['system']['testing']['ref']['energy'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.10550690699938059,\n"," 0.006440664167257637,\n"," 0.14910719306271858,\n"," -0.17694527238290902,\n"," 0.025851356377643242,\n"," -0.0311490537451391,\n"," -0.016132461960751243,\n"," 0.018089817258442054,\n"," -0.1169276492832978,\n"," 0.11217311358223014,\n"," -0.011145042170483066,\n"," -0.18532556012951318,\n"," 0.21620274688575591,\n"," -0.06451687403523465,\n"," -0.17053774719624926,\n"," 0.04993273920445063,\n"," 0.2065744226656534,\n"," -0.0954033824173166,\n"," 0.041813624193309806,\n"," -0.06360954108640726]"]},"metadata":{"tags":[]},"execution_count":41}]}]}