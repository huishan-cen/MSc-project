{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test_projector_nn.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNiLOOPXy1Rrlyycj/f0mTc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"if85Sc0jOBmq","executionInfo":{"status":"ok","timestamp":1626389633711,"user_tz":-60,"elapsed":16667,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"ad26e021-c290-4c74-d744-b79c7a587218"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HeCIdApVqzlt","executionInfo":{"status":"ok","timestamp":1626389635111,"user_tz":-60,"elapsed":191,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"f9a669e1-1aad-4bf9-c1b2-8a1c51a73551"},"source":["cd drive/MyDrive/MSc_Project/neural_xc_playground/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/MSc_Project/neural_xc_playground\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPsOWYfkMbcB","executionInfo":{"status":"ok","timestamp":1625698733719,"user_tz":-60,"elapsed":4977,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"82b2489f-d5ec-4ed3-bd1f-3b67b4fce736"},"source":["#!git clone https://github.com/semodi/neuralxc.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'neuralxc'...\n","remote: Enumerating objects: 3318, done.\u001b[K\n","remote: Counting objects: 100% (1510/1510), done.\u001b[K\n","remote: Compressing objects: 100% (684/684), done.\u001b[K\n","remote: Total 3318 (delta 989), reused 1251 (delta 792), pack-reused 1808\u001b[K\n","Receiving objects: 100% (3318/3318), 19.38 MiB | 14.36 MiB/s, done.\n","Resolving deltas: 100% (2033/2033), done.\n","Checking out files: 100% (267/267), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OLlQIuoyMhd9","executionInfo":{"status":"ok","timestamp":1626389637668,"user_tz":-60,"elapsed":494,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"01c82d4c-4a18-468f-b775-59f57266a38b"},"source":["cd neuralxc/"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WKdPFb4j9R6P"},"source":["## Install"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iCOKexN3P9Bo","executionInfo":{"status":"ok","timestamp":1626389748580,"user_tz":-60,"elapsed":109017,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"3980e05b-ba64-4aa1-e766-dfc8639cc987"},"source":["!sh install.sh"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/semodi/libnxc.git@9c3b895840a000968866a44841291926fe29aab7 (from -r requirements.txt (line 23))\n","  Cloning https://github.com/semodi/libnxc.git (to revision 9c3b895840a000968866a44841291926fe29aab7) to /tmp/pip-req-build-n0r78w81\n","  Running command git clone -q https://github.com/semodi/libnxc.git /tmp/pip-req-build-n0r78w81\n","  Running command git checkout -q 9c3b895840a000968866a44841291926fe29aab7\n","Collecting ase==3.17\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/af/ea30928def88748d76946c0af0e6a8f9c2d2e65e3d65da527fe80e9ba06e/ase-3.17.0-py3-none-any.whl (1.8MB)\n","\u001b[K     |████████████████████████████████| 1.8MB 5.0MB/s \n","\u001b[?25hRequirement already satisfied: dask in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2.12.0)\n","Collecting h5py==2.9.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/fd/2ca5c4f4ed33ac4178f9c4d551e3946ab480866e3cd67a65a67a4bb35367/h5py-2.9.0-cp37-cp37m-manylinux1_x86_64.whl (2.8MB)\n","\u001b[K     |████████████████████████████████| 2.8MB 29.2MB/s \n","\u001b[?25hCollecting ipyparallel\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/e9/03a9189eb39276396309faf28bf833b4328befe4513bbf375b811a36a076/ipyparallel-6.3.0-py3-none-any.whl (199kB)\n","\u001b[K     |████████████████████████████████| 204kB 39.2MB/s \n","\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (5.5.0)\n","Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (1.0.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.19.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (1.1.5)\n","Collecting periodictable\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/47/bc639be580ffa41cb859a409c71da2c7ccaf196f8b7c8aa7a0473ba84b9e/periodictable-1.6.0.tar.gz (686kB)\n","\u001b[K     |████████████████████████████████| 696kB 34.9MB/s \n","\u001b[?25hCollecting scikit-learn==0.20.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/cc/a84e1748a2a70d0f3e081f56cefc634f3b57013b16faa6926d3a6f0598df/scikit_learn-0.20.3-cp37-cp37m-manylinux1_x86_64.whl (5.4MB)\n","\u001b[K     |████████████████████████████████| 5.4MB 38.3MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (1.4.1)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (3.6.4)\n","Collecting pytest-cov\n","  Downloading https://files.pythonhosted.org/packages/ba/84/576b071aef9ac9301e5c0ff35d117e12db50b87da6f12e745e9c5f745cc2/pytest_cov-2.12.1-py2.py3-none-any.whl\n","Collecting dill==0.3.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/96/518a8ea959a734b70d2e95fef98bcbfdc7adad1c1e5f5dd9148c835205a5/dill-0.3.2.zip (177kB)\n","\u001b[K     |████████████████████████████████| 184kB 45.9MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 18)) (1.9.0+cu102)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 19)) (3.3.0)\n","Collecting codecov\n","  Downloading https://files.pythonhosted.org/packages/93/9f/bbea5b6231308458963cb5c067bc5643da9949689702fa5a382714b59699/codecov-2.1.11-py2.py3-none-any.whl\n","Collecting pyscf\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/d6/af4ce5035977cb011e4dbe9979bf254129a36d48cb569b86e57b5a72c5b1/pyscf-1.7.6.post1-cp37-cp37m-manylinux1_x86_64.whl (29.7MB)\n","\u001b[K     |████████████████████████████████| 29.7MB 1.8MB/s \n","\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 22)) (0.8.9)\n","Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from ase==3.17->-r requirements.txt (line 1)) (1.1.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.9.0->-r requirements.txt (line 5)) (1.15.0)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (5.3.5)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (22.1.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (2.8.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (4.4.2)\n","Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (5.1.1)\n","Requirement already satisfied: ipykernel>=4.4 in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (4.10.1)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (0.2.0)\n","Requirement already satisfied: traitlets>=4.3 in /usr/local/lib/python3.7/dist-packages (from ipyparallel->-r requirements.txt (line 6)) (5.0.5)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 7)) (57.0.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 7)) (2.6.1)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 7)) (0.8.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 7)) (0.7.5)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 7)) (1.0.18)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 7)) (4.8.0)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 8)) (5.1.1)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 8)) (5.3.1)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 8)) (7.6.3)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 8)) (5.2.0)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 8)) (5.6.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 9)) (1.3.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 9)) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 9)) (0.10.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 11)) (2018.9)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 15)) (1.10.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 15)) (21.2.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 15)) (8.8.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 15)) (1.4.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 15)) (0.7.1)\n","Collecting coverage>=5.2.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/e0/fc9f7bd9b84e6b41d0aad1a113e36714aac0c0a9b307aca5f9af443bc50f/coverage-5.5-cp37-cp37m-manylinux2010_x86_64.whl (242kB)\n","\u001b[K     |████████████████████████████████| 245kB 41.9MB/s \n","\u001b[?25hRequirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from pytest-cov->-r requirements.txt (line 16)) (0.10.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->-r requirements.txt (line 18)) (3.7.4.3)\n","Requirement already satisfied: requests>=2.7.9 in /usr/local/lib/python3.7/dist-packages (from codecov->-r requirements.txt (line 20)) (2.23.0)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->ase==3.17->-r requirements.txt (line 1)) (1.1.0)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->ase==3.17->-r requirements.txt (line 1)) (1.0.1)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->ase==3.17->-r requirements.txt (line 1)) (2.11.3)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->ase==3.17->-r requirements.txt (line 1)) (7.1.2)\n","Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipyparallel->-r requirements.txt (line 6)) (4.7.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->-r requirements.txt (line 7)) (0.2.5)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->-r requirements.txt (line 7)) (0.7.0)\n","Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->-r requirements.txt (line 8)) (1.9.0)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->-r requirements.txt (line 8)) (5.1.3)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->-r requirements.txt (line 8)) (1.7.1)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->-r requirements.txt (line 8)) (0.10.1)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 8)) (3.5.1)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 8)) (1.0.0)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (0.5.0)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (0.3)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (0.8.4)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (1.4.3)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (0.7.1)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (3.3.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov->-r requirements.txt (line 20)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov->-r requirements.txt (line 20)) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov->-r requirements.txt (line 20)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov->-r requirements.txt (line 20)) (1.24.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->ase==3.17->-r requirements.txt (line 1)) (2.0.1)\n","Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->-r requirements.txt (line 8)) (2.6.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->-r requirements.txt (line 8)) (21.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->-r requirements.txt (line 8)) (0.5.1)\n","Building wheels for collected packages: periodictable, dill, pylibnxc\n","  Building wheel for periodictable (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for periodictable: filename=periodictable-1.6.0-cp37-none-any.whl size=749750 sha256=284cba5d9cae2d44eb6c45c8881d4cac1fd6c1d01d7fa601db435247b3ebd741\n","  Stored in directory: /root/.cache/pip/wheels/eb/78/08/4cb95d4ae156e978980596c1f25bb8365d884de1725ef9a306\n","  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for dill: filename=dill-0.3.2-cp37-none-any.whl size=78927 sha256=2d226d78b6ba89915eac04c7fcbc930040913df3dd926ad6a7c5e6591da654b3\n","  Stored in directory: /root/.cache/pip/wheels/27/4b/a2/34ccdcc2f158742cfe9650675560dea85f78c3f4628f7daad0\n","  Building wheel for pylibnxc (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pylibnxc: filename=pylibnxc-0.1-cp37-none-any.whl size=13977 sha256=ce3117f0c5a4b5499769cd6952ae5cf9979b042165ef4e1bf91f99f66dfec9c3\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-32bdbyrl/wheels/b2/6c/b4/59f56fd21ac3b81416fd97d1d8c7016582d9219d7a51104f9c\n","Successfully built periodictable dill pylibnxc\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement h5py~=3.1.0, but you'll have h5py 2.9.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: multiprocess 0.70.12.2 has requirement dill>=0.3.4, but you'll have dill 0.3.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement coverage==3.7.1, but you'll have coverage 5.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: coveralls 0.5 has requirement coverage<3.999,>=3.6, but you'll have coverage 5.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: pytest-cov 2.12.1 has requirement pytest>=4.6, but you'll have pytest 3.6.4 which is incompatible.\u001b[0m\n","Installing collected packages: ase, h5py, ipyparallel, periodictable, scikit-learn, coverage, pytest-cov, dill, codecov, pyscf, pylibnxc\n","  Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","  Found existing installation: coverage 3.7.1\n","    Uninstalling coverage-3.7.1:\n","      Successfully uninstalled coverage-3.7.1\n","  Found existing installation: dill 0.3.4\n","    Uninstalling dill-0.3.4:\n","      Successfully uninstalled dill-0.3.4\n","Successfully installed ase-3.17.0 codecov-2.1.11 coverage-5.5 dill-0.3.2 h5py-2.9.0 ipyparallel-6.3.0 periodictable-1.6.0 pylibnxc-0.1 pyscf-1.7.6.post1 pytest-cov-2.12.1 scikit-learn-0.20.3\n","Obtaining file:///content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc\n","Installing collected packages: neuralxc\n","  Running setup.py develop for neuralxc\n","Successfully installed neuralxc\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yPsX7OhUNz6S"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"R1ElYhb3Nz83","executionInfo":{"status":"ok","timestamp":1626017847389,"user_tz":-60,"elapsed":9,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"8b31a657-a4d2-44a3-b679-0219070853a2"},"source":["pwd"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R6wvnSL-N0w9","executionInfo":{"status":"ok","timestamp":1626017855541,"user_tz":-60,"elapsed":208,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"964bec3f-c6f0-4942-80d9-1087e5643336"},"source":["cd .."],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tp9H68YTN0zX","executionInfo":{"status":"ok","timestamp":1626017857562,"user_tz":-60,"elapsed":207,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"324c8e82-63a0-4527-d9e7-0a9a5807f654"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mneuralxc\u001b[0m/      test_neural_XC_2.ipynb    test_projector_nn.ipynb\n","\u001b[01;34mneuralxc_old\u001b[0m/  test_neural_XC.ipynb\n","pre.json       test_neural_XC_new.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rGB7vxyKN01h","executionInfo":{"status":"ok","timestamp":1626389749071,"user_tz":-60,"elapsed":493,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}}},"source":["import pyscf"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"5_RvPobSN03-","executionInfo":{"status":"ok","timestamp":1626389749071,"user_tz":-60,"elapsed":5,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}}},"source":["from pyscf import dft, gto"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"V4WgyaChOZBk","executionInfo":{"status":"ok","timestamp":1626389753257,"user_tz":-60,"elapsed":185,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}}},"source":["import numpy as np"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CdUndw__Nz_G","executionInfo":{"status":"ok","timestamp":1626302651749,"user_tz":-60,"elapsed":1488,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"496e9545-8f51-4cd3-e29d-782d3a0f8c9b"},"source":["mol = gto.M(atom='O  0  0  0; H  0 1 0 ; H 0 0 1', basis='6-31g*')\n","mf = dft.RKS(mol)\n","mf.xc = 'PBE'\n","mf.grids.level = 3\n","mf.kernel()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["converged SCF energy = -76.3154747149681\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["-76.31547471496809"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"IajhGStlOQqF","executionInfo":{"status":"ok","timestamp":1626302653422,"user_tz":-60,"elapsed":349,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}}},"source":["rho = pyscf.dft.numint.get_rho(mf._numint, mol, mf.make_rdm1(), mf.grids)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CQf8kar6OQsY","executionInfo":{"status":"ok","timestamp":1626381570772,"user_tz":-60,"elapsed":336,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"b0b13ea5-1f6a-4f13-938b-032cd9956d92"},"source":["rho"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2.91089561e+02, 2.91063025e+02, 2.90911157e+02, ...,\n","       1.88899499e-03, 1.82087871e-03, 1.82087871e-03])"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FkNO3WFqPiGI","executionInfo":{"status":"ok","timestamp":1626302660355,"user_tz":-60,"elapsed":434,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"f7c5db52-9b5d-402b-e383-9fabfd9d4b4e"},"source":["rho.shape #(29193, )"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(29193,)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mNSN9gTEPi-y","executionInfo":{"status":"ok","timestamp":1626302663095,"user_tz":-60,"elapsed":3,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"fa9a8101-b3eb-4750-9ed4-77593d81f1ac"},"source":[" mf.grids.weights #(29193,)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.39057893e-13, 4.54432003e-12, 5.39245502e-11, ...,\n","       4.74191911e-02, 5.59911427e-02, 5.59911427e-02])"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oiCHPBfzPsgB","executionInfo":{"status":"ok","timestamp":1626302667296,"user_tz":-60,"elapsed":723,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"a0b8d882-4cf8-44eb-c0ff-3b57b6e3aa68"},"source":["mf.grids.coords #(29193, 3)"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 8.16664970e-05,  0.00000000e+00,  0.00000000e+00],\n","       [ 2.98885715e-04,  0.00000000e+00,  0.00000000e+00],\n","       [ 7.50353928e-04,  0.00000000e+00,  0.00000000e+00],\n","       ...,\n","       [-2.53740746e+00,  1.16059666e+00,  1.54288794e+00],\n","       [ 2.53740746e+00, -1.16059666e+00,  1.54288794e+00],\n","       [-2.53740746e+00, -1.16059666e+00,  1.54288794e+00]])"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pbYqLP7eRPik","executionInfo":{"status":"ok","timestamp":1626389562454,"user_tz":-60,"elapsed":205,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"739a2b8b-9685-44dd-8ad3-9175bf03534b"},"source":["import sys\n","sys.path"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '/content',\n"," '/env/python',\n"," '/usr/lib/python37.zip',\n"," '/usr/lib/python3.7',\n"," '/usr/lib/python3.7/lib-dynload',\n"," '/usr/local/lib/python3.7/dist-packages',\n"," '/usr/lib/python3/dist-packages',\n"," '/usr/local/lib/python3.7/dist-packages/IPython/extensions',\n"," '/root/.ipython']"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"YBSytRkbRbaV","executionInfo":{"status":"ok","timestamp":1626389565496,"user_tz":-60,"elapsed":209,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}}},"source":["sys.path.append('/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/')"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"POo-XPDEReY0","executionInfo":{"status":"ok","timestamp":1626019630738,"user_tz":-60,"elapsed":3,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"2b571b76-a646-43ec-b4a8-867b1b9e14a2"},"source":["sys.path"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '/content',\n"," '/env/python',\n"," '/usr/lib/python37.zip',\n"," '/usr/lib/python3.7',\n"," '/usr/lib/python3.7/lib-dynload',\n"," '/usr/local/lib/python3.7/dist-packages',\n"," '/usr/lib/python3/dist-packages',\n"," '/usr/local/lib/python3.7/dist-packages/IPython/extensions',\n"," '/root/.ipython',\n"," '/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/']"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"x6Z1jzo6OQuq"},"source":["import neuralxc as xc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i4ebxHAYVM5V"},"source":["import neuralxc.ml.utils"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rfvHMq1MQ0Ja","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"error","timestamp":1626389589055,"user_tz":-60,"elapsed":244,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"30ae864e-f200-40da-8adb-cf6160026b6b"},"source":["from neuralxc.projector import DensityProjector"],"execution_count":16,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-ad6fb5bd5407>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mneuralxc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDensityProjector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neuralxc'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":334},"id":"tmV41Lx9OQxX","executionInfo":{"status":"error","timestamp":1626381575455,"user_tz":-60,"elapsed":365,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"7ee6161d-47c3-4339-c69b-d83c280a8fb9"},"source":["density_projector = DensityProjector(grid_coords=mf.grids.coords, grid_weights=mf.grids.weights,\n","                                                  basis_instructions={'application':'pyscf','basis': {'O': {'n': 2, 'l': 3, 'r_o': 1}, 'H': {'n': 2, 'l': 2, 'r_o': 1.5}},\n","                    'projector': 'ortho', 'grid' : 'radial'})"],"execution_count":25,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-c822c1332c46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m density_projector = DensityProjector(grid_coords=mf.grids.coords, grid_weights=mf.grids.weights,\n\u001b[1;32m      2\u001b[0m                                                   basis_instructions={'application':'pyscf','basis': {'O': {'n': 2, 'l': 3, 'r_o': 1}, 'H': {'n': 2, 'l': 2, 'r_o': 1.5}},\n\u001b[0;32m----> 3\u001b[0;31m                     'projector': 'ortho', 'grid' : 'radial'})\n\u001b[0m","\u001b[0;32m/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/projector/projector.py\u001b[0m in \u001b[0;36mDensityProjector\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Projector: {} not registered'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojector_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprojector_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'mol'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JgU_fkIEXN3l","executionInfo":{"status":"ok","timestamp":1626389423811,"user_tz":-60,"elapsed":10550,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"d104422c-4c81-4fa5-f140-20e0bbf2d80c"},"source":["!pip install pyscf"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting pyscf\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/d6/af4ce5035977cb011e4dbe9979bf254129a36d48cb569b86e57b5a72c5b1/pyscf-1.7.6.post1-cp37-cp37m-manylinux1_x86_64.whl (29.7MB)\n","\u001b[K     |████████████████████████████████| 29.7MB 196kB/s \n","\u001b[?25hRequirement already satisfied: scipy!=1.5.0,!=1.5.1 in /usr/local/lib/python3.7/dist-packages (from pyscf) (1.4.1)\n","Requirement already satisfied: h5py>2.2 in /usr/local/lib/python3.7/dist-packages (from pyscf) (3.1.0)\n","Requirement already satisfied: numpy!=1.16,!=1.17,>1.8 in /usr/local/lib/python3.7/dist-packages (from pyscf) (1.19.5)\n","Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py>2.2->pyscf) (1.5.2)\n","Installing collected packages: pyscf\n","Successfully installed pyscf-1.7.6.post1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CL9ctM2KXFrl","executionInfo":{"status":"ok","timestamp":1626389460191,"user_tz":-60,"elapsed":255,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}}},"source":["import pyscf\n","from pyscf import dft, gto"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9ZKA3iyXuFl","executionInfo":{"status":"ok","timestamp":1626389781895,"user_tz":-60,"elapsed":10994,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}}},"source":["import neuralxc as xc"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pLuGxlLoXFuF","executionInfo":{"status":"ok","timestamp":1626389786148,"user_tz":-60,"elapsed":1270,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"7aa27f12-a23d-4f00-ba2d-4dcadddf1774"},"source":["mol = gto.M(atom='O  0  0  0; H  0 1 0 ; H 0 0 1', basis='6-31g*')\n","mf = dft.RKS(mol)\n","mf.xc = 'PBE'\n","mf.kernel()\n","rho = pyscf.dft.numint.get_rho(mf._numint, mol, mf.make_rdm1(), mf.grids)\n","print('Rho shape', rho.shape)\n","print('Weights shape', mf.grids.weights.shape)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["converged SCF energy = -76.3154747149681\n","Rho shape (29193,)\n","Weights shape (29193,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fuMVLThoZFQ9","executionInfo":{"status":"ok","timestamp":1626390253962,"user_tz":-60,"elapsed":196,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}}},"source":["basis_instructions = {\n","  \"preprocessor\":\n","  {\n","       \"basis\": \"ccpvtz-jkfit\",\n","       \"projector\": \"gaussian\",\n","       \"grid\": \"radial\",\n","       \"extension\": \"chkpt\"\n","  },\n","  \"n_workers\" : 1,\n","  \"engine\": {\"xc\": \"PBE\",\n","             \"application\": \"pyscf\",\n","             \"basis\" : \"def2-TZVP\"}\n","\n","}"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"G5yi1q9TXFzl","executionInfo":{"status":"ok","timestamp":1626390425219,"user_tz":-60,"elapsed":196,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}}},"source":[" density_projector = xc.projector.RadialProjector(grid_coords=mf.grids.coords, grid_weights=mf.grids.weights,\n","                                                   basis_instructions=basis_instructions)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"I5WuOZ0VXF1m","executionInfo":{"status":"error","timestamp":1626390442374,"user_tz":-60,"elapsed":205,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"30a99576-6768-4281-f37c-684cec47cb2c"},"source":[" density_projector()"],"execution_count":18,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-b813428f0b20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdensity_projector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: forward() missing 6 required positional arguments: 'rho', 'positions', 'species', 'unitcell', 'grid', and 'my_box'"]}]},{"cell_type":"code","metadata":{"id":"TROI37BwXF4P"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_swYHm1DXF7Q"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o_1xBlDHPJlJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626384038396,"user_tz":-60,"elapsed":3591,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"a0518fd3-ad9e-4961-8830-259adef34528"},"source":["!neuralxc fit -h"],"execution_count":26,"outputs":[{"output_type":"stream","text":["usage: neuralxc fit [-h] [--hdf5 hdf5 hdf5 hdf5] [--sets sets]\n","                    [--sample sample] [--model model] [--hyperopt]\n","                    preprocessor hyper\n","\n","Fit a NeuralXC model\n","\n","positional arguments:\n","  preprocessor          Path to configuration file for preprocessor\n","  hyper                 Path to .json configuration file setting\n","                        hyperparameters\n","\n","optional arguments:\n","  -h, --help            show this help message and exit\n","  --hdf5 hdf5 hdf5 hdf5\n","                        Path to hdf5 file, baseline data, reference data\n","  --sets sets           Path to file defining sets\n","  --sample sample       Only use a subsample of data contained in hdf5 file\n","  --model model         Continue training model found at this location\n","  --hyperopt            Do a hyperparameter optimzation\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SrkTAw4TPJnl","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1626384424565,"user_tz":-60,"elapsed":426,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"02d8d47a-a346-4deb-e386-452bacbde367"},"source":[""],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc'"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"ZT8RPYREPJp1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626384437249,"user_tz":-60,"elapsed":466,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"737fef3d-9fa9-4483-dc73-724878dd2e53"},"source":["cd examples/quickstart/"],"execution_count":28,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bPFcjENnPJsJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626384448930,"user_tz":-60,"elapsed":419,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"231c91b3-546e-42d6-9815-dfe5742a6598"},"source":["ls sc/"],"execution_count":30,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mbest_model\u001b[0m/       data.hdf5   \u001b[01;34mmodel_it1.jit\u001b[0m/  pre.json        statistics_sc\n","best_params.json  hyper.json  \u001b[01;34mmodel_it2\u001b[0m/      sets.inp        \u001b[01;34mworkdir\u001b[0m/\n","cv_results.csv    \u001b[01;34mmodel_it1\u001b[0m/  \u001b[01;34mmodel_it2.jit\u001b[0m/  statistics_fit\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kR_uV3etN0Bf","executionInfo":{"status":"ok","timestamp":1626385254367,"user_tz":-60,"elapsed":339,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}}},"source":["from neuralxc.ml.utils import load_sets, load_data"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"i8hVHPU8EZCG","executionInfo":{"status":"ok","timestamp":1626384539872,"user_tz":-60,"elapsed":388,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}}},"source":["import h5py"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"hjQJBSClEZEZ","executionInfo":{"status":"ok","timestamp":1626385433431,"user_tz":-60,"elapsed":423,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}}},"source":["f = h5py.File('sc/data.hdf5')"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jH_iKOC9EZHF","executionInfo":{"status":"ok","timestamp":1626385106364,"user_tz":-60,"elapsed":334,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"015e8819-f8e2-4acc-ae55-404a0ca41f96"},"source":["f['system/it0/density/97a66c91908d8f76f249705362d9e536'][:, :]"],"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[8.6250252 , 0.        , 0.        , ..., 0.        , 0.        ,\n","        0.        ],\n","       [8.61334309, 0.        , 0.        , ..., 0.        , 0.        ,\n","        0.        ],\n","       [8.61657682, 0.        , 0.        , ..., 0.        , 0.        ,\n","        0.        ],\n","       ...,\n","       [8.61548783, 0.        , 0.        , ..., 0.        , 0.        ,\n","        0.        ],\n","       [8.6108232 , 0.        , 0.        , ..., 0.        , 0.        ,\n","        0.        ],\n","       [8.61684568, 0.        , 0.        , ..., 0.        , 0.        ,\n","        0.        ]])"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"code","metadata":{"id":"Q0JuCh0WE_q9","executionInfo":{"status":"ok","timestamp":1626384634372,"user_tz":-60,"elapsed":409,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}}},"source":["f.close()"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKo_pACXHZKw","executionInfo":{"status":"ok","timestamp":1626385483189,"user_tz":-60,"elapsed":372,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}}},"source":["X,y = load_data(h5py.File('sc/data.hdf5'), 'system/it0', 'system/ref', '97a66c91908d8f76f249705362d9e536',E0=0)"],"execution_count":66,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ze_v9JY9IQrP","executionInfo":{"status":"ok","timestamp":1626385497728,"user_tz":-60,"elapsed":409,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"d42e3108-eef6-41ff-9ee6-7cb53e0663ad"},"source":["np.shape(X)"],"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10, 525)"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KP3vELe9IUSj","executionInfo":{"status":"ok","timestamp":1626385505511,"user_tz":-60,"elapsed":412,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"90cd96ca-68f1-4ea7-f6c0-04921b5f6509"},"source":["np.shape(y)"],"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10,)"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"code","metadata":{"id":"-I5JAMdwEwXF","executionInfo":{"status":"ok","timestamp":1626385518864,"user_tz":-60,"elapsed":452,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}}},"source":["loaded_data = load_sets(h5py.File('sc/data.hdf5'), 'system/it0', 'system/ref', basis_key='97a66c91908d8f76f249705362d9e536')"],"execution_count":70,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5gTP7b-xEwZg","executionInfo":{"status":"ok","timestamp":1626385532545,"user_tz":-60,"elapsed":412,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"e26498ff-79d9-4fdc-d964-b58b265a14da"},"source":["np.shape(loaded_data)"],"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10, 527)"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GoIidJMYEwb4","executionInfo":{"status":"ok","timestamp":1626385691093,"user_tz":-60,"elapsed":329,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"03d1238e-db25-4037-967c-288234cc5a76"},"source":["np.shape(loaded_data[0])"],"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(527,)"]},"metadata":{"tags":[]},"execution_count":74}]},{"cell_type":"code","metadata":{"id":"x2Us61bJEweV"},"source":["from neuralxc.symmetrizer.symmetrizer import "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OtUpKOCyMsuh","executionInfo":{"status":"ok","timestamp":1625825250328,"user_tz":-60,"elapsed":6563,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"a99bd902-6786-4188-8874-5b103c79d7b2"},"source":["!pip install -U pytest #update pytest"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pytest\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/59/6821e900592fbe261f19d67e4def0cb27e52ef8ed16d9922c144961cc1ee/pytest-6.2.4-py3-none-any.whl (280kB)\n","\r\u001b[K     |█▏                              | 10kB 17.3MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 23.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30kB 29.5MB/s eta 0:00:01\r\u001b[K     |████▊                           | 40kB 26.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 51kB 16.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 61kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 71kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 81kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 92kB 16.9MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 102kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 112kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 122kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 133kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 143kB 15.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 153kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 163kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 174kB 15.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 184kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 194kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 204kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 215kB 15.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 225kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 235kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 245kB 15.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 256kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 266kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 276kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 286kB 15.4MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest) (1.1.1)\n","Collecting pluggy<1.0.0a1,>=0.12\n","  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: toml in /usr/local/lib/python3.7/dist-packages (from pytest) (0.10.2)\n","Requirement already satisfied, skipping upgrade: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from pytest) (4.6.0)\n","Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from pytest) (20.9)\n","Requirement already satisfied, skipping upgrade: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (21.2.0)\n","Requirement already satisfied, skipping upgrade: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.10.0)\n","Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest) (3.4.1)\n","Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytest) (2.4.7)\n","\u001b[31mERROR: datascience 0.10.6 has requirement coverage==3.7.1, but you'll have coverage 5.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","Installing collected packages: pluggy, pytest\n","  Found existing installation: pluggy 0.7.1\n","    Uninstalling pluggy-0.7.1:\n","      Successfully uninstalled pluggy-0.7.1\n","  Found existing installation: pytest 3.6.4\n","    Uninstalling pytest-3.6.4:\n","      Successfully uninstalled pytest-3.6.4\n","Successfully installed pluggy-0.13.1 pytest-6.2.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nwMkIwIJPRAL","executionInfo":{"status":"ok","timestamp":1625825487033,"user_tz":-60,"elapsed":230904,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"9cae6227-78f4-42cc-98ef-bdbbdea1c7cb"},"source":["!pytest -v"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.7.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc\n","plugins: cov-2.12.1, typeguard-2.7.1\n","collected 29 items                                                             \u001b[0m\n","\n","neuralxc/tests/test_drivers.py::test_fit \u001b[32mPASSED\u001b[0m\u001b[32m                          [  3%]\u001b[0m\n","neuralxc/tests/test_drivers.py::test_eval \u001b[32mPASSED\u001b[0m\u001b[32m                         [  6%]\u001b[0m\n","neuralxc/tests/test_drivers.py::test_data \u001b[32mPASSED\u001b[0m\u001b[32m                         [ 10%]\u001b[0m\n","neuralxc/tests/test_drivers.py::test_serialize \u001b[32mPASSED\u001b[0m\u001b[33m                    [ 13%]\u001b[0m\n","neuralxc/tests/test_neuralxc.py::test_siesta_density_getter \u001b[32mPASSED\u001b[0m\u001b[33m       [ 17%]\u001b[0m\n","neuralxc/tests/test_neuralxc.py::test_formatter \u001b[32mPASSED\u001b[0m\u001b[33m                   [ 20%]\u001b[0m\n","neuralxc/tests/test_neuralxc.py::test_grouped_transformers[transformer0-/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/tests/scaler.pckl] \u001b[32mPASSED\u001b[0m\u001b[33m [ 24%]\u001b[0m\n","neuralxc/tests/test_neuralxc.py::test_grouped_transformers[transformer1-/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/tests/var09.pckl] \u001b[32mPASSED\u001b[0m\u001b[33m [ 27%]\u001b[0m\n","neuralxc/tests/test_neuralxc.py::test_species_grouper \u001b[32mPASSED\u001b[0m\u001b[33m             [ 31%]\u001b[0m\n","neuralxc/tests/test_neuralxc.py::test_neuralxc_benzene \u001b[32mPASSED\u001b[0m\u001b[33m            [ 34%]\u001b[0m\n","neuralxc/tests/test_projectors.py::test_density_projector[ortho] \u001b[32mPASSED\u001b[0m\u001b[33m  [ 37%]\u001b[0m\n","neuralxc/tests/test_projectors.py::test_jacobs_projector[ortho-euclidean] \u001b[32mPASSED\u001b[0m\u001b[33m [ 41%]\u001b[0m\n","neuralxc/tests/test_projectors.py::test_jacobs_projector[ortho-radial] \u001b[32mPASSED\u001b[0m\u001b[33m [ 44%]\u001b[0m\n","neuralxc/tests/test_projectors.py::test_jacobs_projector[gaussian-euclidean] \u001b[32mPASSED\u001b[0m\u001b[33m [ 48%]\u001b[0m\n","neuralxc/tests/test_projectors.py::test_jacobs_projector[gaussian-radial] \u001b[32mPASSED\u001b[0m\u001b[33m [ 51%]\u001b[0m\n","neuralxc/tests/test_projectors.py::test_radial_projector \u001b[32mPASSED\u001b[0m\u001b[33m          [ 55%]\u001b[0m\n","neuralxc/tests/test_projectors.py::test_radial_gaussian \u001b[32mPASSED\u001b[0m\u001b[33m           [ 58%]\u001b[0m\n","neuralxc/tests/test_projectors.py::test_gaussian_projector \u001b[32mPASSED\u001b[0m\u001b[33m        [ 62%]\u001b[0m\n","neuralxc/tests/test_projectors.py::test_gaussian_serialized \u001b[32mPASSED\u001b[0m\u001b[33m       [ 65%]\u001b[0m\n","neuralxc/tests/test_pyscf.py::test_radial_model \u001b[32mPASSED\u001b[0m\u001b[33m                   [ 68%]\u001b[0m\n","neuralxc/tests/test_pyscf.py::test_pre \u001b[32mPASSED\u001b[0m\u001b[33m                            [ 72%]\u001b[0m\n","neuralxc/tests/test_pyscf.py::test_sc[ga_ana] \u001b[32mPASSED\u001b[0m\u001b[33m                     [ 75%]\u001b[0m\n","neuralxc/tests/test_pyscf.py::test_sc[ga_rad] \u001b[32mPASSED\u001b[0m\u001b[33m                     [ 79%]\u001b[0m\n","neuralxc/tests/test_pyscf.py::test_sc[or_rad] \u001b[32mPASSED\u001b[0m\u001b[33m                     [ 82%]\u001b[0m\n","neuralxc/tests/test_pyscf.py::test_sc[ga_ana_f] \u001b[32mPASSED\u001b[0m\u001b[33m                   [ 86%]\u001b[0m\n","neuralxc/tests/test_pyscf.py::test_sc[ga_rad_f] \u001b[32mPASSED\u001b[0m\u001b[33m                   [ 89%]\u001b[0m\n","neuralxc/tests/test_pyscf.py::test_pyscf_radial \u001b[31mFAILED\u001b[0m\u001b[31m                   [ 93%]\u001b[0m\n","neuralxc/tests/test_torch_comp.py::test_mybox \u001b[32mPASSED\u001b[0m\u001b[31m                     [ 96%]\u001b[0m\n","neuralxc/tests/test_torch_comp.py::test_stress \u001b[32mPASSED\u001b[0m\u001b[31m                    [100%]\u001b[0m\n","\n","=================================== FAILURES ===================================\n","\u001b[31m\u001b[1m______________________________ test_pyscf_radial _______________________________\u001b[0m\n","\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_pyscf_radial\u001b[39;49;00m():\n","        os.chdir(test_dir)\n","        shcopytree(test_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/driver_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, test_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/driver_data_tmp\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n","        cwd = os.getcwd()\n","        os.chdir(test_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/driver_data_tmp\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n","    \n","        serialize(\u001b[33m'\u001b[39;49;00m\u001b[33mmodel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mbenzene.pyscf.jit\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, as_radial=\u001b[94mFalse\u001b[39;49;00m)\n","        engine = Engine(\u001b[33m'\u001b[39;49;00m\u001b[33mpyscf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, nxc=\u001b[33m'\u001b[39;49;00m\u001b[33mbenzene.pyscf.jit\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",">       atoms = engine.compute(read(\u001b[33m'\u001b[39;49;00m\u001b[33mbenzene_small.traj\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n","\n","\u001b[1m\u001b[31m/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/tests/test_pyscf.py\u001b[0m:128: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31m/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/engines/engine.py\u001b[0m:66: in compute\n","    mf, mol = compute_KS(atoms, basis=\u001b[96mself\u001b[39;49;00m.basis, xc=\u001b[96mself\u001b[39;49;00m.xc, nxc=\u001b[96mself\u001b[39;49;00m.nxc, **\u001b[96mself\u001b[39;49;00m.engine_kwargs)\n","\u001b[1m\u001b[31m/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/pyscf/pyscf.py\u001b[0m:57: in compute_KS\n","    mf.kernel()\n","\u001b[1m\u001b[31m/usr/local/lib/python3.7/dist-packages/pyscf/lib/misc.py\u001b[0m:636: in aliased_fn\n","    \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, fname)(*args, **kwargs)\n","\u001b[1m\u001b[31m/usr/local/lib/python3.7/dist-packages/pyscf/scf/hf.py\u001b[0m:1650: in scf\n","    conv_check=\u001b[96mself\u001b[39;49;00m.conv_check, **kwargs)\n","\u001b[1m\u001b[31m/usr/local/lib/python3.7/dist-packages/pyscf/scf/hf.py\u001b[0m:197: in kernel\n","    mf.dump_chk(\u001b[96mlocals\u001b[39;49;00m())\n","\u001b[1m\u001b[31m/usr/local/lib/python3.7/dist-packages/pyscf/scf/hf.py\u001b[0m:1521: in dump_chk\n","    overwrite_mol=\u001b[94mFalse\u001b[39;49;00m)\n","\u001b[1m\u001b[31m/usr/local/lib/python3.7/dist-packages/pyscf/scf/chkfile.py\u001b[0m:30: in dump_scf\n","    \u001b[94mif\u001b[39;49;00m h5py.is_hdf5(chkfile) \u001b[95mand\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m overwrite_mol:\n","\u001b[1m\u001b[31m/usr/local/lib/python3.7/dist-packages/h5py/_hl/base.py\u001b[0m:41: in is_hdf5\n","    fname = os.path.abspath(fspath(fname))\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","path = 'pyscf.chkpt'\n","\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mabspath\u001b[39;49;00m(path):\n","        \u001b[33m\"\"\"Return an absolute path.\"\"\"\u001b[39;49;00m\n","        path = os.fspath(path)\n","        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m isabs(path):\n","            \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(path, \u001b[96mbytes\u001b[39;49;00m):\n","                cwd = os.getcwdb()\n","            \u001b[94melse\u001b[39;49;00m:\n",">               cwd = os.getcwd()\n","\u001b[1m\u001b[31mE               FileNotFoundError: [Errno 2] No such file or directory\u001b[0m\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.7/posixpath.py\u001b[0m:383: FileNotFoundError\n","----------------------------- Captured stdout call -----------------------------\n","Using symmetrizer  casimir\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/tests/driver_data_tmp/benzene.pyscf.jit\n","NeuralXC: Model successfully loaded\n","----------------------------- Captured stderr call -----------------------------\n","Exception ignored in: <function _TemporaryFileCloser.__del__ at 0x7f7fd98d6050>\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/tempfile.py\", line 587, in __del__\n","    self.close()\n","  File \"/usr/lib/python3.7/tempfile.py\", line 583, in close\n","    unlink(self.name)\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/tests/driver_data_tmp/testing/workdir/1/tmpw4jw_1rr'\n","\u001b[33m=============================== warnings summary ===============================\u001b[0m\n","neuralxc/tests/test_drivers.py: 2 warnings\n","neuralxc/tests/test_pyscf.py: 27 warnings\n","  /usr/local/lib/python3.7/dist-packages/ase/io/jsonio.py:58: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","    a = np.array(obj)\n","\n","neuralxc/tests/test_drivers.py::test_serialize\n","neuralxc/tests/test_pyscf.py::test_pyscf_radial\n","  /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/ml/transformer.py:150: TracerWarning: torch.from_numpy results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n","\n","neuralxc/tests/test_drivers.py::test_serialize\n","neuralxc/tests/test_pyscf.py::test_pyscf_radial\n","  /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/ml/transformer.py:172: TracerWarning: torch.from_numpy results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n","\n","neuralxc/tests/test_projectors.py::test_gaussian_serialized\n","neuralxc/tests/test_pyscf.py::test_sc[ga_rad]\n","neuralxc/tests/test_pyscf.py::test_sc[or_rad]\n","neuralxc/tests/test_pyscf.py::test_sc[ga_rad_f]\n","  /usr/local/lib/python3.7/dist-packages/opt_einsum/contract.py:231: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","    dim = int(sh[cnum])\n","\n","neuralxc/tests/test_projectors.py::test_gaussian_serialized\n","neuralxc/tests/test_pyscf.py::test_sc[ga_rad]\n","neuralxc/tests/test_pyscf.py::test_sc[or_rad]\n","neuralxc/tests/test_pyscf.py::test_sc[ga_rad_f]\n","  /usr/local/lib/python3.7/dist-packages/opt_einsum/parser.py:155: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","    max(shape[loc] for shape, loc in zip(shapes, [x.find(c) for x in inputs]) if loc >= 0) for c in output)\n","\n","neuralxc/tests/test_projectors.py::test_gaussian_serialized\n","neuralxc/tests/test_pyscf.py::test_sc[ga_rad]\n","neuralxc/tests/test_pyscf.py::test_sc[or_rad]\n","neuralxc/tests/test_pyscf.py::test_sc[ga_rad_f]\n","  /usr/local/lib/python3.7/dist-packages/opt_einsum/blas.py:76: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","    if shapes[0][input_left.find(c)] != shapes[1][input_right.find(c)]:\n","\n","neuralxc/tests/test_projectors.py::test_gaussian_serialized\n","neuralxc/tests/test_pyscf.py::test_sc[ga_rad]\n","neuralxc/tests/test_pyscf.py::test_sc[ga_rad_f]\n","  /usr/local/lib/python3.7/dist-packages/torch/_tensor.py:643: TracerWarning: Converting a tensor to a NumPy array might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","    return self.numpy()\n","\n","neuralxc/tests/test_projectors.py::test_gaussian_serialized\n","neuralxc/tests/test_pyscf.py::test_sc[ga_rad]\n","neuralxc/tests/test_pyscf.py::test_sc[ga_rad_f]\n","  /usr/local/lib/python3.7/dist-packages/torch/_tensor.py:655: TracerWarning: torch.from_numpy results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n","    return torch.from_numpy(array)\n","\n","neuralxc/tests/test_projectors.py::test_gaussian_serialized\n","  /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/projector/gaussian.py:262: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n","    Xm, Ym, Zm = box['mesh'].long()\n","\n","neuralxc/tests/test_pyscf.py::test_sc[ga_ana]\n","neuralxc/tests/test_pyscf.py::test_sc[ga_rad]\n","neuralxc/tests/test_pyscf.py::test_sc[or_rad]\n","neuralxc/tests/test_pyscf.py::test_sc[ga_ana_f]\n","neuralxc/tests/test_pyscf.py::test_sc[ga_rad_f]\n","  /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/ml/transformer.py:155: TracerWarning: torch.from_numpy results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n","    support = torch.from_numpy(self.get_support()).bool()\n","\n","neuralxc/tests/test_pyscf.py::test_sc[ga_ana]\n","neuralxc/tests/test_pyscf.py::test_sc[ga_rad]\n","neuralxc/tests/test_pyscf.py::test_sc[or_rad]\n","neuralxc/tests/test_pyscf.py::test_sc[ga_ana_f]\n","neuralxc/tests/test_pyscf.py::test_sc[ga_rad_f]\n","  /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/ml/transformer.py:177: TracerWarning: torch.from_numpy results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n","    X = (X - torch.from_numpy(self.mean_)) / torch.sqrt(torch.from_numpy(self.var_))\n","\n","neuralxc/tests/test_pyscf.py::test_sc[or_rad]\n","  /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/projector/polynomial.py:84: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n","    for k in torch.arange(0, W.size()[0]):\n","\n","neuralxc/tests/test_pyscf.py::test_sc[or_rad]\n","  /usr/local/lib/python3.7/dist-packages/torch/_tensor.py:571: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n","    return torch.tensor(other, dtype=dtype, device=self.device) ** self\n","\n","neuralxc/tests/test_pyscf.py::test_sc[or_rad]\n","  /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/projector/polynomial.py:77: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","    (2*a+6)*(2*a+5)))\n","\n","-- Docs: https://docs.pytest.org/en/stable/warnings.html\n","=========================== short test summary info ============================\n","FAILED neuralxc/tests/test_pyscf.py::test_pyscf_radial - FileNotFoundError: [...\n","\u001b[31m============ \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m28 passed\u001b[0m, \u001b[33m65 warnings\u001b[0m\u001b[31m in 228.55s (0:03:48)\u001b[0m\u001b[31m =============\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tN4bo2WIVB50"},"source":["import copy\n","import os\n","import shutil\n","import sys\n","from abc import ABC, abstractmethod\n","\n","import dill as pickle\n","import json\n","import h5py\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pytest\n","from ase.io import read\n","\n","import neuralxc as xc\n","from neuralxc.constants import Bohr, Hartree\n","from neuralxc.drivers import *\n","from neuralxc.engines import Engine\n","from neuralxc.utils import ConfigFile"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Ir3KbWgVEAl"},"source":["os.chdir('neuralxc/tests/driver_data_tmp')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ECVSBkaXVaT8","executionInfo":{"status":"ok","timestamp":1625825506214,"user_tz":-60,"elapsed":541,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"de9011ba-e176-405e-98d1-3d444273c6b8"},"source":["serialize('model', 'benzene.pyscf.jit', as_radial=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using symmetrizer  casimir\n","Success!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ICKvUPjlVrTX","executionInfo":{"status":"ok","timestamp":1625825507627,"user_tz":-60,"elapsed":779,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"73a7d005-2089-442c-b209-a0d6e29fb86e"},"source":["engine = Engine('pyscf', nxc='benzene.pyscf.jit')\n","atoms = engine.compute(read('benzene_small.traj', '0'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Re-using results\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oE3R-L2DWFzI","executionInfo":{"status":"ok","timestamp":1625825507899,"user_tz":-60,"elapsed":4,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"c30263f6-d7eb-4ec6-9fed-50705210b5c8"},"source":[" atoms.get_potential_energy()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-6311.410418033626"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GCUAl9TKWJEg","executionInfo":{"status":"ok","timestamp":1625825512938,"user_tz":-60,"elapsed":953,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"3a62ac97-782b-4b4d-cd86-0fe6323de77d"},"source":["serialize('model', 'benzene.pyscf_radial.jit', as_radial=True)\n","engine = Engine('pyscf', nxc='benzene.pyscf_radial.jit')\n","atoms_rad = engine.compute(read('benzene_small.traj', '0'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using symmetrizer  casimir\n","Using symmetrizer  casimir\n","Success!\n","Re-using results\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q7wR5wxeuNYb","executionInfo":{"status":"ok","timestamp":1625825514646,"user_tz":-60,"elapsed":494,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"8068e23b-2c54-4c64-f06a-c3f9075cbfc2"},"source":[" atoms_rad.get_potential_energy()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-6311.410418033626"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KXUOhia2wddb","executionInfo":{"status":"ok","timestamp":1625825612615,"user_tz":-60,"elapsed":364,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"be186d2a-ce5f-43ed-fea0-ce5497d36446"},"source":["cd ../../../"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I6GJuEM6W0QL","executionInfo":{"status":"ok","timestamp":1625825615497,"user_tz":-60,"elapsed":320,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"71b9aa4c-7f74-4b6b-d157-32c7cbffc26c"},"source":["ls examples/inputs/ml_basis"],"execution_count":null,"outputs":[{"output_type":"stream","text":["basis_mob_ml.json         basis_sgdml_malonaldehyde.json\n","basis_nxcw01.json         basis_sgdml_toluene.json\n","basis_sgdml_benzene.json  basis_sgdml_water.json\n","basis_sgdml_ethanol.json\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GV0j19J1i4CG"},"source":["##Command - neuralxc help"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MgyBs28Inlkx","executionInfo":{"status":"ok","timestamp":1625701363870,"user_tz":-60,"elapsed":2575,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"29b9ff6a-34b5-4b5c-de21-1e1319ca1fe5"},"source":["!neuralxc --help"],"execution_count":null,"outputs":[{"output_type":"stream","text":["usage: neuralxc [-h]\n","                {basis,data,fit,sc,eval,predict,serialize,pre,default,engine}\n","                ...\n","\n","Add data to hdf5 file\n","\n","positional arguments:\n","  {basis,data,fit,sc,eval,predict,serialize,pre,default,engine}\n","\n","optional arguments:\n","  -h, --help            show this help message and exit\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vH0Hh3UMWl-q"},"source":["### neuralxc basis"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nY6qvN01WlUd","executionInfo":{"status":"ok","timestamp":1625701414043,"user_tz":-60,"elapsed":2484,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"d08f71c6-aac1-4806-c2e3-fd7f3f9cd88b"},"source":["!neuralxc basis -h"],"execution_count":null,"outputs":[{"output_type":"stream","text":["usage: neuralxc basis [-h] basis\n","\n","Plot radial basis functions\n","\n","positional arguments:\n","  basis       Path to .json file containing the basis to plot\n","\n","optional arguments:\n","  -h, --help  show this help message and exit\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZdxmbqVYmgAR"},"source":["### neuralxc sc"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bECv5gJwtCB9","executionInfo":{"status":"ok","timestamp":1623794681577,"user_tz":-60,"elapsed":2429,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"08a3b3d8-8c7e-4c40-a22b-8091613fcbbc"},"source":["!neuralxc sc -h"],"execution_count":null,"outputs":[{"output_type":"stream","text":["usage: neuralxc sc [-h] [--data data] [--maxit maxit] [--tol tol]\n","                   [--sets sets] [--nozero] [--model0 model0] [--hyperopt]\n","                   xyz preprocessor hyper\n","\n","Fit a NeuralXC model selfconsistently\n","\n","positional arguments:\n","  xyz              Path to .xyz/.traj file containing structures and reference\n","                   data\n","  preprocessor     Path to configuration file for preprocessor\n","  hyper            Path to .json configuration file setting hyperparameters\n","\n","optional arguments:\n","  -h, --help       show this help message and exit\n","  --data data      Start from this dataset instead of computing iteration 0\n","  --maxit maxit    Maximum number of iterations (default: 5)\n","  --tol tol        Tolerance in energy defining whether iterative training\n","                   converged (default: 0.0005 eV)\n","  --sets sets      Path to file defining sets\n","  --nozero         Do not automatically set energy origins for every dataset\n","                   by using min\n","  --model0 model0  Build new model on top of model0 as a stacked estimator\n","  --hyperopt       Do a hyperparameter optimzation\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lWjjZ9R6mnVZ"},"source":["### neuralxc eval"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"byWSDcGRRCJg","executionInfo":{"status":"ok","timestamp":1623794678196,"user_tz":-60,"elapsed":31747,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"c9416ff9-df54-4caf-c06c-5eb51e71ce6e"},"source":["!neuralxc eval -h"],"execution_count":null,"outputs":[{"output_type":"stream","text":["usage: neuralxc eval [-h] [--model model] [--hdf5 hdf5 hdf5 hdf5] [--plot]\n","                     [--savefig SAVEFIG] [--cutoff cutoff] [--sample sample]\n","                     [--invert_sample] [--keep_mean] [--hashkey HASHKEY]\n","\n","Evaluate a NeuralXC model\n","\n","optional arguments:\n","  -h, --help            show this help message and exit\n","  --model model         Path to NeuralXC model\n","  --hdf5 hdf5 hdf5 hdf5\n","                        Path to hdf5 file, baseline data, reference data\n","  --plot                Create scatterplot?\n","  --savefig SAVEFIG     Save scatterplot?\n","  --cutoff cutoff       Cut off extreme datapoints\n","  --sample sample       Evaluate on sample. Path to sample file\n","  --invert_sample       Invert the sample provided (evaluate on datapoints not\n","                        in sample)\n","  --keep_mean           If set, don't subract parallelity error from MAE and\n","                        RMSE\n","  --hashkey HASHKEY     Manually choose which basis hash key to apply model to\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KOBWMDz7mu6L"},"source":["### neuralxc data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0dovX3nEwCRF","executionInfo":{"status":"ok","timestamp":1623795598256,"user_tz":-60,"elapsed":2627,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"2aab2255-3b3a-4797-bf85-678e3c82b1b5"},"source":["!neuralxc data -h"],"execution_count":null,"outputs":[{"output_type":"stream","text":["usage: neuralxc data [-h] {add,inspect,split,delete,sample,merge} ...\n","\n","Routines to manipulate datasets\n","\n","positional arguments:\n","  {add,inspect,split,delete,sample,merge}\n","\n","optional arguments:\n","  -h, --help            show this help message and exit\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GhT0x2E6mxes"},"source":["### neuralxc engine\n","calculate dft energy"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xYb8R2_kwujf","executionInfo":{"status":"ok","timestamp":1623795790922,"user_tz":-60,"elapsed":3131,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"4082737b-60d0-47db-dd71-3133911fbdd2"},"source":["!neuralxc engine -h"],"execution_count":null,"outputs":[{"output_type":"stream","text":["usage: neuralxc engine [-h] [--workdir workdir] preprocessor xyz\n","\n","Run engine for structures stored in .xyz/.traj file\n","\n","positional arguments:\n","  preprocessor       Config file for preprocessor\n","  xyz                .xyz or .traj file containing structures\n","\n","optional arguments:\n","  -h, --help         show this help message and exit\n","  --workdir workdir  Specify work-directory. If not specified uses .tmp/ and\n","                     deletes after calculation has finished\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"J3P_D-8Bdk6k"},"source":["#### PBE energy of butane"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"20RYuZXixbT9","executionInfo":{"status":"ok","timestamp":1623960737932,"user_tz":-60,"elapsed":1194701,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"b4639dd1-336d-475a-cd4a-e3a2f9f03e90"},"source":["!neuralxc engine basis_sgdml_benzene.json workdir/testing.traj --workdir ./tmp"],"execution_count":null,"outputs":[{"output_type":"stream","text":["converged SCF energy = -158.213511336514\n","converged SCF energy = -158.201412578238\n","converged SCF energy = -158.189950939404\n","converged SCF energy = -158.199422033359\n","converged SCF energy = -158.196163848141\n","converged SCF energy = -158.19820283554\n","converged SCF energy = -158.201586477319\n","converged SCF energy = -158.200987213881\n","converged SCF energy = -158.192222003618\n","converged SCF energy = -158.196165114757\n","converged SCF energy = -158.194574022852\n","converged SCF energy = -158.187506634185\n","converged SCF energy = -158.193918785934\n","converged SCF energy = -158.196533994202\n","converged SCF energy = -158.197999954619\n","converged SCF energy = -158.199055561256\n","converged SCF energy = -158.200512444359\n","converged SCF energy = -158.199887597465\n","converged SCF energy = -158.199554184651\n","converged SCF energy = -158.200125696983\n","converged SCF energy = -158.201718873854\n","converged SCF energy = -158.199072895897\n","converged SCF energy = -158.188315751662\n","converged SCF energy = -158.204416758261\n","converged SCF energy = -158.197442225669\n","converged SCF energy = -158.196572555745\n","converged SCF energy = -158.195829983587\n","converged SCF energy = -158.202545291629\n","converged SCF energy = -158.198711164268\n","converged SCF energy = -158.189625074732\n","converged SCF energy = -158.202347151204\n","converged SCF energy = -158.195896849628\n","converged SCF energy = -158.196439811267\n","converged SCF energy = -158.197123599712\n","converged SCF energy = -158.193856716796\n","converged SCF energy = -158.190963436891\n","converged SCF energy = -158.196626980464\n","converged SCF energy = -158.195323740469\n","converged SCF energy = -158.2021477247\n","converged SCF energy = -158.196296604415\n","converged SCF energy = -158.19617059226\n","converged SCF energy = -158.197776863921\n","converged SCF energy = -158.204391488155\n","converged SCF energy = -158.19797442575\n","converged SCF energy = -158.201108058592\n","converged SCF energy = -158.191804198037\n","converged SCF energy = -158.188915853119\n","converged SCF energy = -158.197258802255\n","converged SCF energy = -158.197646896123\n","converged SCF energy = -158.189998618247\n","converged SCF energy = -158.183200178043\n","converged SCF energy = -158.191088611585\n","converged SCF energy = -158.199447849018\n","converged SCF energy = -158.198132248269\n","converged SCF energy = -158.198590254919\n","converged SCF energy = -158.197820619781\n","converged SCF energy = -158.19643057072\n","converged SCF energy = -158.194138887755\n","converged SCF energy = -158.200134715225\n","converged SCF energy = -158.187670854982\n","converged SCF energy = -158.193953169322\n","converged SCF energy = -158.194561280886\n","converged SCF energy = -158.200630577236\n","converged SCF energy = -158.195945998497\n","converged SCF energy = -158.195556256226\n","converged SCF energy = -158.196174755678\n","converged SCF energy = -158.196540379447\n","converged SCF energy = -158.189226337019\n","converged SCF energy = -158.194762675464\n","converged SCF energy = -158.183138803878\n","converged SCF energy = -158.189092317817\n","converged SCF energy = -158.193687071001\n","converged SCF energy = -158.192373684391\n","converged SCF energy = -158.193778383987\n","converged SCF energy = -158.18668783064\n","converged SCF energy = -158.195638573593\n","converged SCF energy = -158.190174596119\n","converged SCF energy = -158.197517322584\n","converged SCF energy = -158.194605122443\n","converged SCF energy = -158.19110883718\n","converged SCF energy = -158.195141896326\n","converged SCF energy = -158.191276430937\n","converged SCF energy = -158.195347868696\n","converged SCF energy = -158.192451006582\n","converged SCF energy = -158.187395486196\n","converged SCF energy = -158.197360954057\n","converged SCF energy = -158.202276411038\n","converged SCF energy = -158.198159432943\n","converged SCF energy = -158.196154973029\n","converged SCF energy = -158.199542604162\n","converged SCF energy = -158.191884380276\n","converged SCF energy = -158.202105320968\n","converged SCF energy = -158.189708680537\n","converged SCF energy = -158.198959618506\n","converged SCF energy = -158.192462325722\n","converged SCF energy = -158.194268318542\n","converged SCF energy = -158.200208094108\n","converged SCF energy = -158.194758505404\n","converged SCF energy = -158.198364141549\n","converged SCF energy = -158.200215981915\n","converged SCF energy = -158.197225585557\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CrYqystjnKQj"},"source":["### neuralxc pre"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xeoKnuAlq_X2","executionInfo":{"status":"ok","timestamp":1624723572852,"user_tz":-60,"elapsed":12230,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"860bde38-da6e-4dd1-f7a5-43255d97cd7d"},"source":["!neuralxc pre -h"],"execution_count":null,"outputs":[{"output_type":"stream","text":["^C\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cS2wuWGfvzCY"},"source":["Running `neuralxc pre ...` gives a numpy array, does not give hdf5 files"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-oJMkAwutHQW","executionInfo":{"status":"ok","timestamp":1624723575633,"user_tz":-60,"elapsed":2786,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"05fbbd59-2fb3-412b-afbf-5b3fe2b2722a"},"source":["!neuralxc pre basis_sgdml_benzene.json --srcdir workdir/testing/workdir --xyz workdir/butane.xyz --dest ./tmp"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/usr/local/bin/neuralxc\", line 7, in <module>\n","    exec(compile(f.read(), __file__, 'exec'))\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/bin/neuralxc\", line 4, in <module>\n","    from neuralxc.drivers import *\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/__init__.py\", line 17, in <module>\n","    from . import (base, config, constants, datastructures, drivers, ml, projector, pyscf, symmetrizer, utils)\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/datastructures/__init__.py\", line 1, in <module>\n","    from . import hdf5\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/datastructures/hdf5.py\", line 7, in <module>\n","    import neuralxc.ml.utils\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/ml/__init__.py\", line 1, in <module>\n","    from . import transformer, utils\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/ml/utils.py\", line 15, in <module>\n","    from neuralxc.preprocessor import Preprocessor\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/preprocessor/__init__.py\", line 1, in <module>\n","    from .driver import driver\n","  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n","  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n","  File \"<frozen importlib._bootstrap_external>\", line 818, in get_code\n","  File \"<frozen importlib._bootstrap_external>\", line 917, in get_data\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FhbkNAYAEnrO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fDdBttdUYzfC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K5oix5YvY4Ig"},"source":["### run new tests/examples"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z1HhOZt_Y2UU","executionInfo":{"status":"ok","timestamp":1625825895980,"user_tz":-60,"elapsed":279,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"8f1c55de-3139-4d24-eb42-1b023e11f33f"},"source":["cd examples/quickstart/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hq_nLXwFZ03a","executionInfo":{"status":"ok","timestamp":1625826160525,"user_tz":-60,"elapsed":261038,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"b85dc020-05e8-44ea-9fc7-0a1f84fe25fa"},"source":["!neuralxc sc training_structures.xyz config.json hyperparameters.json --hyperopt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","====== Iteration 0 ======\n","\n","Running SCF calculations ...\n","-----------------------------\n","\n","converged SCF energy = -76.3545540706805\n","converged SCF energy = -76.3508207847105\n","converged SCF energy = -76.3557077643316\n","converged SCF energy = -76.356824320776\n","converged SCF energy = -76.3739444533522\n","converged SCF energy = -76.369504751822\n","converged SCF energy = -76.3694359857327\n","converged SCF energy = -76.3496333319949\n","converged SCF energy = -76.3557216068752\n","converged SCF energy = -76.366280553873\n","\n","Projecting onto basis ...\n","-----------------------------\n","\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","10 systems found, adding 97a66c91908d8f76f249705362d9e536\n","10 systems found, adding energy\n","10 systems found, adding energy\n","\n","Baseline accuracy\n","-----------------------------\n","\n","{'mae': 0.05993, 'max': 0.09156, 'mean deviation': 0.0, 'rmse': 0.06635}\n","\n","Fitting initial ML model ...\n","-----------------------------\n","\n","Using symmetrizer  trace\n","Fitting 4 folds for each of 3 candidates, totalling 12 fits\n","[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","Activation unknown, defaulting to GELU\n","ModuleDict(\n","  (X): Linear(in_features=19, out_features=1, bias=True)\n",")\n","Epoch 0 ||  Training loss : 0.820312  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.021887  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.016470  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 3000 ||  Training loss : 0.010597  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 4000 ||  Training loss : 0.006609  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 5000 ||  Training loss : 0.005009  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 6000 ||  Training loss : 0.004084  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 7000 ||  Training loss : 0.003379  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 8000 ||  Training loss : 0.003009  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 9000 ||  Training loss : 0.002807  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10000 ||  Training loss : 0.002611  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 11000 ||  Training loss : 0.002397  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 12000 ||  Training loss : 0.002133  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 13000 ||  Training loss : 0.002092  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 14000 ||  Training loss : 0.001569  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 15000 ||  Training loss : 0.001346  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 16000 ||  Training loss : 0.001154  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 17000 ||  Training loss : 0.000989  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 18000 ||  Training loss : 0.000847  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 19000 ||  Training loss : 0.000736  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20000 ||  Training loss : 0.000622  Validation loss : 0.000000  Learning rate: 0.001\n","Activation unknown, defaulting to GELU\n","ModuleDict(\n","  (X): Linear(in_features=19, out_features=1, bias=True)\n",")\n","Epoch 0 ||  Training loss : 0.713415  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.016518  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.014700  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 3000 ||  Training loss : 0.012053  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 4000 ||  Training loss : 0.008818  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 5000 ||  Training loss : 0.005603  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 6000 ||  Training loss : 0.003326  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 7000 ||  Training loss : 0.002288  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 8000 ||  Training loss : 0.001648  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 9000 ||  Training loss : 0.000999  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10000 ||  Training loss : 0.000450  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 11000 ||  Training loss : 0.000158  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 12000 ||  Training loss : 0.000104  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 13000 ||  Training loss : 0.000089  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 14000 ||  Training loss : 0.000077  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 15000 ||  Training loss : 0.000066  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 16000 ||  Training loss : 0.000057  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 17000 ||  Training loss : 0.000104  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 18000 ||  Training loss : 0.000097  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 19000 ||  Training loss : 0.000047  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20000 ||  Training loss : 0.000028  Validation loss : 0.000000  Learning rate: 0.001\n","Activation unknown, defaulting to GELU\n","ModuleDict(\n","  (X): Linear(in_features=19, out_features=1, bias=True)\n",")\n","Epoch 0 ||  Training loss : 0.993576  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.005929  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.005490  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 3000 ||  Training loss : 0.005067  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 4000 ||  Training loss : 0.004820  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 5000 ||  Training loss : 0.004721  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 6000 ||  Training loss : 0.004648  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 7000 ||  Training loss : 0.004554  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 8000 ||  Training loss : 0.004436  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 9000 ||  Training loss : 0.004295  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10000 ||  Training loss : 0.004122  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 11000 ||  Training loss : 0.003891  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 12000 ||  Training loss : 0.003570  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 13000 ||  Training loss : 0.003121  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 14000 ||  Training loss : 0.002646  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 15000 ||  Training loss : 0.002241  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 16000 ||  Training loss : 0.001899  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 17000 ||  Training loss : 0.001612  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 18000 ||  Training loss : 0.001373  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 19000 ||  Training loss : 0.001175  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20000 ||  Training loss : 0.001010  Validation loss : 0.000000  Learning rate: 0.001\n","Activation unknown, defaulting to GELU\n","ModuleDict(\n","  (X): Linear(in_features=19, out_features=1, bias=True)\n",")\n","Epoch 0 ||  Training loss : 0.843521  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.010515  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.005791  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 3000 ||  Training loss : 0.003480  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 4000 ||  Training loss : 0.002370  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 5000 ||  Training loss : 0.001883  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 6000 ||  Training loss : 0.001472  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 7000 ||  Training loss : 0.001178  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 8000 ||  Training loss : 0.000999  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 9000 ||  Training loss : 0.000837  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10000 ||  Training loss : 0.000671  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 11000 ||  Training loss : 0.000543  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 12000 ||  Training loss : 0.000477  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 13000 ||  Training loss : 0.000451  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 14000 ||  Training loss : 0.000440  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 15000 ||  Training loss : 0.000436  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 16000 ||  Training loss : 0.000434  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 17000 ||  Training loss : 0.000432  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 18000 ||  Training loss : 0.000430  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 19000 ||  Training loss : 0.000429  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20000 ||  Training loss : 0.000427  Validation loss : 0.000000  Learning rate: 0.001\n","Activation unknown, defaulting to GELU\n","ModuleDict(\n","  (X): Linear(in_features=19, out_features=1, bias=True)\n",")\n","Epoch 0 ||  Training loss : 0.138112  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.007164  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.007107  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 3000 ||  Training loss : 0.007107  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 4000 ||  Training loss : 0.007107  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 5000 ||  Training loss : 0.007107  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 6000 ||  Training loss : 0.007107  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 7000 ||  Training loss : 0.007107  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 8000 ||  Training loss : 0.007107  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 9000 ||  Training loss : 0.007107  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10000 ||  Training loss : 0.007107  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 11000 ||  Training loss : 0.007107  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 12000 ||  Training loss : 0.007107  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    14: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 13000 ||  Training loss : 0.007107  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 14000 ||  Training loss : 0.007107  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 15000 ||  Training loss : 0.007107  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 16000 ||  Training loss : 0.007107  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 17000 ||  Training loss : 0.007107  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 18000 ||  Training loss : 0.007107  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 19000 ||  Training loss : 0.007107  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 20000 ||  Training loss : 0.007107  Validation loss : 0.000000  Learning rate: 0.0001\n","Activation unknown, defaulting to GELU\n","ModuleDict(\n","  (X): Linear(in_features=19, out_features=1, bias=True)\n",")\n","Epoch 0 ||  Training loss : 0.729533  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.008737  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.006701  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 3000 ||  Training loss : 0.006772  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 4000 ||  Training loss : 0.006773  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 5000 ||  Training loss : 0.006773  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 6000 ||  Training loss : 0.006773  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 7000 ||  Training loss : 0.006773  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 8000 ||  Training loss : 0.006773  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 9000 ||  Training loss : 0.006773  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10000 ||  Training loss : 0.006773  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 11000 ||  Training loss : 0.006773  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 12000 ||  Training loss : 0.006773  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    14: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 13000 ||  Training loss : 0.006773  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 14000 ||  Training loss : 0.006773  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 15000 ||  Training loss : 0.006773  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 16000 ||  Training loss : 0.006773  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 17000 ||  Training loss : 0.006773  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 18000 ||  Training loss : 0.006773  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 19000 ||  Training loss : 0.006773  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 20000 ||  Training loss : 0.006773  Validation loss : 0.000000  Learning rate: 0.0001\n","Activation unknown, defaulting to GELU\n","ModuleDict(\n","  (X): Linear(in_features=19, out_features=1, bias=True)\n",")\n","Epoch 0 ||  Training loss : 0.640439  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.007319  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.007237  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 3000 ||  Training loss : 0.007237  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 4000 ||  Training loss : 0.007237  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 5000 ||  Training loss : 0.007237  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 6000 ||  Training loss : 0.007237  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 7000 ||  Training loss : 0.007237  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 8000 ||  Training loss : 0.007237  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 9000 ||  Training loss : 0.007237  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10000 ||  Training loss : 0.007237  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 11000 ||  Training loss : 0.007237  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 12000 ||  Training loss : 0.007237  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    14: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 13000 ||  Training loss : 0.007293  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 14000 ||  Training loss : 0.007237  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 15000 ||  Training loss : 0.007237  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 16000 ||  Training loss : 0.007237  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 17000 ||  Training loss : 0.007237  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 18000 ||  Training loss : 0.007237  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 19000 ||  Training loss : 0.007237  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 20000 ||  Training loss : 0.007237  Validation loss : 0.000000  Learning rate: 0.0001\n","Activation unknown, defaulting to GELU\n","ModuleDict(\n","  (X): Linear(in_features=19, out_features=1, bias=True)\n",")\n","Epoch 0 ||  Training loss : 0.646591  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.005494  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.005493  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 3000 ||  Training loss : 0.005493  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 4000 ||  Training loss : 0.005493  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 5000 ||  Training loss : 0.005493  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 6000 ||  Training loss : 0.005493  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 7000 ||  Training loss : 0.005493  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 8000 ||  Training loss : 0.005493  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 9000 ||  Training loss : 0.005493  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10000 ||  Training loss : 0.005493  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 11000 ||  Training loss : 0.005493  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 12000 ||  Training loss : 0.005493  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    14: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 13000 ||  Training loss : 0.005493  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 14000 ||  Training loss : 0.005493  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 15000 ||  Training loss : 0.005493  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 16000 ||  Training loss : 0.005493  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 17000 ||  Training loss : 0.005493  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 18000 ||  Training loss : 0.005493  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 19000 ||  Training loss : 0.005493  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 20000 ||  Training loss : 0.005493  Validation loss : 0.000000  Learning rate: 0.0001\n","Activation unknown, defaulting to GELU\n","ModuleDict(\n","  (X): Linear(in_features=19, out_features=1, bias=True)\n",")\n","Epoch 0 ||  Training loss : 0.747119  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.003661  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.003628  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 3000 ||  Training loss : 0.003602  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 4000 ||  Training loss : 0.003579  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 5000 ||  Training loss : 0.003527  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 6000 ||  Training loss : 0.003421  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 7000 ||  Training loss : 0.003286  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 8000 ||  Training loss : 0.003163  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 9000 ||  Training loss : 0.003077  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10000 ||  Training loss : 0.003031  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 11000 ||  Training loss : 0.003017  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 12000 ||  Training loss : 0.003014  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 13000 ||  Training loss : 0.003013  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 14000 ||  Training loss : 0.003012  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 15000 ||  Training loss : 0.003012  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 16000 ||  Training loss : 0.003012  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 17000 ||  Training loss : 0.003012  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 18000 ||  Training loss : 0.003013  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 19000 ||  Training loss : 0.003012  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20000 ||  Training loss : 0.003012  Validation loss : 0.000000  Learning rate: 0.001\n","Activation unknown, defaulting to GELU\n","ModuleDict(\n","  (X): Linear(in_features=19, out_features=1, bias=True)\n",")\n","Epoch 0 ||  Training loss : 0.499330  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.006633  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.006055  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 3000 ||  Training loss : 0.005211  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 4000 ||  Training loss : 0.004213  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 5000 ||  Training loss : 0.003256  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 6000 ||  Training loss : 0.002535  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 7000 ||  Training loss : 0.002086  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 8000 ||  Training loss : 0.001791  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 9000 ||  Training loss : 0.001569  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10000 ||  Training loss : 0.001426  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 11000 ||  Training loss : 0.001370  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 12000 ||  Training loss : 0.001357  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 13000 ||  Training loss : 0.001354  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 14000 ||  Training loss : 0.001353  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 15000 ||  Training loss : 0.001353  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 16000 ||  Training loss : 0.001353  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 17000 ||  Training loss : 0.001368  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 18000 ||  Training loss : 0.002557  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 19000 ||  Training loss : 0.001354  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20000 ||  Training loss : 0.001353  Validation loss : 0.000000  Learning rate: 0.001\n","Activation unknown, defaulting to GELU\n","ModuleDict(\n","  (X): Linear(in_features=19, out_features=1, bias=True)\n",")\n","Epoch 0 ||  Training loss : 0.220982  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.008829  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.005608  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 3000 ||  Training loss : 0.003571  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 4000 ||  Training loss : 0.002983  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 5000 ||  Training loss : 0.003015  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 6000 ||  Training loss : 0.003110  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 7000 ||  Training loss : 0.003149  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 8000 ||  Training loss : 0.003156  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 9000 ||  Training loss : 0.003157  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10000 ||  Training loss : 0.003157  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 11000 ||  Training loss : 0.003157  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 12000 ||  Training loss : 0.003162  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 13000 ||  Training loss : 0.003157  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 14000 ||  Training loss : 0.003157  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 15000 ||  Training loss : 0.003157  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 16000 ||  Training loss : 0.003157  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 17000 ||  Training loss : 0.003157  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 18000 ||  Training loss : 0.003157  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 19000 ||  Training loss : 0.003157  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 20000 ||  Training loss : 0.003157  Validation loss : 0.000000  Learning rate: 0.0001\n","Activation unknown, defaulting to GELU\n","ModuleDict(\n","  (X): Linear(in_features=19, out_features=1, bias=True)\n",")\n","Epoch 0 ||  Training loss : 0.448648  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.003385  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.001468  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 3000 ||  Training loss : 0.001447  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 4000 ||  Training loss : 0.001422  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 5000 ||  Training loss : 0.001404  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 6000 ||  Training loss : 0.001443  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 7000 ||  Training loss : 0.001540  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 8000 ||  Training loss : 0.001644  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 9000 ||  Training loss : 0.001698  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10000 ||  Training loss : 0.001710  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 11000 ||  Training loss : 0.001712  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 12000 ||  Training loss : 0.001712  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 13000 ||  Training loss : 0.001713  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 14000 ||  Training loss : 0.001713  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 15000 ||  Training loss : 0.001713  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 16000 ||  Training loss : 0.001713  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 17000 ||  Training loss : 0.001713  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 18000 ||  Training loss : 0.001713  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 19000 ||  Training loss : 0.001713  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 20000 ||  Training loss : 0.001713  Validation loss : 0.000000  Learning rate: 0.0001\n","[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  2.5min finished\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n","  DeprecationWarning)\n","Activation unknown, defaulting to GELU\n","ModuleDict(\n","  (X): Linear(in_features=19, out_features=1, bias=True)\n",")\n","Epoch 0 ||  Training loss : 0.308562  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.010681  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.006045  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 3000 ||  Training loss : 0.003968  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 4000 ||  Training loss : 0.003548  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 5000 ||  Training loss : 0.003366  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 6000 ||  Training loss : 0.003190  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 7000 ||  Training loss : 0.003049  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 8000 ||  Training loss : 0.002979  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 9000 ||  Training loss : 0.002961  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10000 ||  Training loss : 0.003007  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 11000 ||  Training loss : 0.002995  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 12000 ||  Training loss : 0.002956  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 13000 ||  Training loss : 0.002956  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 14000 ||  Training loss : 0.002956  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 15000 ||  Training loss : 0.002956  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 16000 ||  Training loss : 0.002986  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 17000 ||  Training loss : 0.002956  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 18000 ||  Training loss : 0.002956  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 19000 ||  Training loss : 0.002956  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20000 ||  Training loss : 0.002957  Validation loss : 0.000000  Learning rate: 0.001\n","\n","\n","====== Iteration 1 ======\n","Using symmetrizer  trace\n","Success!\n","\n","Running SCF calculations ...\n","-----------------------------\n","\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","Overwritten attributes  get_veff  of <class 'pyscf.dft.rks.RKS'>\n","converged SCF energy = -76.3529197713763\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3482793832916\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3557959390123\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3566039757126\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3773117992736\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3717768478271\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3728837188896\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3468443329775\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3554699384749\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3696155591904\n","\n","Projecting onto basis...\n","-----------------------------\n","\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","10 systems found, adding 97a66c91908d8f76f249705362d9e536\n","10 systems found, adding energy\n","\n","Results\n","-----------------------------\n","\n","|   Iteration |   mean deviation |    rmse |     mae |     max |\n","|------------:|-----------------:|--------:|--------:|--------:|\n","|           0 |                0 | 0.06635 | 0.05993 | 0.09156 |\n","|           1 |                0 | 0.00435 | 0.00337 | 0.00883 |\n","\n","Using symmetrizer  trace\n","Dataset 0 old STD: 0.004353946999224922\n","Dataset 0 new STD: 0.06257300841152824\n","Epoch 0 ||  Training loss : 0.004354  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.002911  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.002900  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 3000 ||  Training loss : 0.002898  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 4000 ||  Training loss : 0.002897  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 5000 ||  Training loss : 0.002897  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 6000 ||  Training loss : 0.002948  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 7000 ||  Training loss : 0.002897  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 8000 ||  Training loss : 0.002897  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 9000 ||  Training loss : 0.002897  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10000 ||  Training loss : 0.002897  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 11000 ||  Training loss : 0.002897  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 12000 ||  Training loss : 0.002897  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 13000 ||  Training loss : 0.002897  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 14000 ||  Training loss : 0.002899  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 15000 ||  Training loss : 0.002897  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 16000 ||  Training loss : 0.002897  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 17000 ||  Training loss : 0.002897  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 18000 ||  Training loss : 0.002897  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 19000 ||  Training loss : 0.002897  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 20000 ||  Training loss : 0.002898  Validation loss : 0.000000  Learning rate: 0.0001\n","\n","\n","====== Iteration 2 ======\n","Using symmetrizer  trace\n","Success!\n","\n","Running SCF calculations ...\n","-----------------------------\n","\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3527538832035\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3481893596885\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3557337796742\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3565633561268\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3774572042714\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3718873546818\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3730802052925\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3467132637839\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3554921298575\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3696980741422\n","\n","Projecting onto basis...\n","-----------------------------\n","\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","10 systems found, adding 97a66c91908d8f76f249705362d9e536\n","10 systems found, adding energy\n","\n","Results\n","-----------------------------\n","\n","|   Iteration |   mean deviation |    rmse |     mae |     max |\n","|------------:|-----------------:|--------:|--------:|--------:|\n","|           0 |                0 | 0.06635 | 0.05993 | 0.09156 |\n","|           1 |                0 | 0.00435 | 0.00337 | 0.00883 |\n","|           2 |               -0 | 0.0029  | 0.00234 | 0.00505 |\n","\n","Using symmetrizer  trace\n","Dataset 0 old STD: 0.0028974522988318248\n","Dataset 0 new STD: 0.0632262747661753\n","Epoch 0 ||  Training loss : 0.002897  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.002895  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.002893  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 3000 ||  Training loss : 0.002892  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 4000 ||  Training loss : 0.002896  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 5000 ||  Training loss : 0.002892  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 6000 ||  Training loss : 0.002892  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 7000 ||  Training loss : 0.002892  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 8000 ||  Training loss : 0.002892  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 9000 ||  Training loss : 0.002892  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10000 ||  Training loss : 0.002893  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 11000 ||  Training loss : 0.002892  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 12000 ||  Training loss : 0.003228  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 13000 ||  Training loss : 0.002892  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 14000 ||  Training loss : 0.002892  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 15000 ||  Training loss : 0.003214  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 16000 ||  Training loss : 0.002892  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 17000 ||  Training loss : 0.002892  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 18000 ||  Training loss : 0.002892  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 19000 ||  Training loss : 0.002892  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 20000 ||  Training loss : 0.002892  Validation loss : 0.000000  Learning rate: 0.0001\n","=============== Self consistent training converged ============\n","====== Testing ======\n","\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3730400685615\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3676057754515\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3550863963793\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3720248572034\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3747084130388\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3414133587731\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3771517951067\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.353034344059\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3454440972137\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3356379007892\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3450140418836\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.37605280286\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3735255284472\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3560282143181\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3435689638399\n","15 systems found, adding energy\n","15 systems found, adding energy\n","\n","Test results...\n","-----------------------------\n","\n","{'mae': 0.0095, 'max': 0.02714, 'mean deviation': 0.0, 'rmse': 0.01154}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OyUnkxoibHbL","executionInfo":{"status":"ok","timestamp":1625826205404,"user_tz":-60,"elapsed":10145,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"82bbd8d0-cf16-4fb0-96fb-deb1c3860589"},"source":["!neuralxc engine config_with_model.json more_testing.xyz"],"execution_count":null,"outputs":[{"output_type":"stream","text":["NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/final_model.jit\n","NeuralXC: Model successfully loaded\n","Overwritten attributes  get_veff  of <class 'pyscf.dft.rks.RKS'>\n","converged SCF energy = -76.366388071393\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/final_model.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3234973018712\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/final_model.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3463519408305\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/final_model.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3733583460979\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/final_model.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -76.3472216022783\n","Exception ignored in: <function _TemporaryFileCloser.__del__ at 0x7f989accca70>\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/tempfile.py\", line 587, in __del__\n","    self.close()\n","  File \"/usr/lib/python3.7/tempfile.py\", line 583, in close\n","    unlink(self.name)\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/.tmp/4/tmpddykymx3'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HxR2AMHMbRGa","executionInfo":{"status":"ok","timestamp":1625826215532,"user_tz":-60,"elapsed":2621,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"a81153f8-1cc1-47a3-92c9-aec38e4b49bb"},"source":["!neuralxc data add data.hdf5 more_testing final_model energy --traj results.traj"],"execution_count":null,"outputs":[{"output_type":"stream","text":["5 systems found, adding energy\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L4uqbA7ZdCHD","executionInfo":{"status":"ok","timestamp":1625826218856,"user_tz":-60,"elapsed":2281,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"758e7b02-08e2-4c73-8154-3500cb656edb"},"source":["!neuralxc data add data.hdf5 more_testing reference energy --traj more_testing.xyz #add reference energy to data.hdf5"],"execution_count":null,"outputs":[{"output_type":"stream","text":["5 systems found, adding energy\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bJOVwDSpdUFB","executionInfo":{"status":"ok","timestamp":1625703234792,"user_tz":-60,"elapsed":2970,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"c228cff2-957d-4190-fae4-e0c4e2530ea8"},"source":["#!neuralxc eval --hdf5 data.hdf5 more_testing/final_model more_testing/reference --plot"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'mae': 0.00974, 'max': 0.0207, 'mean deviation': 0.0, 'rmse': 0.01186}\n","<Figure size 1000x800 with 2 Axes>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uq9ICtyyeuHT"},"source":["from neuralxc.drivers.model import eval_driver"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":535},"id":"QnXEsSXIe3u0","executionInfo":{"status":"ok","timestamp":1625829915077,"user_tz":-60,"elapsed":861,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"107a1043-0be7-4449-b1a6-787d3da26bcf"},"source":["eval_driver([\"data.hdf5\", \"more_testing/final_model\", \"more_testing/reference\"], plot=True)#performance on more_testing data"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'mae': 0.00964, 'max': 0.0205, 'mean deviation': 0.0, 'rmse': 0.01178}\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x576 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"execute_result","data":{"text/plain":["{'mae': 0.00964, 'max': 0.0205, 'mean deviation': 0.0, 'rmse': 0.01178}"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":535},"id":"GqpLbQvalgnb","executionInfo":{"status":"ok","timestamp":1625705427577,"user_tz":-60,"elapsed":580,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"4ce2dab4-66f5-49c8-c62b-caed905435ed"},"source":["eval_driver([\"sc/data.hdf5\", \"system/it0\", \"system/ref\"], plot=True)#performance on trainging data after 1 iteration"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'mae': 0.00236, 'max': 0.00508, 'mean deviation': 0.0, 'rmse': 0.0029}\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x576 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"execute_result","data":{"text/plain":["{'mae': 0.00236, 'max': 0.00508, 'mean deviation': 0.0, 'rmse': 0.0029}"]},"metadata":{"tags":[]},"execution_count":116}]},{"cell_type":"code","metadata":{"id":"j2Vox3EAgKlO"},"source":["import h5py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yjrgszUUgNvh"},"source":["f = h5py.File('data.hdf5', 'r')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EGwE2TEXgW5v","executionInfo":{"status":"ok","timestamp":1625831006243,"user_tz":-60,"elapsed":287,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"3a787f1b-571c-42d7-ac1b-e73d2b6c4283"},"source":["np.array(f['more_testing/reference/energy']) #read reference energy from data.hdf5, does not match with more_testing.xyz"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-0.41929806,  0.7568133 ,  0.15686876, -0.61070275,  0.11631875])"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EPOuSH_Qkoz3","executionInfo":{"status":"ok","timestamp":1625831007652,"user_tz":-60,"elapsed":288,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"a3465e6d-37ca-464a-a774-10922475a61f"},"source":["np.array(f['more_testing/final_model/energy']) #nxc energies on the testing molecules"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-0.4088407 ,  0.75827658,  0.13637018, -0.59851154,  0.11270549])"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"markdown","metadata":{"id":"HP2lPGskN7_S"},"source":["## read neural exchange correlation which is a TorchScript"]},{"cell_type":"code","metadata":{"id":"SbG3p-9Qoro6"},"source":["import torch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k05oV4SKotwt"},"source":["xc = torch.jit.load(\"final_model.jit/xc_X\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x2M-uaeQo4y1","executionInfo":{"status":"ok","timestamp":1625830338769,"user_tz":-60,"elapsed":323,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"a83e2c3e-b8f5-4ef0-c4cc-94bf4420ada3"},"source":["print(xc.code)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["def forward(self,\n","    c: Tensor) -> Tensor:\n","  return (self.model).forward(c, )\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zpCkilGvpSh9"},"source":["import pickle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GD2nUfkLpUNG"},"source":["pipeline = pickle.load(open(\"final_model/pipeline.pckl\",\"rb\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JkCIJQVwpszK","executionInfo":{"status":"ok","timestamp":1625706388296,"user_tz":-60,"elapsed":212,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"30eeafd3-b9d1-4a2d-e110-1d2a01288ca0"},"source":["pipeline"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[('var_selector', GroupedVarianceThreshold(threshold=1e-10)),\n","  ('scaler', GroupedStandardScaler(threshold=None)),\n","  ('estimator',\n","   NetworkEstimator(activation='GeLU', alpha=0.001, b=0.001, batch_size=0,\n","            max_steps=20001, n_layers=0, n_nodes=4, valid_size=0))],\n"," {'X': {'basis': '/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/quickstart-basis',\n","   'l': 5,\n","   'n': 7},\n","  'application': 'pyscf',\n","  'basis': {'file': '/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart/quickstart-basis'},\n","  'delta': False,\n","  'dfit': False,\n","  'extension': 'chkpt',\n","  'grad': 0,\n","  'grid': 'analytical',\n","  'operator': 'delta',\n","  'projector': 'gaussian',\n","  'projector_type': 'pyscf',\n","  'spec_agnostic': True},\n"," {'symmetrizer_type': 'trace'}]"]},"metadata":{"tags":[]},"execution_count":142}]},{"cell_type":"code","metadata":{"id":"aalYgzgbD6iR"},"source":["import ase.io"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jn5ynvqMDINI","executionInfo":{"status":"ok","timestamp":1625830691728,"user_tz":-60,"elapsed":376,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"15339fb5-f6e1-4275-cc39-bb13a9b61532"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["config.json             hyperparameters.json  \u001b[0m\u001b[01;34mtesting\u001b[0m/\n","config_with_model.json  more_testing.xyz      testing.xyz\n","data.hdf5               quickstart-basis      training_structures.xyz\n","\u001b[01;34mfinal_model\u001b[0m/            results.traj\n","\u001b[01;34mfinal_model.jit\u001b[0m/        \u001b[01;34msc\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GSMeoXU6EIJl"},"source":["more_testing_results = ase.io.read(\"results.traj\",index=\":\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VmYlF2IcESOB","executionInfo":{"status":"ok","timestamp":1625831572164,"user_tz":-60,"elapsed":305,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"1710901a-e9d1-4b2c-f6f7-d28cb61259c1"},"source":["for i in more_testing_results: #nxc energies on the testing molecules\n","    print(i.get_potential_energy(), i.get_potential_energy()*0.0367493) #convert to hartree"],"execution_count":null,"outputs":[{"output_type":"stream","text":["-2078.035265097308 -76.3663413676405\n","-2076.86814781097 -76.32345062434969\n","-2077.4900542141368 -76.34630524933158\n","-2078.2249359329994 -76.37331163808257\n","-2077.5137189075053 -76.34717491024759\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uQztCRJ6HXa9"},"source":["`neuralxc data add` command seems to change the energies in results.traj and more_tetsing.xyz"]},{"cell_type":"markdown","metadata":{"id":"iOt4F5PGDItf"},"source":["## retry on 20 propane spec_agnostic: True"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"sFZX2ELZDORE","executionInfo":{"status":"ok","timestamp":1625831039104,"user_tz":-60,"elapsed":283,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"1541f6a6-e261-4b04-e778-4d891343b87e"},"source":["pwd"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/quickstart'"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VUL9yIBcDOTz","executionInfo":{"status":"ok","timestamp":1625831040573,"user_tz":-60,"elapsed":359,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"3b0c1d37-19c9-4811-90e1-88a8cd426d21"},"source":["cd ../example_scripts/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X1I9ai5LDOWC"},"source":["cp -r ../../../neuralxc_old/examples/example_scripts/train_model_20_propane/ ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"comsr2ddDOZf","executionInfo":{"status":"ok","timestamp":1625831136945,"user_tz":-60,"elapsed":284,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"7669c21e-1727-4052-d00d-19722bf23aed"},"source":["cd train_model_20_propane/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n2OwF12aILVm","executionInfo":{"status":"ok","timestamp":1625836900964,"user_tz":-60,"elapsed":4806023,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"0627419c-e0f3-4963-b51c-d97b852520f1"},"source":["!sh train_20_propane.sh"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cp: -r not specified; omitting directory '../workdir'\n","using unit 0.04336410390059322\n","using unit 0.04336410390059322\n","FILEPATH basis_sgdml_benzene.json\n","Traceback (most recent call last):\n","  File \"../../../scripts/fix_paths.py\", line 12, in <module>\n","    basis = json.load(open(path, 'r'))\n","FileNotFoundError: [Errno 2] No such file or directory: 'basis_sgdml_benzene.json'\n","\n","====== Iteration 0 ======\n","\n","Running SCF calculations ...\n","-----------------------------\n","\n","converged SCF energy = -118.985059329835\n","converged SCF energy = -118.987204980336\n","converged SCF energy = -118.983042693749\n","converged SCF energy = -118.99598104807\n","converged SCF energy = -118.987741009125\n","converged SCF energy = -118.989522233165\n","converged SCF energy = -118.990655609566\n","converged SCF energy = -118.987395139792\n","converged SCF energy = -118.992027320202\n","converged SCF energy = -118.985179395939\n","converged SCF energy = -118.99016990482\n","converged SCF energy = -118.994902424432\n","converged SCF energy = -118.980943971746\n","converged SCF energy = -118.99150707055\n","converged SCF energy = -118.994414563851\n","converged SCF energy = -118.986973457006\n","converged SCF energy = -118.982485980348\n","converged SCF energy = -118.992255331717\n","converged SCF energy = -118.98697079252\n","converged SCF energy = -118.990563243066\n","\n","Projecting onto basis ...\n","-----------------------------\n","\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 97a66c91908d8f76f249705362d9e536\n","20 systems found, adding energy\n","20 systems found, adding energy\n","\n","Baseline accuracy\n","-----------------------------\n","\n","{'mae': 0.01552, 'max': 0.03613, 'mean deviation': -0.0, 'rmse': 0.01934}\n","\n","Fitting initial ML model ...\n","-----------------------------\n","\n","Using symmetrizer  trace\n","Fitting 3 folds for each of 1 candidates, totalling 3 fits\n","[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","ModuleDict(\n","  (C): Sequential(\n","    (0): Linear(in_features=22, out_features=8, bias=True)\n","    (1): GELU()\n","    (2): Linear(in_features=8, out_features=8, bias=True)\n","    (3): GELU()\n","    (4): Linear(in_features=8, out_features=8, bias=True)\n","    (5): GELU()\n","    (6): Linear(in_features=8, out_features=1, bias=True)\n","  )\n","  (H): Sequential(\n","    (0): Linear(in_features=10, out_features=8, bias=True)\n","    (1): GELU()\n","    (2): Linear(in_features=8, out_features=8, bias=True)\n","    (3): GELU()\n","    (4): Linear(in_features=8, out_features=8, bias=True)\n","    (5): GELU()\n","    (6): Linear(in_features=8, out_features=1, bias=True)\n","  )\n",")\n","Epoch 0 ||  Training loss : 1.238834  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.068752  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 0.031369  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 0.020264  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 400 ||  Training loss : 0.015425  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 500 ||  Training loss : 0.012759  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 600 ||  Training loss : 0.011118  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 700 ||  Training loss : 0.010052  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 800 ||  Training loss : 0.009353  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 900 ||  Training loss : 0.008923  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.008705  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1100 ||  Training loss : 0.008653  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1200 ||  Training loss : 0.008724  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1300 ||  Training loss : 0.008879  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1400 ||  Training loss : 0.009084  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1500 ||  Training loss : 0.009317  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1600 ||  Training loss : 0.009561  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1700 ||  Training loss : 0.009807  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1800 ||  Training loss : 0.010050  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1900 ||  Training loss : 0.010290  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.010525  Validation loss : 0.000000  Learning rate: 0.001\n","ModuleDict(\n","  (C): Sequential(\n","    (0): Linear(in_features=22, out_features=8, bias=True)\n","    (1): GELU()\n","    (2): Linear(in_features=8, out_features=8, bias=True)\n","    (3): GELU()\n","    (4): Linear(in_features=8, out_features=8, bias=True)\n","    (5): GELU()\n","    (6): Linear(in_features=8, out_features=1, bias=True)\n","  )\n","  (H): Sequential(\n","    (0): Linear(in_features=10, out_features=8, bias=True)\n","    (1): GELU()\n","    (2): Linear(in_features=8, out_features=8, bias=True)\n","    (3): GELU()\n","    (4): Linear(in_features=8, out_features=8, bias=True)\n","    (5): GELU()\n","    (6): Linear(in_features=8, out_features=1, bias=True)\n","  )\n",")\n","Epoch 0 ||  Training loss : 1.694632  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.064836  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 0.029998  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 0.016916  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 400 ||  Training loss : 0.011842  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 500 ||  Training loss : 0.009705  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 600 ||  Training loss : 0.008720  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 700 ||  Training loss : 0.008261  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 800 ||  Training loss : 0.008098  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 900 ||  Training loss : 0.008144  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.008371  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1100 ||  Training loss : 0.008769  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1200 ||  Training loss : 0.009324  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1300 ||  Training loss : 0.010007  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1400 ||  Training loss : 0.010779  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1500 ||  Training loss : 0.011598  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1600 ||  Training loss : 0.012428  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1700 ||  Training loss : 0.013238  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1800 ||  Training loss : 0.014009  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    20: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 1900 ||  Training loss : 0.014729  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.014806  Validation loss : 0.000000  Learning rate: 0.0001\n","ModuleDict(\n","  (C): Sequential(\n","    (0): Linear(in_features=22, out_features=8, bias=True)\n","    (1): GELU()\n","    (2): Linear(in_features=8, out_features=8, bias=True)\n","    (3): GELU()\n","    (4): Linear(in_features=8, out_features=8, bias=True)\n","    (5): GELU()\n","    (6): Linear(in_features=8, out_features=1, bias=True)\n","  )\n","  (H): Sequential(\n","    (0): Linear(in_features=10, out_features=8, bias=True)\n","    (1): GELU()\n","    (2): Linear(in_features=8, out_features=8, bias=True)\n","    (3): GELU()\n","    (4): Linear(in_features=8, out_features=8, bias=True)\n","    (5): GELU()\n","    (6): Linear(in_features=8, out_features=1, bias=True)\n","  )\n",")\n","Epoch 0 ||  Training loss : 1.877149  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.136453  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 0.060146  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 0.035137  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 400 ||  Training loss : 0.027158  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 500 ||  Training loss : 0.022286  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 600 ||  Training loss : 0.018161  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 700 ||  Training loss : 0.014735  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 800 ||  Training loss : 0.012172  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 900 ||  Training loss : 0.010479  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.009505  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1100 ||  Training loss : 0.009050  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1200 ||  Training loss : 0.008945  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1300 ||  Training loss : 0.009072  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1400 ||  Training loss : 0.009353  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1500 ||  Training loss : 0.009737  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1600 ||  Training loss : 0.010188  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1700 ||  Training loss : 0.010679  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1800 ||  Training loss : 0.011196  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1900 ||  Training loss : 0.011725  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.012258  Validation loss : 0.000000  Learning rate: 0.001\n","[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   10.6s finished\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n","  DeprecationWarning)\n","ModuleDict(\n","  (C): Sequential(\n","    (0): Linear(in_features=22, out_features=8, bias=True)\n","    (1): GELU()\n","    (2): Linear(in_features=8, out_features=8, bias=True)\n","    (3): GELU()\n","    (4): Linear(in_features=8, out_features=8, bias=True)\n","    (5): GELU()\n","    (6): Linear(in_features=8, out_features=1, bias=True)\n","  )\n","  (H): Sequential(\n","    (0): Linear(in_features=10, out_features=8, bias=True)\n","    (1): GELU()\n","    (2): Linear(in_features=8, out_features=8, bias=True)\n","    (3): GELU()\n","    (4): Linear(in_features=8, out_features=8, bias=True)\n","    (5): GELU()\n","    (6): Linear(in_features=8, out_features=1, bias=True)\n","  )\n",")\n","Epoch 0 ||  Training loss : 4.358874  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.764264  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 0.295451  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 0.183037  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 400 ||  Training loss : 0.135677  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 500 ||  Training loss : 0.108926  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 600 ||  Training loss : 0.091087  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 700 ||  Training loss : 0.077395  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 800 ||  Training loss : 0.066113  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 900 ||  Training loss : 0.056641  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.048768  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1100 ||  Training loss : 0.042446  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1200 ||  Training loss : 0.037591  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1300 ||  Training loss : 0.033979  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1400 ||  Training loss : 0.031281  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1500 ||  Training loss : 0.029167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1600 ||  Training loss : 0.027423  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1700 ||  Training loss : 0.025954  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1800 ||  Training loss : 0.024719  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1900 ||  Training loss : 0.023689  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.022826  Validation loss : 0.000000  Learning rate: 0.001\n","\n","\n","====== Iteration 1 ======\n","Using symmetrizer  trace\n","Success!\n","\n","Running SCF calculations ...\n","-----------------------------\n","\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","Overwritten attributes  get_veff  of <class 'pyscf.dft.rks.RKS'>\n","SCF not converged.\n","SCF energy = -14.0150786480883\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -141.008731375134\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -217.475920515386\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -99.5224509586202\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -95.2698384737085\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -37.6007142079321\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -107.484639237177\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -258.667073333685\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -42.9929876842522\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -182.874896468756\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -162.738934431366\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -36.5036547543413\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -132.973227308793\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -182.353145667967\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -190.047472689113\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -140.958144060321\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -30.6582687967095\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -131.174673855663\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -263.600951385658\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -259.993162980852\n","\n","Projecting onto basis...\n","-----------------------------\n","\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 97a66c91908d8f76f249705362d9e536\n","20 systems found, adding energy\n","\n","Results\n","-----------------------------\n","\n","|   Iteration |   mean deviation |       rmse |        mae |        max |\n","|------------:|-----------------:|-----------:|-----------:|-----------:|\n","|           0 |               -0 |    0.01934 |    0.01552 |    0.03613 |\n","|           1 |               -0 | 2102.65    | 1730       | 3461.47    |\n","\n","Using symmetrizer  trace\n","Dataset 0 old STD: 2102.6456226195737\n","Dataset 0 new STD: 615.4342972615007\n","Epoch 0 ||  Training loss : 2102.645623  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 209.595173  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 145.209025  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 119.638111  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 400 ||  Training loss : 101.674060  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 500 ||  Training loss : 87.198846  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 600 ||  Training loss : 70.981414  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 700 ||  Training loss : 54.847327  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 800 ||  Training loss : 46.773540  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 900 ||  Training loss : 39.714767  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 32.876764  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1100 ||  Training loss : 25.025441  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1200 ||  Training loss : 18.996079  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1300 ||  Training loss : 15.126207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1400 ||  Training loss : 12.283419  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1500 ||  Training loss : 10.186829  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1600 ||  Training loss : 8.524357  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1700 ||  Training loss : 7.177362  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1800 ||  Training loss : 6.101805  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1900 ||  Training loss : 5.285639  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 4.640212  Validation loss : 0.000000  Learning rate: 0.001\n","\n","\n","====== Iteration 2 ======\n","Using symmetrizer  trace\n","Success!\n","\n","Running SCF calculations ...\n","-----------------------------\n","\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -221.345426404204\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -259.945852588295\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -242.973007914911\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -188.583916714261\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -189.466755667888\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -235.150201326361\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -188.137172709749\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -245.445935916341\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -261.08493045171\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -213.528692355446\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -212.668753350594\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -213.248234020193\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -242.563511082891\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -237.151146492393\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -253.691437405878\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -229.21985868016\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -206.667165085721\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -230.628401347852\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -228.206632494335\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -249.669357379925\n","\n","Projecting onto basis...\n","-----------------------------\n","\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 97a66c91908d8f76f249705362d9e536\n","20 systems found, adding energy\n","\n","Results\n","-----------------------------\n","\n","|   Iteration |   mean deviation |       rmse |        mae |        max |\n","|------------:|-----------------:|-----------:|-----------:|-----------:|\n","|           0 |               -0 |    0.01934 |    0.01552 |    0.03613 |\n","|           1 |               -0 | 2102.65    | 1730       | 3461.47    |\n","|           2 |               -0 |  605.778   |  506.41    | 1070.28    |\n","\n","Using symmetrizer  trace\n","Dataset 0 old STD: 605.777944469979\n","Dataset 0 new STD: 116.08449488523921\n","Epoch 0 ||  Training loss : 605.777944  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 57.456697  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 50.754473  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 44.591320  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 400 ||  Training loss : 38.796670  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 500 ||  Training loss : 33.986727  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 600 ||  Training loss : 30.232380  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 700 ||  Training loss : 31.358241  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 800 ||  Training loss : 24.941382  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 900 ||  Training loss : 23.328058  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 22.267716  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1100 ||  Training loss : 21.552876  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1200 ||  Training loss : 21.117057  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1300 ||  Training loss : 20.757243  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1400 ||  Training loss : 20.466692  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1500 ||  Training loss : 20.215458  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1600 ||  Training loss : 20.263753  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1700 ||  Training loss : 19.815205  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1800 ||  Training loss : 19.622453  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1900 ||  Training loss : 19.433823  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 26.478316  Validation loss : 0.000000  Learning rate: 0.001\n","\n","\n","====== Iteration 3 ======\n","Using symmetrizer  trace\n","Success!\n","\n","Running SCF calculations ...\n","-----------------------------\n","\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -258.644949713689\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -260.929972124549\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -260.27833937363\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -255.048807104054\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -257.591575334818\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -253.650179110904\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -255.100255550579\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -262.000871676011\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -259.987568158884\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -259.288865452416\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -256.450332020534\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -258.614139529381\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -258.049921130885\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -257.911273184033\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -259.077356237725\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -255.222128163519\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -255.856743453705\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -259.794538798277\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -263.27591974789\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -259.730154698268\n","\n","Projecting onto basis...\n","-----------------------------\n","\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 97a66c91908d8f76f249705362d9e536\n","20 systems found, adding energy\n","\n","Results\n","-----------------------------\n","\n","|   Iteration |   mean deviation |       rmse |        mae |        max |\n","|------------:|-----------------:|-----------:|-----------:|-----------:|\n","|           0 |               -0 |    0.01934 |    0.01552 |    0.03613 |\n","|           1 |               -0 | 2102.65    | 1730       | 3461.47    |\n","|           2 |               -0 |  605.778   |  506.41    | 1070.28    |\n","|           3 |                0 |   66.7164  |   54.5268  |  134.758   |\n","\n","Using symmetrizer  trace\n","Dataset 0 old STD: 66.71642595068165\n","Dataset 0 new STD: 112.75476266110654\n","Epoch 0 ||  Training loss : 66.716426  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 32.333184  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 25.899643  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 23.160982  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 400 ||  Training loss : 21.358473  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 500 ||  Training loss : 19.898723  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 600 ||  Training loss : 18.678235  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 700 ||  Training loss : 17.718685  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 800 ||  Training loss : 17.185165  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 900 ||  Training loss : 20.031326  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 16.341975  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1100 ||  Training loss : 21.191461  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1200 ||  Training loss : 15.936619  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1300 ||  Training loss : 16.496296  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1400 ||  Training loss : 15.674870  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1500 ||  Training loss : 15.310144  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1600 ||  Training loss : 15.139105  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1700 ||  Training loss : 15.374809  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1800 ||  Training loss : 17.180130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1900 ||  Training loss : 14.476287  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 14.474032  Validation loss : 0.000000  Learning rate: 0.001\n","\n","\n","====== Iteration 4 ======\n","Using symmetrizer  trace\n","Success!\n","\n","Running SCF calculations ...\n","-----------------------------\n","\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -259.101562672751\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -275.025334182966\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -268.117922578893\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -258.830552044505\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -261.376019093723\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -264.364707279438\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -258.804873539118\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -260.951607918444\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -268.499390624377\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -267.890035053149\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -260.831046797471\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -260.469423386578\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -260.74894238914\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -262.817197813882\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -266.417429647131\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -266.60451713278\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -261.478387617473\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -265.039243235379\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -265.976553077188\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -266.46526791554\n","\n","Projecting onto basis...\n","-----------------------------\n","\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 97a66c91908d8f76f249705362d9e536\n","20 systems found, adding energy\n","\n","Results\n","-----------------------------\n","\n","|   Iteration |   mean deviation |       rmse |        mae |        max |\n","|------------:|-----------------:|-----------:|-----------:|-----------:|\n","|           0 |               -0 |    0.01934 |    0.01552 |    0.03613 |\n","|           1 |               -0 | 2102.65    | 1730       | 3461.47    |\n","|           2 |               -0 |  605.778   |  506.41    | 1070.28    |\n","|           3 |                0 |   66.7164  |   54.5268  |  134.758   |\n","|           4 |               -0 |  110.941   |   93.8549  |  300.28    |\n","\n","Using symmetrizer  trace\n","Dataset 0 old STD: 110.94056523935457\n","Dataset 0 new STD: 77.99079222211799\n","Epoch 0 ||  Training loss : 110.940565  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 31.759981  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 24.169545  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 23.565951  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 400 ||  Training loss : 23.319331  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 500 ||  Training loss : 23.026051  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 600 ||  Training loss : 22.740921  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 700 ||  Training loss : 22.478763  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 800 ||  Training loss : 22.207180  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 900 ||  Training loss : 21.924867  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 34.507167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1100 ||  Training loss : 21.285690  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1200 ||  Training loss : 19.809769  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1300 ||  Training loss : 19.588887  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1400 ||  Training loss : 19.295324  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1500 ||  Training loss : 18.962840  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1600 ||  Training loss : 18.742628  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1700 ||  Training loss : 25.900646  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1800 ||  Training loss : 18.219343  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1900 ||  Training loss : 20.076679  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 17.807777  Validation loss : 0.000000  Learning rate: 0.001\n","\n","\n","====== Iteration 5 ======\n","Using symmetrizer  trace\n","Success!\n","\n","Running SCF calculations ...\n","-----------------------------\n","\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -274.107012391315\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -275.439494626165\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -285.660354228973\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -264.436735512983\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -276.351464672527\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -287.003252869657\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -272.44786927249\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -281.981747232106\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -276.262980626899\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -284.706124929312\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -277.469196013038\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -278.439300858034\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -285.549684247331\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -273.189769194773\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -276.462530154948\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -274.908244712373\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -275.43521115563\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -282.778669600638\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -283.984811184192\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -282.700306338817\n","\n","Projecting onto basis...\n","-----------------------------\n","\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 97a66c91908d8f76f249705362d9e536\n","20 systems found, adding energy\n","\n","Results\n","-----------------------------\n","\n","|   Iteration |   mean deviation |       rmse |        mae |        max |\n","|------------:|-----------------:|-----------:|-----------:|-----------:|\n","|           0 |               -0 |    0.01934 |    0.01552 |    0.03613 |\n","|           1 |               -0 | 2102.65    | 1730       | 3461.47    |\n","|           2 |               -0 |  605.778   |  506.41    | 1070.28    |\n","|           3 |                0 |   66.7164  |   54.5268  |  134.758   |\n","|           4 |               -0 |  110.941   |   93.8549  |  300.28    |\n","|           5 |                0 |  151.345   |  126.946   |  381.926   |\n","\n","Using symmetrizer  trace\n","Dataset 0 old STD: 151.34491508306294\n","Dataset 0 new STD: 119.93944046847254\n","Epoch 0 ||  Training loss : 151.344915  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 31.719508  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 24.555232  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 22.128836  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 400 ||  Training loss : 20.882299  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 500 ||  Training loss : 19.840166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 600 ||  Training loss : 19.134683  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 700 ||  Training loss : 18.511107  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 800 ||  Training loss : 17.988165  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 900 ||  Training loss : 17.526513  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 17.083156  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1100 ||  Training loss : 16.404368  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1200 ||  Training loss : 15.925732  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1300 ||  Training loss : 15.462141  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1400 ||  Training loss : 15.001879  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1500 ||  Training loss : 14.538914  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1600 ||  Training loss : 14.149941  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1700 ||  Training loss : 13.702505  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1800 ||  Training loss : 13.270056  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1900 ||  Training loss : 12.889036  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 12.585137  Validation loss : 0.000000  Learning rate: 0.001\n","====== Testing ======\n","\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -274.107012391315\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","SCF not converged.\n","SCF energy = -275.439494626165\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/neuralxc\", line 7, in <module>\n","    exec(compile(f.read(), __file__, 'exec'))\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/bin/neuralxc\", line 240, in <module>\n","    func(**args_dict)\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/drivers/model.py\", line 309, in sc_driver\n","    kwargs=engine_kwargs)\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/preprocessor/driver.py\", line 139, in driver\n","    results = calculate_distributed(atoms, app, dir, kwargs, nworkers)\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/preprocessor/driver.py\", line 106, in calculate_distributed\n","    results = [get_result(f) for f in futures]\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/preprocessor/driver.py\", line 106, in <listcomp>\n","    results = [get_result(f) for f in futures]\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/preprocessor/driver.py\", line 37, in calculate_system\n","    atoms = eng.compute(atoms)\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/engines/engine.py\", line 66, in compute\n","    mf, mol = compute_KS(atoms, basis=self.basis, xc=self.xc, nxc=self.nxc, **self.engine_kwargs)\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/pyscf/pyscf.py\", line 57, in compute_KS\n","    mf.kernel()\n","  File \"/usr/local/lib/python3.7/dist-packages/pyscf/lib/misc.py\", line 636, in aliased_fn\n","    return getattr(self, fname)(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/pyscf/scf/hf.py\", line 1650, in scf\n","    conv_check=self.conv_check, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/pyscf/scf/hf.py\", line 177, in kernel\n","    vhf = mf.get_veff(mol, dm, dm_last, vhf)\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/pyscf/pyscf.py\", line 68, in get_veff\n","    nxc = model.get_V(dm)\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/neuralxc.py\", line 38, in get_V\n","    V = self.projector.get_V(dEdC)\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/projector/pyscf.py\", line 140, in get_V\n","    V = contract('ijk, k', self.eri3c, dEdC)\n","  File \"/usr/local/lib/python3.7/dist-packages/opt_einsum/contract.py\", line 507, in contract\n","    return _core_contract(operands, contraction_list, backend=backend, **einsum_kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/opt_einsum/contract.py\", line 573, in _core_contract\n","    new_view = _tensordot(*tmp_operands, axes=(tuple(left_pos), tuple(right_pos)), backend=backend)\n","  File \"/usr/local/lib/python3.7/dist-packages/opt_einsum/sharing.py\", line 131, in cached_tensordot\n","    return tensordot(x, y, axes, backend=backend)\n","  File \"/usr/local/lib/python3.7/dist-packages/opt_einsum/contract.py\", line 374, in _tensordot\n","    return fn(x, y, axes=axes)\n","  File \"<__array_function__ internals>\", line 6, in tensordot\n","  File \"/usr/local/lib/python3.7/dist-packages/numpy/core/numeric.py\", line 1104, in tensordot\n","    bt = b.transpose(newaxes_b).reshape(newshape_b)\n","KeyboardInterrupt\n","^C\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Bpc9ygfsaSfk"},"source":["## retry on 20 propane spec_agnostic: false"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YDKcLwUAa4zp","executionInfo":{"status":"ok","timestamp":1625836933901,"user_tz":-60,"elapsed":340,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"132fca7b-682f-4947-a251-e79202d925c5"},"source":["cd ../train_model_10_propane_agnostic_false/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3oLD2gdlbtj2","executionInfo":{"status":"ok","timestamp":1625837669317,"user_tz":-60,"elapsed":650268,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"9e235160-66b9-4a58-c92a-bc7883893b51"},"source":["!sh train_20_propane.sh"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cp: -r not specified; omitting directory '../workdir'\n","using unit 0.04336410390059322\n","using unit 0.04336410390059322\n","FILEPATH basis_sgdml_benzene.json\n","Traceback (most recent call last):\n","  File \"../../../scripts/fix_paths.py\", line 12, in <module>\n","    basis = json.load(open(path, 'r'))\n","FileNotFoundError: [Errno 2] No such file or directory: 'basis_sgdml_benzene.json'\n","\n","====== Iteration 0 ======\n","\n","Running SCF calculations ...\n","-----------------------------\n","\n","converged SCF energy = -118.985059329835\n","converged SCF energy = -118.987204980336\n","converged SCF energy = -118.983042693749\n","converged SCF energy = -118.99598104807\n","converged SCF energy = -118.987741009125\n","converged SCF energy = -118.989522233165\n","converged SCF energy = -118.990655609566\n","converged SCF energy = -118.987395139792\n","converged SCF energy = -118.992027320202\n","converged SCF energy = -118.985179395939\n","\n","Projecting onto basis ...\n","-----------------------------\n","\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","10 systems found, adding 97a66c91908d8f76f249705362d9e536\n","10 systems found, adding energy\n","10 systems found, adding energy\n","\n","Baseline accuracy\n","-----------------------------\n","\n","{'mae': 0.01725, 'max': 0.03816, 'mean deviation': 0.0, 'rmse': 0.02096}\n","\n","Fitting initial ML model ...\n","-----------------------------\n","\n","Using symmetrizer  trace\n","Fitting 3 folds for each of 1 candidates, totalling 3 fits\n","[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","ModuleDict(\n","  (C): Sequential(\n","    (0): Linear(in_features=22, out_features=8, bias=True)\n","    (1): GELU()\n","    (2): Linear(in_features=8, out_features=8, bias=True)\n","    (3): GELU()\n","    (4): Linear(in_features=8, out_features=8, bias=True)\n","    (5): GELU()\n","    (6): Linear(in_features=8, out_features=1, bias=True)\n","  )\n","  (H): Sequential(\n","    (0): Linear(in_features=10, out_features=8, bias=True)\n","    (1): GELU()\n","    (2): Linear(in_features=8, out_features=8, bias=True)\n","    (3): GELU()\n","    (4): Linear(in_features=8, out_features=8, bias=True)\n","    (5): GELU()\n","    (6): Linear(in_features=8, out_features=1, bias=True)\n","  )\n",")\n","Epoch 0 ||  Training loss : 1.956792  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.063870  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 0.035923  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 0.025007  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 400 ||  Training loss : 0.019242  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 500 ||  Training loss : 0.015884  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 600 ||  Training loss : 0.013678  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 700 ||  Training loss : 0.012062  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 800 ||  Training loss : 0.010743  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 900 ||  Training loss : 0.009561  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.008444  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1100 ||  Training loss : 0.007380  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1200 ||  Training loss : 0.006416  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1300 ||  Training loss : 0.005658  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1400 ||  Training loss : 0.005247  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1500 ||  Training loss : 0.005284  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1600 ||  Training loss : 0.005730  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1700 ||  Training loss : 0.006448  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1800 ||  Training loss : 0.007301  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1900 ||  Training loss : 0.008205  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.009120  Validation loss : 0.000000  Learning rate: 0.001\n","ModuleDict(\n","  (C): Sequential(\n","    (0): Linear(in_features=22, out_features=8, bias=True)\n","    (1): GELU()\n","    (2): Linear(in_features=8, out_features=8, bias=True)\n","    (3): GELU()\n","    (4): Linear(in_features=8, out_features=8, bias=True)\n","    (5): GELU()\n","    (6): Linear(in_features=8, out_features=1, bias=True)\n","  )\n","  (H): Sequential(\n","    (0): Linear(in_features=10, out_features=8, bias=True)\n","    (1): GELU()\n","    (2): Linear(in_features=8, out_features=8, bias=True)\n","    (3): GELU()\n","    (4): Linear(in_features=8, out_features=8, bias=True)\n","    (5): GELU()\n","    (6): Linear(in_features=8, out_features=1, bias=True)\n","  )\n",")\n","Epoch 0 ||  Training loss : 2.267791  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.134659  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 0.058556  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 0.032144  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 400 ||  Training loss : 0.021084  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 500 ||  Training loss : 0.014550  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 600 ||  Training loss : 0.010341  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 700 ||  Training loss : 0.007701  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 800 ||  Training loss : 0.006117  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 900 ||  Training loss : 0.005204  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.004693  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1100 ||  Training loss : 0.004405  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1200 ||  Training loss : 0.004238  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1300 ||  Training loss : 0.004139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1400 ||  Training loss : 0.004081  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1500 ||  Training loss : 0.004055  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1600 ||  Training loss : 0.004058  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1700 ||  Training loss : 0.004092  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1800 ||  Training loss : 0.004165  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1900 ||  Training loss : 0.004284  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.004458  Validation loss : 0.000000  Learning rate: 0.001\n","ModuleDict(\n","  (C): Sequential(\n","    (0): Linear(in_features=22, out_features=8, bias=True)\n","    (1): GELU()\n","    (2): Linear(in_features=8, out_features=8, bias=True)\n","    (3): GELU()\n","    (4): Linear(in_features=8, out_features=8, bias=True)\n","    (5): GELU()\n","    (6): Linear(in_features=8, out_features=1, bias=True)\n","  )\n","  (H): Sequential(\n","    (0): Linear(in_features=10, out_features=8, bias=True)\n","    (1): GELU()\n","    (2): Linear(in_features=8, out_features=8, bias=True)\n","    (3): GELU()\n","    (4): Linear(in_features=8, out_features=8, bias=True)\n","    (5): GELU()\n","    (6): Linear(in_features=8, out_features=1, bias=True)\n","  )\n",")\n","Epoch 0 ||  Training loss : 0.422667  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.018432  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 0.011828  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 0.010054  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 400 ||  Training loss : 0.009612  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 500 ||  Training loss : 0.009913  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 600 ||  Training loss : 0.010748  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 700 ||  Training loss : 0.011806  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 800 ||  Training loss : 0.012854  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 900 ||  Training loss : 0.013787  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.014576  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1100 ||  Training loss : 0.015225  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1200 ||  Training loss : 0.015749  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1300 ||  Training loss : 0.016169  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1400 ||  Training loss : 0.016507  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 1500 ||  Training loss : 0.016780  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1600 ||  Training loss : 0.016807  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 1700 ||  Training loss : 0.016833  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 1800 ||  Training loss : 0.016860  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 1900 ||  Training loss : 0.016887  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 2000 ||  Training loss : 0.016916  Validation loss : 0.000000  Learning rate: 0.0001\n","[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   10.2s finished\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n","  DeprecationWarning)\n","ModuleDict(\n","  (C): Sequential(\n","    (0): Linear(in_features=22, out_features=8, bias=True)\n","    (1): GELU()\n","    (2): Linear(in_features=8, out_features=8, bias=True)\n","    (3): GELU()\n","    (4): Linear(in_features=8, out_features=8, bias=True)\n","    (5): GELU()\n","    (6): Linear(in_features=8, out_features=1, bias=True)\n","  )\n","  (H): Sequential(\n","    (0): Linear(in_features=10, out_features=8, bias=True)\n","    (1): GELU()\n","    (2): Linear(in_features=8, out_features=8, bias=True)\n","    (3): GELU()\n","    (4): Linear(in_features=8, out_features=8, bias=True)\n","    (5): GELU()\n","    (6): Linear(in_features=8, out_features=1, bias=True)\n","  )\n",")\n","Epoch 0 ||  Training loss : 1.948557  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.197360  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 0.124326  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 0.075071  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 400 ||  Training loss : 0.043199  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 500 ||  Training loss : 0.027352  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 600 ||  Training loss : 0.020749  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 700 ||  Training loss : 0.017968  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 800 ||  Training loss : 0.016538  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 900 ||  Training loss : 0.015586  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.014830  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1100 ||  Training loss : 0.014179  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1200 ||  Training loss : 0.013604  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1300 ||  Training loss : 0.013086  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1400 ||  Training loss : 0.012609  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1500 ||  Training loss : 0.012147  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1600 ||  Training loss : 0.011665  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1700 ||  Training loss : 0.011130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1800 ||  Training loss : 0.010516  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1900 ||  Training loss : 0.009821  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.009077  Validation loss : 0.000000  Learning rate: 0.001\n","\n","\n","====== Iteration 1 ======\n","Using symmetrizer  trace\n","Success!\n","\n","Running SCF calculations ...\n","-----------------------------\n","\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","Overwritten attributes  get_veff  of <class 'pyscf.dft.rks.RKS'>\n","converged SCF energy = -118.984515087649\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.988584243262\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.98321049052\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.995687007546\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.988487548742\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.989623446545\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.989376918492\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.988444708121\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.992816723436\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.984468183539\n","\n","Projecting onto basis...\n","-----------------------------\n","\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","10 systems found, adding 97a66c91908d8f76f249705362d9e536\n","10 systems found, adding energy\n","\n","Results\n","-----------------------------\n","\n","|   Iteration |   mean deviation |    rmse |     mae |     max |\n","|------------:|-----------------:|--------:|--------:|--------:|\n","|           0 |                0 | 0.02096 | 0.01725 | 0.03816 |\n","|           1 |               -0 | 0.00884 | 0.00734 | 0.01733 |\n","\n","Using symmetrizer  trace\n","Dataset 0 old STD: 0.008836873206672783\n","Dataset 0 new STD: 0.020230713490224064\n","Epoch 0 ||  Training loss : 0.008837  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.010446  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 0.016836  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 0.019882  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 400 ||  Training loss : 0.020220  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 500 ||  Training loss : 0.020231  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 600 ||  Training loss : 0.020231  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 700 ||  Training loss : 0.020231  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 800 ||  Training loss : 0.020231  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 900 ||  Training loss : 0.020231  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.020231  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 1100 ||  Training loss : 0.020231  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1200 ||  Training loss : 0.020231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 1300 ||  Training loss : 0.020231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 1400 ||  Training loss : 0.020231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 1500 ||  Training loss : 0.020231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 1600 ||  Training loss : 0.020231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 1700 ||  Training loss : 0.020231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 1800 ||  Training loss : 0.020231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 1900 ||  Training loss : 0.020231  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 2000 ||  Training loss : 0.020231  Validation loss : 0.000000  Learning rate: 0.0001\n","\n","\n","====== Iteration 2 ======\n","Using symmetrizer  trace\n","Success!\n","\n","Running SCF calculations ...\n","-----------------------------\n","\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.985289429566\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.987435080067\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.98327279348\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.996211147801\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.987971108856\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.989752332895\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.990885709296\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.987625239522\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.992257419932\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.98540949567\n","\n","Projecting onto basis...\n","-----------------------------\n","\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","10 systems found, adding 97a66c91908d8f76f249705362d9e536\n","10 systems found, adding energy\n","\n","Results\n","-----------------------------\n","\n","|   Iteration |   mean deviation |    rmse |     mae |     max |\n","|------------:|-----------------:|--------:|--------:|--------:|\n","|           0 |                0 | 0.02096 | 0.01725 | 0.03816 |\n","|           1 |               -0 | 0.00884 | 0.00734 | 0.01733 |\n","|           2 |               -0 | 0.02096 | 0.01725 | 0.03816 |\n","\n","Using symmetrizer  trace\n","Dataset 0 old STD: 0.02096210221973794\n","Dataset 0 new STD: 0.02096210221973794\n","Epoch 0 ||  Training loss : 0.020962  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.020962  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 0.020962  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 0.020962  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 400 ||  Training loss : 0.020962  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 500 ||  Training loss : 0.020962  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 600 ||  Training loss : 0.020962  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 700 ||  Training loss : 0.020962  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 800 ||  Training loss : 0.020962  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 900 ||  Training loss : 0.020962  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.020962  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 1100 ||  Training loss : 0.020962  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1200 ||  Training loss : 0.020962  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 1300 ||  Training loss : 0.020962  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 1400 ||  Training loss : 0.020962  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 1500 ||  Training loss : 0.020962  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 1600 ||  Training loss : 0.020962  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 1700 ||  Training loss : 0.020962  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 1800 ||  Training loss : 0.020962  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 1900 ||  Training loss : 0.020962  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 2000 ||  Training loss : 0.020962  Validation loss : 0.000000  Learning rate: 0.0001\n","=============== Self consistent training converged ============\n","====== Testing ======\n","\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.985289429566\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.987435080067\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.98327279348\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.996211147801\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.987971108856\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.989752332895\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.990885709296\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.987625239522\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.992257419932\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.98540949567\n","10 systems found, adding energy\n","10 systems found, adding energy\n","\n","Test results...\n","-----------------------------\n","\n","{'mae': 0.01725, 'max': 0.03816, 'mean deviation': -0.0, 'rmse': 0.02096}\n","/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_10_propane_agnostic_false\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":535},"id":"Bl5yTdv7cKMv","executionInfo":{"status":"ok","timestamp":1625837790966,"user_tz":-60,"elapsed":787,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"2fdcb1d2-bc0b-4d87-fa03-be9c3b475efe"},"source":["eval_driver([\"workdir/sc/data.hdf5\", \"system/it0\", \"system/ref\"], plot=True)#performance on trainging data after 1 iteration"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'mae': 0.01725, 'max': 0.03816, 'mean deviation': -0.0, 'rmse': 0.02096}\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x576 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"execute_result","data":{"text/plain":["{'mae': 0.01725, 'max': 0.03816, 'mean deviation': -0.0, 'rmse': 0.02096}"]},"metadata":{"tags":[]},"execution_count":90}]},{"cell_type":"markdown","metadata":{"id":"Csr23PKdEpb_"},"source":["## train on 20 methane test on 10 "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jbQcVA8LE0Do","executionInfo":{"status":"ok","timestamp":1624806431913,"user_tz":-60,"elapsed":328,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"4dadce5c-7890-4fec-cc0f-6f508c6fa83c"},"source":["cd examples/example_scripts"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1KA4w-rYG68i","executionInfo":{"status":"ok","timestamp":1624806434568,"user_tz":-60,"elapsed":582,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"7ab34971-b6fb-43c2-bdac-db205de336ec"},"source":["cd train_model_20methane_Cheng_ccsdt/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lg-oHU3uG-OP","executionInfo":{"status":"ok","timestamp":1624808445039,"user_tz":-60,"elapsed":1949813,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"b4bc069d-d48a-4af2-ce63-d8517443bcca"},"source":["!sh train_20_CH4.sh"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 5 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588698872543\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603699120466\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596787136638\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564476942995\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566705237157\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607279372583\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577914114363\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609873964805\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586262398065\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584930966727\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585530568103\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611119520178\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.460511777901\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614139074502\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580038083432\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612598856678\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598803741656\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574136949215\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591920245789\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586167209241\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285579288\n","Dataset 0 new STD: 0.015165952285579288\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015178  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015181  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015177  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 6 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588700201044\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603700448967\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596788465139\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564478271495\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566706565658\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607280701084\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577915442863\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609875293306\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586263726566\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584932295227\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585531896604\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611120848678\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605119107511\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614140403003\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580039411933\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612600185178\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598805070157\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574138277715\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459192157429\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586168537742\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285590503\n","Dataset 0 new STD: 0.015165952285590503\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015179  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015181  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015177  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 7 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588700550046\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603700797969\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596788814141\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564478620497\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.456670691466\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607281050086\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577915791865\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609875642308\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586264075568\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584932644229\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585532245606\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461112119768\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605119456513\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614140752005\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580039760935\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461260053418\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598805419159\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574138626717\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591921923292\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586168886744\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285583133\n","Dataset 0 new STD: 0.015165952285583132\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015179  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015181  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015177  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 8 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588700351765\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603700599688\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459678861586\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564478422216\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566706716379\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607280851805\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577915593584\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609875444026\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586263877287\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584932445948\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585532047324\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611120999399\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605119258232\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614140553724\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580039562654\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612600335899\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598805220878\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574138428436\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591921725011\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586168688463\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285601551\n","Dataset 0 new STD: 0.015165952285601551\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015179  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015181  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015177  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 9 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588699863695\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603700111618\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459678812779\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564477934147\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566706228309\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607280363735\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577915105515\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609874955957\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586263389217\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584931957879\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585531559255\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461112051133\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605118770162\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614140065654\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580039074585\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461259984783\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598804732808\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574137940367\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591921236942\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586168200393\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.01516595228558604\n","Dataset 0 new STD: 0.01516595228558604\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015385  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015206  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 10 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588699433294\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603699681217\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596787697389\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564477503746\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566705797908\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607279933334\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577914675114\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609874525556\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586262958816\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584931527478\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585531128854\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611120080929\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605118339761\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614139635253\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580038644183\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612599417429\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598804302407\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574137509966\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459192080654\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586167769992\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285584754\n","Dataset 0 new STD: 0.015165952285584754\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015386  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 11 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588699040864\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603699288786\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596787304959\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564477111315\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566705405477\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607279540904\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577914282683\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609874133125\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586262566385\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584931135047\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585530736423\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611119688498\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605117947331\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614139242823\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580038251753\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612599024998\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598803909976\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574137117535\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459192041411\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586167377561\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285588409\n","Dataset 0 new STD: 0.015165952285588409\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015385  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 12 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588698673436\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603698921359\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596786937531\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564476743887\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.456670503805\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607279173476\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577913915255\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609873765698\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586262198958\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584930767619\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585530368996\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461111932107\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605117579903\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614138875395\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580037884325\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461259865757\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598803542549\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574136750107\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591920046682\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586167010134\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285600666\n","Dataset 0 new STD: 0.015165952285600666\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015386  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 13 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588698322436\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603698570359\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596786586531\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564476392887\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566704687049\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607278822476\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577913564255\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609873414697\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586261847957\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584930416619\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585530017995\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461111897007\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605117228903\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614138524395\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580037533325\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461259830657\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598803191548\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574136399107\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591919695682\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586166659133\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285588636\n","Dataset 0 new STD: 0.015165952285588636\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015385  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 14 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588697982216\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603698230139\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596786246311\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564476052668\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.456670434683\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607278482256\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577913224036\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609873074478\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586261507738\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.45849300764\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585529677776\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611118629851\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605116888683\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614138184175\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580037193105\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612597966351\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598802851329\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574136058888\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591919355463\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586166318914\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.01516595228560363\n","Dataset 0 new STD: 0.01516595228560363\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015386  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 15 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588697649059\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603697896982\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596785913154\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.456447571951\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566704013673\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607278149099\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577912890878\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.460987274132\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586261174581\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584929743242\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585529344618\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611118296693\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605116555526\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614137851018\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580036859948\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612597633193\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598802518172\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.457413572573\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591919022305\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586165985757\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285571126\n","Dataset 0 new STD: 0.015165952285571126\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015386  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 16 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588697320525\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603697568448\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459678558462\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564475390977\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566703685139\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607277820565\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577912562345\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609872412787\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586260846047\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584929414709\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585529016085\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461111796816\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605116226992\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614137522484\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580036531414\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461259730466\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598802189638\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574135397197\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591918693772\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586165657223\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285587208\n","Dataset 0 new STD: 0.015165952285587208\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015385  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 17 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588696995023\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603697242946\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596785259118\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564475065474\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566703359637\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607277495063\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577912236842\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609872087284\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586260520545\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584929089206\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585528690582\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611117642657\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.460511590149\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614137196982\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580036205912\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612596979157\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598801864136\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574135071694\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591918368269\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.458616533172\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285578599\n","Dataset 0 new STD: 0.015165952285578599\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015385  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 18 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588696671514\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603696919437\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596784935609\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564474741965\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566703036128\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607277171554\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577911913333\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609871763775\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586260197036\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584928765697\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585528367073\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611117319148\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605115577981\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614136873473\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580035882403\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612596655648\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598801540626\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574134748185\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459191804476\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586165008211\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': -0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285613323\n","Dataset 0 new STD: 0.015165952285613323\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015385  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 19 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4588696349325\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603696597248\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459678461342\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564474419776\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566702713939\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4607276849365\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577911591144\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609871441587\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586259874847\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584928443508\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585528044885\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611116996959\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605115255792\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614136551284\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580035560214\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612596333459\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598801218438\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574134425996\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591917722571\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586164686023\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.01516595228560993\n","Dataset 0 new STD: 0.015165952285609928\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015385  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 20 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.458869602802\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603696275943\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4596784292115\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4564474098472\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4566702392634\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.460727652806\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4577911269839\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609871120282\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586259553542\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584928122204\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.458552772358\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611116675655\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4605114934487\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614136229979\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580035238909\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612596012155\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4598800897133\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4574134104692\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4591917401266\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4586164364718\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01296, 'max': 0.02907, 'mean deviation': 0.0, 'rmse': 0.01517}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.015165952285613564\n","Dataset 0 new STD: 0.015165952285613564\n","1\n","Epoch 0 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.015385  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.015207  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.015170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.015168  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.015167  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.015166  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Testing ======\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602720670981\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4561183206506\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4580035238909\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611412144713\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4579827069849\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585051459794\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4593660200192\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4615559515409\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4568776855781\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4600042564116\n","10 systems found, adding energy\n","10 systems found, adding energy\n","{'mae': 0.00852, 'max': 0.02302, 'mean deviation': 0.0, 'rmse': 0.01112}\n","/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_Cheng_ccsdt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J9f6RekYHk5G","executionInfo":{"status":"ok","timestamp":1624727008314,"user_tz":-60,"elapsed":175,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"ac32a425-c872-4170-c797-88c2b39dee30"},"source":["cd ../train_model_20methane_pyscf_ccsdt/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3G_VwQTCHsqE","executionInfo":{"status":"ok","timestamp":1624729042587,"user_tz":-60,"elapsed":2034294,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"909ed143-8c70-4903-9a30-25c41a424761"},"source":["!sh train_20_CH4.sh"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 5 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538582927\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.460253883085\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595626847022\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316653379\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565544947541\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119082967\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576753824747\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713675189\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102108449\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770677111\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370278487\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959230562\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957489394\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612978784887\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578877793816\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438567062\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459764345204\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976659599\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590759956174\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585006919625\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206088512\n","Dataset 0 new STD: 0.022127678206088516\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022137  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022138  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 6 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538475045\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602538722968\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459562673914\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316545496\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565544839658\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606118975085\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576753716864\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713567306\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102000567\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770569228\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370170604\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959122679\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957381512\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612978677004\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578877685934\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438459179\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643344158\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976551716\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590759848291\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585006811742\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.02212767820606867\n","Dataset 0 new STD: 0.02212767820606867\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022138  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 7 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538660396\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602538908319\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595626924491\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316730848\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.456554502501\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119160436\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576753902216\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713752658\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102185918\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.458377075458\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370355956\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959308031\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957566863\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612978862355\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578877871285\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438644531\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643529509\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976737068\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590760033642\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585006997094\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206094806\n","Dataset 0 new STD: 0.022127678206094806\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022140  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022136  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 8 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538537641\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602538785564\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595626801736\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316608092\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565544902255\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119037681\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.457675377946\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713629903\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102063163\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770631824\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370233201\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959185275\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957444108\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.46129787396\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.457887774853\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438521775\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643406754\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976614312\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590759910887\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585006874339\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206097283\n","Dataset 0 new STD: 0.022127678206097283\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022138  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 9 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538715246\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602538963169\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595626979341\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316785698\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.456554507986\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119215286\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576753957066\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713807508\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102240768\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.458377080943\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370410806\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959362881\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957621713\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612978917205\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578877926136\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438699381\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643584359\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976791918\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590760088493\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585007051944\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206087752\n","Dataset 0 new STD: 0.022127678206087752\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022140  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022136  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 10 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538588708\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602538836631\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595626852803\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316659159\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565544953321\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119088747\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576753830527\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713680969\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102114229\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770682891\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370284267\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959236342\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957495175\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612978790667\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578877799597\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438572842\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459764345782\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976665379\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590759961954\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585006925405\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206082493\n","Dataset 0 new STD: 0.022127678206082493\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022138  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 11 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538764424\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602539012347\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595627028519\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316834875\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565545129038\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119264464\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576754006243\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713856686\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102289946\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770858607\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370459984\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959412058\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957670891\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612978966383\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578877975313\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438748558\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643633537\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976841095\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459076013767\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it11.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585007101122\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206098334\n","Dataset 0 new STD: 0.022127678206098334\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022140  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022136  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 12 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538636846\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602538884769\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595626900941\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316707297\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.456554500146\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119136886\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576753878665\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713729108\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102162368\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770731029\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370332406\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.460995928448\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957543313\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612978838805\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578877847735\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.461143862098\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643505959\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976713517\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590760010092\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it12.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585006973544\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206082118\n","Dataset 0 new STD: 0.022127678206082118\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022138  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 13 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538811989\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602539059912\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595627076085\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316882441\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565545176603\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119312029\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576754053809\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713904251\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102337511\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770906173\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370507549\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959459624\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957718457\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612979013949\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578878022879\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438796124\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643681102\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976888661\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590760185236\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it13.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585007148687\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206113447\n","Dataset 0 new STD: 0.022127678206113447\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022140  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022136  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 14 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538684172\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602538932095\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595626948267\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316754623\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565545048786\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119184212\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576753925991\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713776434\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102209694\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770778355\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370379732\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959331806\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957590639\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612978886131\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578877895061\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438668306\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643553285\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976760843\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590760057418\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it14.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.458500702087\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.02212767820607841\n","Dataset 0 new STD: 0.02212767820607841\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022138  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 15 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538859199\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602539107122\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595627123294\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.456331692965\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565545223813\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119359239\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576754101018\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.460871395146\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102384721\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770953382\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370554758\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959506833\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957765666\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612979061158\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578878070088\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438843333\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643728312\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.457297693587\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590760232445\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it15.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585007195897\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.02212767820606717\n","Dataset 0 new STD: 0.02212767820606717\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022140  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022136  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 16 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538731344\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602538979267\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595626995439\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316801795\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565545095958\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119231384\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576753973163\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713823605\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102256866\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770825527\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370426903\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959378978\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957637811\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612978933303\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578877942233\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438715478\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643600457\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976808015\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459076010459\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it16.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585007068042\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206079738\n","Dataset 0 new STD: 0.02212767820607974\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022138  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 17 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538906322\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602539154245\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595627170417\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316976773\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565545270935\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119406362\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576754148141\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713998583\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102431843\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583771000505\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370601881\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959553956\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957812789\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612979108281\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578878117211\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438890456\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643775435\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976982993\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590760279568\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it17.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585007243019\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206084772\n","Dataset 0 new STD: 0.022127678206084772\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022140  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022136  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 18 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538778413\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602539026336\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595627042508\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316848864\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565545143026\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119278452\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576754020232\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713870674\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102303934\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770872596\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370473972\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959426047\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.460395768488\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612978980372\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578877989302\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438762547\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643647525\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976855084\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590760151659\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it18.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.458500711511\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206097207\n","Dataset 0 new STD: 0.022127678206097207\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022138  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 19 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538953333\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602539201256\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595627217428\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563317023784\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565545317947\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119453373\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576754195152\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608714045595\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102478855\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583771047516\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370648893\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959600967\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.46039578598\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612979155292\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578878164222\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438937467\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643822446\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572977030004\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590760326579\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it19.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585007290031\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.022127678206113447\n","Dataset 0 new STD: 0.022127678206113444\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022140  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022136  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 20 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4587538825414\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4602539073337\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4595627089509\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4563316895866\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4565545190028\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4606119325454\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4576754067234\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4608713917676\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585102350936\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583770919598\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4584370520974\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4609959473049\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4603957731881\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4612979027373\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578878036303\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4611438809549\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4597643694527\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4572976902086\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4590760198661\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/sc/model_it20.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4585007162112\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","20 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","20 systems found, adding energy\n","{'mae': 0.01876, 'max': 0.04284, 'mean deviation': -0.0, 'rmse': 0.02213}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.02212767820610274\n","Dataset 0 new STD: 0.02212767820610274\n","1\n","Epoch 0 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.022138  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.022139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.022135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.022130  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.022128  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Testing ======\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4601563468375\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.45600260039\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578878036303\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4610254942107\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4578669867243\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4583894257188\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4592502997586\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4614402312804\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.4567619653176\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -40.459888536151\n","10 systems found, adding energy\n","10 systems found, adding energy\n","{'mae': 0.01221, 'max': 0.03256, 'mean deviation': 0.0, 'rmse': 0.01608}\n","/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20methane_pyscf_ccsdt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wALta_uxJz8R"},"source":["## Train model_50_20_merge\n","5 iter"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1l0hvC6MQAgI","executionInfo":{"status":"ok","timestamp":1623969537858,"user_tz":-60,"elapsed":347,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"a09b2da8-aca8-437f-844c-29064770efcd"},"source":["cd examples/example_scripts/train_model_50_20_merge/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"da2CESLSQ_Ty","executionInfo":{"status":"ok","timestamp":1623955240209,"user_tz":-60,"elapsed":193,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"c2bf8b66-933a-4e8d-ca05-bbce9af57a96"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["basis_sgdml_benzene.json  test_subset.csv          train_subset_propane.csv\n","hyperparameters.json      train_50_20.sh           \u001b[0m\u001b[01;34mworkdir\u001b[0m/\n","README.txt                train_subset_ethane.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6UYSjhAuQ__o","executionInfo":{"status":"ok","timestamp":1623959120903,"user_tz":-60,"elapsed":3877182,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"5fbce264-1df8-4a23-a781-7acc5e942787"},"source":["!sh train_50_20.sh "],"execution_count":null,"outputs":[{"output_type":"stream","text":["cp: -r not specified; omitting directory '../workdir'\n","using unit 0.04336410390059322\n","using unit 0.04336410390059322\n","/usr/local/lib/python3.7/dist-packages/ase/io/jsonio.py:58: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  a = np.array(obj)\n","using unit 0.04336410390059322\n","FILEPATH basis_sgdml_benzene.json\n","Traceback (most recent call last):\n","  File \"../../../scripts/fix_paths.py\", line 13, in <module>\n","    basis['engine_kwargs']['pseudoloc'] = os.path.abspath(basis['engine_kwargs']['pseudoloc'])\n","KeyError: 'pseudoloc'\n","====== Iteration 0 ======\n","converged SCF energy = -79.6844271138828\n","converged SCF energy = -79.6943500665332\n","converged SCF energy = -79.6868898453716\n","converged SCF energy = -79.6922359554811\n","converged SCF energy = -79.6865586039518\n","converged SCF energy = -79.6913095653311\n","converged SCF energy = -79.6937771392765\n","converged SCF energy = -79.6883665751506\n","converged SCF energy = -79.6908131651242\n","converged SCF energy = -79.6928989718341\n","converged SCF energy = -79.689887874575\n","converged SCF energy = -79.6935007220325\n","converged SCF energy = -79.6914501649869\n","converged SCF energy = -79.6881811353526\n","converged SCF energy = -79.6851843297105\n","converged SCF energy = -79.6846655122534\n","converged SCF energy = -79.69204076459\n","converged SCF energy = -79.6914734714862\n","converged SCF energy = -79.6897241902576\n","converged SCF energy = -79.6882188962681\n","converged SCF energy = -79.692173845558\n","converged SCF energy = -79.6855847560228\n","converged SCF energy = -79.6875161777395\n","converged SCF energy = -79.6920402683761\n","converged SCF energy = -79.6871181611662\n","converged SCF energy = -79.6830808966929\n","converged SCF energy = -79.6797909673712\n","converged SCF energy = -79.6888245411406\n","converged SCF energy = -79.6939602555435\n","converged SCF energy = -79.6924510308243\n","converged SCF energy = -79.6932460467215\n","converged SCF energy = -79.6833029688501\n","converged SCF energy = -79.685599311649\n","converged SCF energy = -79.6914625432481\n","converged SCF energy = -79.6944041042538\n","converged SCF energy = -79.692535068146\n","converged SCF energy = -79.6921868033789\n","converged SCF energy = -79.6889093916453\n","converged SCF energy = -79.6902240755121\n","converged SCF energy = -79.6924157415376\n","converged SCF energy = -79.6951037013665\n","converged SCF energy = -79.6864766988545\n","converged SCF energy = -79.6952561636311\n","converged SCF energy = -79.6870497694799\n","converged SCF energy = -79.6834160934014\n","converged SCF energy = -79.6887380420963\n","converged SCF energy = -79.6848380417075\n","converged SCF energy = -79.692599814125\n","converged SCF energy = -79.690091826451\n","converged SCF energy = -79.6909261579376\n","converged SCF energy = -118.940210536202\n","converged SCF energy = -118.940596594795\n","converged SCF energy = -118.937541724306\n","converged SCF energy = -118.952538068452\n","converged SCF energy = -118.942789383792\n","converged SCF energy = -118.94475157552\n","converged SCF energy = -118.947818500794\n","converged SCF energy = -118.941952877314\n","converged SCF energy = -118.946537584784\n","converged SCF energy = -118.940833957381\n","converged SCF energy = -118.946983174052\n","converged SCF energy = -118.950053476769\n","converged SCF energy = -118.936409631262\n","converged SCF energy = -118.947404363513\n","converged SCF energy = -118.949410728218\n","converged SCF energy = -118.942215468016\n","converged SCF energy = -118.939608592427\n","converged SCF energy = -118.947629121785\n","converged SCF energy = -118.941922685014\n","converged SCF energy = -118.945064132137\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.0934, 'mean deviation': 0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Epoch 0 ||  Training loss : 2.680926  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 2.270776  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 1.872122  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 1.483722  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 1.124191  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.826411  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.602909  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.425758  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.275676  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.157964  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.083823  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 110 ||  Training loss : 0.059576  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.057761  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 130 ||  Training loss : 0.057190  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 140 ||  Training loss : 0.055697  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 150 ||  Training loss : 0.054788  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 160 ||  Training loss : 0.054461  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 170 ||  Training loss : 0.054116  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 180 ||  Training loss : 0.053690  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 190 ||  Training loss : 0.053257  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 0.052861  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 210 ||  Training loss : 0.052506  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 220 ||  Training loss : 0.052187  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 230 ||  Training loss : 0.051896  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 240 ||  Training loss : 0.051626  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 250 ||  Training loss : 0.051372  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 260 ||  Training loss : 0.051132  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 270 ||  Training loss : 0.050907  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 280 ||  Training loss : 0.050695  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 290 ||  Training loss : 0.050497  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 300 ||  Training loss : 0.050312  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 310 ||  Training loss : 0.050138  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 320 ||  Training loss : 0.049974  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 330 ||  Training loss : 0.049820  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 340 ||  Training loss : 0.049674  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 350 ||  Training loss : 0.049537  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 360 ||  Training loss : 0.049407  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 370 ||  Training loss : 0.049283  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 380 ||  Training loss : 0.049166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 390 ||  Training loss : 0.049055  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 400 ||  Training loss : 0.048949  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 410 ||  Training loss : 0.048848  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 420 ||  Training loss : 0.048751  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 430 ||  Training loss : 0.048659  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 440 ||  Training loss : 0.048570  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 450 ||  Training loss : 0.048485  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 460 ||  Training loss : 0.048404  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 470 ||  Training loss : 0.048325  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 480 ||  Training loss : 0.048250  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 490 ||  Training loss : 0.048177  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 500 ||  Training loss : 0.048107  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 510 ||  Training loss : 0.048039  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 520 ||  Training loss : 0.047973  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 530 ||  Training loss : 0.047910  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 540 ||  Training loss : 0.047848  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 550 ||  Training loss : 0.047789  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 560 ||  Training loss : 0.047731  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 570 ||  Training loss : 0.047675  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 580 ||  Training loss : 0.047621  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 590 ||  Training loss : 0.047569  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 600 ||  Training loss : 0.047518  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 610 ||  Training loss : 0.047468  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 620 ||  Training loss : 0.047420  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 630 ||  Training loss : 0.047373  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 640 ||  Training loss : 0.047327  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 650 ||  Training loss : 0.047283  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 660 ||  Training loss : 0.047240  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 670 ||  Training loss : 0.047198  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 680 ||  Training loss : 0.047158  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 690 ||  Training loss : 0.047118  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 700 ||  Training loss : 0.047079  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 710 ||  Training loss : 0.047042  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 720 ||  Training loss : 0.047005  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 730 ||  Training loss : 0.046970  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 740 ||  Training loss : 0.046935  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 750 ||  Training loss : 0.046901  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 760 ||  Training loss : 0.046869  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 770 ||  Training loss : 0.046837  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 780 ||  Training loss : 0.046806  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 790 ||  Training loss : 0.046776  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 800 ||  Training loss : 0.046746  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 810 ||  Training loss : 0.046717  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 820 ||  Training loss : 0.046690  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 830 ||  Training loss : 0.046662  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 840 ||  Training loss : 0.046636  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 850 ||  Training loss : 0.046610  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 860 ||  Training loss : 0.046585  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 870 ||  Training loss : 0.046560  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 880 ||  Training loss : 0.046537  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 890 ||  Training loss : 0.046513  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 900 ||  Training loss : 0.046491  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 910 ||  Training loss : 0.046468  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 920 ||  Training loss : 0.046447  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 930 ||  Training loss : 0.046426  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 940 ||  Training loss : 0.046405  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 950 ||  Training loss : 0.046385  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 960 ||  Training loss : 0.046365  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 970 ||  Training loss : 0.046345  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 980 ||  Training loss : 0.046326  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 990 ||  Training loss : 0.046308  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1000 ||  Training loss : 0.046290  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1010 ||  Training loss : 0.046272  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1020 ||  Training loss : 0.046254  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1030 ||  Training loss : 0.046237  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1040 ||  Training loss : 0.046220  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1050 ||  Training loss : 0.046203  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1060 ||  Training loss : 0.046187  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1070 ||  Training loss : 0.046170  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1080 ||  Training loss : 0.046154  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1090 ||  Training loss : 0.046139  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1100 ||  Training loss : 0.046123  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1110 ||  Training loss : 0.046108  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1120 ||  Training loss : 0.046093  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1130 ||  Training loss : 0.046078  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1140 ||  Training loss : 0.046063  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1150 ||  Training loss : 0.046048  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1160 ||  Training loss : 0.046033  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1170 ||  Training loss : 0.046019  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1180 ||  Training loss : 0.046004  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1190 ||  Training loss : 0.045990  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1200 ||  Training loss : 0.045976  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1210 ||  Training loss : 0.045962  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1220 ||  Training loss : 0.045948  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1230 ||  Training loss : 0.045934  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1240 ||  Training loss : 0.045920  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1250 ||  Training loss : 0.045907  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1260 ||  Training loss : 0.045893  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1270 ||  Training loss : 0.045879  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1280 ||  Training loss : 0.045866  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1290 ||  Training loss : 0.045852  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1300 ||  Training loss : 0.045839  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1310 ||  Training loss : 0.045825  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1320 ||  Training loss : 0.045812  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1330 ||  Training loss : 0.045798  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1340 ||  Training loss : 0.045785  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1350 ||  Training loss : 0.045772  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1360 ||  Training loss : 0.045759  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1370 ||  Training loss : 0.045745  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1380 ||  Training loss : 0.045732  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1390 ||  Training loss : 0.045719  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1400 ||  Training loss : 0.045706  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1410 ||  Training loss : 0.045693  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1420 ||  Training loss : 0.045680  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1430 ||  Training loss : 0.045667  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1440 ||  Training loss : 0.045654  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1450 ||  Training loss : 0.045641  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1460 ||  Training loss : 0.045628  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1470 ||  Training loss : 0.045615  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1480 ||  Training loss : 0.045603  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1490 ||  Training loss : 0.045590  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1500 ||  Training loss : 0.045577  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1510 ||  Training loss : 0.045565  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1520 ||  Training loss : 0.045552  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1530 ||  Training loss : 0.045539  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1540 ||  Training loss : 0.045527  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1550 ||  Training loss : 0.045515  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1560 ||  Training loss : 0.045502  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1570 ||  Training loss : 0.045490  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1580 ||  Training loss : 0.045478  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1590 ||  Training loss : 0.045466  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1600 ||  Training loss : 0.045454  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1610 ||  Training loss : 0.045442  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1620 ||  Training loss : 0.045430  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1630 ||  Training loss : 0.045418  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1640 ||  Training loss : 0.045406  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1650 ||  Training loss : 0.045395  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1660 ||  Training loss : 0.045383  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1670 ||  Training loss : 0.045372  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1680 ||  Training loss : 0.045360  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1690 ||  Training loss : 0.045349  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1700 ||  Training loss : 0.045338  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1710 ||  Training loss : 0.045327  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1720 ||  Training loss : 0.045316  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1730 ||  Training loss : 0.045306  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1740 ||  Training loss : 0.045295  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1750 ||  Training loss : 0.045284  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1760 ||  Training loss : 0.045274  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1770 ||  Training loss : 0.045264  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1780 ||  Training loss : 0.045254  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1790 ||  Training loss : 0.045244  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1800 ||  Training loss : 0.045234  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1810 ||  Training loss : 0.045224  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1820 ||  Training loss : 0.045215  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1830 ||  Training loss : 0.045205  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1840 ||  Training loss : 0.045196  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1850 ||  Training loss : 0.045187  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1860 ||  Training loss : 0.045178  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1870 ||  Training loss : 0.045169  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1880 ||  Training loss : 0.045160  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1890 ||  Training loss : 0.045152  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1900 ||  Training loss : 0.045143  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1910 ||  Training loss : 0.045135  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1920 ||  Training loss : 0.045127  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1930 ||  Training loss : 0.045118  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1940 ||  Training loss : 0.045110  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1950 ||  Training loss : 0.045103  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1960 ||  Training loss : 0.045095  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1970 ||  Training loss : 0.045087  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1980 ||  Training loss : 0.045080  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 1990 ||  Training loss : 0.045073  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 2000 ||  Training loss : 0.045065  Validation loss : 0.000000  Learning rate: 0.001\n","1\n","====== Iteration 1 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","Overwritten attributes  get_veff  of <class 'pyscf.dft.rks.RKS'>\n","converged SCF energy = -79.6812716297853\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912453311934\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6835007105281\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6892065022271\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6831946134441\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6882167869532\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6904986810002\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6850628909231\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.687975960365\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896813734741\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6867016753223\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6906238506668\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881421902778\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6849496530413\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6820993544199\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6814414344865\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6891030732183\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883700992507\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.686534013248\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851722817418\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6889675525757\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6824298006197\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6844150072878\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.689132682872\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6837488293911\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6798272020352\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6764514408521\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.685651980716\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908166485835\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6893171669734\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900806008642\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6799096885021\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6824146482864\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6884614891927\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913134027128\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6891062991422\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6890782579071\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6858008147691\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.687017572717\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6893848748087\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.691998385335\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832521816968\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921153040746\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6839240098357\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6800836716774\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6853480775726\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6818242520907\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6895843793963\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6867241942039\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6880250166799\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937793189321\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938002598851\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.935035931238\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950375228961\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940295823969\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942324305585\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945733129722\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939350739032\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944021044295\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938543584423\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944814397965\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947581631453\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.93393384405\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945116377885\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946955571101\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939762935281\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937423680917\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945316150339\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.93948981043\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it1.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942616317023\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03685, 'max': 0.10222, 'mean deviation': 0.0, 'rmse': 0.04758}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.047577617986875875\n","Dataset 0 new STD: 0.047592881410371546\n","1\n","Epoch 0 ||  Training loss : 0.047578  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.047681  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.048225  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.048018  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.048166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.048277  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.048298  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.048313  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.048275  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.048228  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.048166  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.048099  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.048088  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.048080  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.048074  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.048067  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.048060  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.048053  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.048046  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.048039  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.048032  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.048025  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.048017  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.048016  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.048015  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.048014  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.048014  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.048013  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.048012  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.048011  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.048010  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.048010  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.048009  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.048008  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.048008  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.048008  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.048008  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.048007  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.048006  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.048005  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.048004  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 2 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6838610371986\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6937919104537\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6862986023629\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.691700386665\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6859407662889\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907649983368\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931884879377\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6877817696719\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6903113563159\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923305284307\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6893295641027\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6929916080063\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908752257897\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6875923680701\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.684642445676\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6840839012952\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6915173463633\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6909280924784\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6891606534753\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.687672402861\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6916139749216\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6850202111854\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6869581571817\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6915365268246\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6865165432497\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6824953889566\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.679170474316\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6882516942286\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6933988984345\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6918735984291\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6926828250517\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.682703006381\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6850201721267\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6909429595503\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6938580705919\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919200123835\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6916350903333\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883568372254\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896646412958\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6918864554105\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6945435710425\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6858811905096\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6946890664729\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864767144849\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6828023071983\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881356896947\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843049661659\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6920772374419\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6894727923508\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6904106504746\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940873555473\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941189502372\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938179255322\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.953241152291\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943432652327\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945412299793\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948547034577\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942596647224\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947173510802\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941511074496\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947697770492\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950702499396\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937082290615\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948086067491\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.95005955284\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942877787396\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940339252094\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948292879731\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942558709363\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945690321054\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03451, 'max': 0.0951, 'mean deviation': 0.0, 'rmse': 0.04464}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.04464456723144677\n","Dataset 0 new STD: 0.04418924376315862\n","1\n","Epoch 0 ||  Training loss : 0.044645  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.044687  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.044578  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.044403  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.044352  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.044260  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.044245  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.044210  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.044204  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.044193  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.044187  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 110 ||  Training loss : 0.044186  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.044185  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 130 ||  Training loss : 0.044185  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 140 ||  Training loss : 0.044185  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 150 ||  Training loss : 0.044186  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 160 ||  Training loss : 0.044187  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 170 ||  Training loss : 0.044187  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 180 ||  Training loss : 0.044188  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 190 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 200 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    22: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 210 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 220 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 240 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 250 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 260 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 270 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 280 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 290 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 300 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 310 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 320 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 330 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 350 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 360 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 370 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 380 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 390 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 400 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 410 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 420 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 430 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 440 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 460 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 470 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 480 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 490 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 500 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 510 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 520 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 530 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 540 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 550 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.044190  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.044189  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 3 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843527217307\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6942756810433\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868155816844\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921614886416\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864844096036\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912351615584\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6937028394271\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6882922899657\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907386071671\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928246096418\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898135016208\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6934261682907\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913758485509\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881068375634\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851099065263\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6845912132908\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919662515777\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913990527313\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896498089138\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881444726569\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6920994567401\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855104110628\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6874417987186\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919657155956\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870439439208\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6830066121196\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.679716791104\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887501781028\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6938858839873\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923766912729\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931716868966\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832287223916\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855249570799\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913880599623\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943296780822\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924608634939\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921123923741\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888349951019\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901497014926\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923412773779\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950292887635\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.68640240799\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6951818035442\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.686975435542\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6833418925339\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6886638065288\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847635958803\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6925253325161\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900176466665\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908516295634\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940983366698\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94136963213\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938314622556\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.953310745286\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943562229995\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945524384449\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948591107389\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942725749194\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947310467137\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941606728946\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947755833759\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950826290365\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937182416595\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948177096843\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950183585538\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942988282781\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940381187574\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948401941081\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942695529171\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945837047405\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.09339, 'mean deviation': 0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.04379871968073516\n","Dataset 0 new STD: 0.043800787548092815\n","1\n","Epoch 0 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.043816  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043976  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043806  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043806  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043804  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043804  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043805  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 4 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843425405033\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6942654936122\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868052917855\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921513691859\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864740626569\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912249887229\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6936925807988\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6882820181401\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907285634617\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928144030304\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898033033999\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.693416121423\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913656029291\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6880965771552\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6850997500279\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6845809531861\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919561707501\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913888928329\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896396181834\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881343168628\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6920892724293\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855001892197\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6874316057927\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919556676134\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870336152473\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6829963401081\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.679706428648\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.688739972233\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6938756848418\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923664657487\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931614778374\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832184185544\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855147442988\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913779536117\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943195246437\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924505251822\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921022263729\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888248165893\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901395042718\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923311552469\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950191245711\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6863921420245\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6951715949283\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.686965204935\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6833315507048\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6886534936271\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847534583177\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6925152249014\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900072872224\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908415613827\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940780936352\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941167029872\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938112136233\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.95310844416\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943359787618\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945321973102\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.9483888647\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942523285173\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947107994666\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941404348414\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947553546452\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950623876004\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936980024494\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947974748732\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949981133509\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942785866042\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940178954367\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948199520684\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942493089392\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945634547488\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.09339, 'mean deviation': 0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.04380096252265617\n","Dataset 0 new STD: 0.043801291336320713\n","1\n","Epoch 0 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.043820  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043908  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043816  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043811  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043804  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043805  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 5 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843357053692\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6942586580617\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6867984387121\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.692144545755\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864671984466\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912181565113\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6936857321601\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6882751681697\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907217539555\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928075637496\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6897964662663\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6934093109639\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913587575333\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6880897282621\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6850929206032\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6845741050784\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919493541563\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913820624755\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896327818648\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881274871869\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.692082437068\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6854933481233\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.687424769358\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919488572917\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870267552247\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6829894897506\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6796995621043\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887331330453\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6938688472803\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923596230888\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931546386284\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832115624991\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855079036996\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913711332064\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943126951545\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924436624843\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6920953945244\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888179829723\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901326671974\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923243318111\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950122925331\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6863852918923\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6951647555554\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.686958361793\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6833246877637\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6886466359172\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847466322506\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6925084041234\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900004211386\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908347472486\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940718725206\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941104787077\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.938049914415\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.953046255166\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943297573144\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945259764286\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.9483266864\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942461067045\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947045774705\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94134214553\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947491360454\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950561665692\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.93691781962\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947912551119\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949918917707\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942723656822\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940116777847\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948137310673\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942430874417\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945572322571\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.0934, 'mean deviation': 0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.04380136512623971\n","Dataset 0 new STD: 0.04380139585992881\n","1\n","Epoch 0 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.043880  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043914  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043805  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043813  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043805  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Testing ======\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.214619119776\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.202520364397\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.19105872461\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200529817863\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197271634115\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199310621856\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.202694264046\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.202094996676\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.193329792283\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197272899444\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195681803165\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.188614424961\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195026568081\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197641779024\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199107742761\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200163344057\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.201620230101\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200995380841\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200661970638\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.201233482256\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.202826658385\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200180680842\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.189423537622\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.205524543323\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198550010241\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197680342397\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.19693776976\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.203653076091\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199818949394\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.190732860174\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.203454937304\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197004634714\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197547595131\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198231385276\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.194964500666\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.192071222673\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197734765683\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196431524428\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.203255511211\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197404387413\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197278382517\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198884645603\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.205499275971\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.19908221208\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.202215841394\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.192911986343\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.190023637083\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198366587003\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198754681553\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.191106406327\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.184307966878\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.192196394547\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200555633809\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199240034957\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199698038306\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198928406349\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197538355604\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195246672638\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.201242499341\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.188778641125\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195060954008\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195669063547\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.20173836338\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197053782564\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196664039262\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197282542818\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197648161019\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.190334125712\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195870456866\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.184246590679\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.19020010394\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.194794855907\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.193481471028\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.194886168076\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.187795619613\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196746356824\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.191282383891\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198625106181\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195712906278\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.192216623157\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196249680239\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.192384217237\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196455654053\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.193558790354\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.188503273039\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198468738459\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.203384195472\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199267219296\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197262758823\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200650388121\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.192992166159\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.203213104738\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.190816465164\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200067404755\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.193570108451\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.19537610633\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.20131587921\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195866288769\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199471928795\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.201323765668\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198333370854\n","101 systems found, adding energy\n","101 systems found, adding energy\n","{'mae': 0.04258, 'max': 0.15613, 'mean deviation': 0.0, 'rmse': 0.05406}\n","/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wISksOGPJn8E"},"source":["## Train model_50_20_merge_2\n","10 iter"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WPZrXlpEHBXM","executionInfo":{"status":"ok","timestamp":1623969543893,"user_tz":-60,"elapsed":265,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"82d2e727-1c5a-4806-ad48-2670f76d64b0"},"source":["cd .."],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dNbOgwnUILL2","executionInfo":{"status":"ok","timestamp":1623969546566,"user_tz":-60,"elapsed":299,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"8ab10295-d6ac-477f-8c1f-7ad47be40941"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mtrain_model\u001b[0m/  \u001b[01;34mtrain_model_50\u001b[0m/  \u001b[01;34mtrain_model_50_20\u001b[0m/  \u001b[01;34mtrain_model_50_20_merge\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0K7EZV5lIL63"},"source":["cp -r train_model_50_20_merge train_model_50_20_merge_2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"db_NEX_jIPZu","executionInfo":{"status":"ok","timestamp":1623969726423,"user_tz":-60,"elapsed":5,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"bc567b93-9078-4773-d1c7-066e1e853849"},"source":["cd train_model_50_20_merge_2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C8jbpOVeISAz","executionInfo":{"status":"ok","timestamp":1623969902036,"user_tz":-60,"elapsed":277,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"7d21ff19-b6fd-4754-fb30-0b5999ba16bf"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["basis_sgdml_benzene.json  test_subset.csv          train_subset_propane.csv\n","hyperparameters.json      train_50_20.sh           \u001b[0m\u001b[01;34mworkdir\u001b[0m/\n","README.txt                train_subset_ethane.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qntl_k0jJitQ","executionInfo":{"status":"ok","timestamp":1623976608935,"user_tz":-60,"elapsed":3677671,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"a8b6ad66-5a23-470b-db37-ce2ba5fe2626"},"source":["!sh train_50_20.sh"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843901786287\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943131325923\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868532710639\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921987393529\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6865223187686\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912725252443\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6937404928121\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883299368556\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907756327691\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928621282724\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898509542565\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6934632192027\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914134244329\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881445014032\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851472517751\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6846288156667\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6920033939253\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914363885627\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.689687263217\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881818094088\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921369109771\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.685547919239\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.687479250341\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6920027454098\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870817407899\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6830442441916\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6797547016271\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887876708721\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6939233587864\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924142639647\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6932091832304\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832664689377\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855624766575\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914252401629\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943670191201\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924987323248\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921497808108\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888723971613\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901871591921\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923785154879\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950666877945\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864400962013\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6952193118395\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870130125466\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.683379774478\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887015875177\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.684800856978\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.692562537359\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900555679169\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908887267184\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940443021329\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940829834491\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937774480583\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.95277011829\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943021998709\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944984060124\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948050305589\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942185584775\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94677033015\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941066314145\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94721514576\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950286033143\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936642023051\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947636604258\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949643364796\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94244794503\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939840346829\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947861626816\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94215535105\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it2.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945296995054\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03385, 'max': 0.09338, 'mean deviation': 0.0, 'rmse': 0.04379}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.04379236657704381\n","Dataset 0 new STD: 0.04379834998622868\n","1\n","Epoch 0 ||  Training loss : 0.043792  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.044228  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043807  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043814  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043812  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043812  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043800  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043799  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 3 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843789474098\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943019000558\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868416790647\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921877888843\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6865104377652\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912613988159\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6937289729294\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883184088129\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907649983892\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928508053954\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898397081111\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.693452555308\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914019986025\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881329690129\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851361631744\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6846173459018\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919925979266\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914253049543\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896760237894\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881707297336\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921256790802\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855365896017\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6874680112759\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919921016464\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870699949289\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6830327303649\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6797428012101\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887763747026\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6939120890837\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924028644222\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931978802791\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832548025782\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855511452283\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.691414376616\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943559377146\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924869019372\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921386368664\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888612251497\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901759090478\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923675749375\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950555348575\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864285325305\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6952079971911\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870016030847\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6833679272039\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6886898758386\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847898751363\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6925516474984\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900436602786\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908779912483\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940333856778\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940719915692\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937665044993\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.952661388811\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942912704411\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944874896078\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947941821048\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942076197967\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946660905455\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940957277885\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94710649438\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950176797346\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936532951782\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947527683962\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949534048842\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942338788578\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939731912666\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947752442352\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942046005648\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it3.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94518745286\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.0934, 'mean deviation': 0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.043801402783190405\n","Dataset 0 new STD: 0.04380140593726139\n","1\n","Epoch 0 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.043825  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043811  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043831  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043808  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043804  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 4 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843762545217\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6942992071719\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868389860112\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921850961195\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6865077445917\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912587059698\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6937262799159\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883157157902\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907623057623\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928481124728\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898370152142\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6934498626705\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913993056262\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881302759917\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851334703494\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6846146528923\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919899052282\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914226121255\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896733308966\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881680369067\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921229861967\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855338966621\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6874653183786\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919894090137\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870673018058\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6830300373322\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.679740108011\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887736817794\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6939093961824\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924001714629\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931951873609\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832521094896\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855484522878\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914116838864\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943532448925\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924842087858\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921359440176\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888585322842\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901732161507\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923648821761\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950528420055\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864258394936\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6952053042701\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6869989101186\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6833652340411\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6886871827362\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847871823455\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6925489547634\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900409670909\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908752985753\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940443701232\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940829759825\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937774889335\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.95277123348\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943022548821\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.944984740549\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948051665822\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942186042345\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946770749813\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94106712241\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94721633908\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950286641797\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936642796291\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947637528542\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949643893247\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942448633046\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939841757454\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947862286814\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942155850043\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it4.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945297297167\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.0934, 'mean deviation': 0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.043801406788502875\n","Dataset 0 new STD: 0.04380140680001133\n","1\n","Epoch 0 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.044389  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.044017  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043888  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043833  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043809  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 5 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843731560174\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6942961086678\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868358875062\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921819976154\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6865046460864\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912556074656\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.693723181411\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.688312617285\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907592072582\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928450139687\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898339167095\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.693446764167\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913962071213\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881271774872\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851303718451\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6846115543878\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919868067242\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914195136206\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896702323918\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881649384025\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921198876924\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855307981574\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6874622198743\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919863105105\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870642033006\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6830269388274\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6797370095056\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887705832749\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6939062976778\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923970729584\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931920888559\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832490109844\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855453537834\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914085853822\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943501463885\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924811102803\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921328455131\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888554337798\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901701176466\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923617836721\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950497435013\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.686422740989\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6952022057657\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6869958116141\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6833621355361\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6886840842308\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847840838418\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6925458562593\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900378685855\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908722000717\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940525919888\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940911978481\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937857107992\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.952853452138\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943104767477\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945066959206\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948133884479\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942268261001\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94685296847\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941149341066\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947298557738\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950368860456\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936725014949\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947719747199\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949726111904\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942530851703\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939923976113\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94794450547\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942238068699\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it5.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945379515823\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.0934, 'mean deviation': -0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.04380140680070295\n","Dataset 0 new STD: 0.04380140680177187\n","1\n","Epoch 0 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.044043  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043830  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043806  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 6 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843715574711\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6942945101214\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868342889599\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921803990695\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.68650304754\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912540089194\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6937215828648\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883110187388\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907576087125\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928434154224\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898323181634\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6934451656206\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913946085749\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881255789406\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851287732987\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6846099558416\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919852081782\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914179150744\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896686338457\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881633398563\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921182891461\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855291996112\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6874606213277\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919847119643\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870626047541\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.683025340281\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6797354109591\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887689847288\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6939046991314\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923954744123\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931904903099\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832474124381\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855437552371\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914069868363\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943485478421\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924795117339\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.692131246967\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888538352336\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901685191002\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923601851259\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950481449549\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.686421142443\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.695200607219\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6869942130679\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6833605369896\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6886824856844\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847824852957\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6925442577132\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900362700391\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908706015257\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940596230529\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940982289121\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937927418632\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.95292376278\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943175078118\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945137269846\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948204195121\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942338571641\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946923279111\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941219651708\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947368868379\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950439171096\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936795325589\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94779005784\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949796422546\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942601162344\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939994286753\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948014816112\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94230837934\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it6.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945449826464\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.0934, 'mean deviation': -0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.043801406804574235\n","Dataset 0 new STD: 0.043801406804577774\n","1\n","Epoch 0 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.043865  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043818  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043815  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043804  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 7 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843709125548\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.694293865205\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868336440438\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921797541529\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6865024026236\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912533640031\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6937209379485\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883103738228\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907569637958\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.692842770506\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898316732469\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6934445207044\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913939636586\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881249340243\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851281283825\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6846093109253\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919845632619\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914172701579\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896679889293\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.68816269494\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921176442296\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855285546947\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6874599764116\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.691984067048\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870619598381\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.683024695365\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6797347660428\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887683398123\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6939040542152\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923948294957\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931898453934\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832467675218\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855431103208\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914063419198\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943479029257\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924788668177\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921306020509\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888531903171\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901678741842\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923595402096\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950475000387\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864204975266\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6951999623029\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6869935681515\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6833598920734\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6886818407682\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847818403795\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6925436127969\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900356251227\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908699566091\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940613181815\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940999240407\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937944369918\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.952940714065\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943192029404\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945154221132\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948221146407\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942355522927\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946940230396\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941236602993\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947385819664\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950456122381\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936812276875\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947807009125\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949813373831\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94261811363\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940011238039\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948031767397\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942325330626\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it7.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945466777749\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.0934, 'mean deviation': -0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.043801406802590724\n","Dataset 0 new STD: 0.043801406802590724\n","1\n","Epoch 0 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.043894  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043806  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043811  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043805  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 8 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843706528489\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6942936054991\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868333843379\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.692179494447\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6865021429177\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912531042972\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6937206782426\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883101141169\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907567040899\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928425108001\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.689831413541\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6934442609985\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913937039527\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881246743184\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851278686765\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6846090512194\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.691984303556\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.691417010452\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896677292234\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881624352341\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921173845237\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855282949888\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6874597167057\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919838073421\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870617001322\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6830244356591\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6797345063369\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887680801064\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6939037945093\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923945697898\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931895856875\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832465078159\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855428506149\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914060822139\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943476432198\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924786071118\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.692130342345\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888529306112\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901676144783\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923592805037\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950472403328\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864202378207\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.695199702597\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6869933084456\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6833596323675\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6886815810623\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847815806736\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.692543353091\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900353654168\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908696969032\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940567389546\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940953448138\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937898577648\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.952894921796\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943146237135\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945108428863\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948175354137\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942309730658\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946894438127\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941190810724\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947340027395\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950410330112\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936766484606\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947761216856\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949767581561\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94257232136\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.93996544577\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947985975128\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942279538357\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it8.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94542098548\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.0934, 'mean deviation': 0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.04380140680248595\n","Dataset 0 new STD: 0.04380140680248595\n","1\n","Epoch 0 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.043922  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043827  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043803  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043807  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 9 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843704535956\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6942934062458\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868331850845\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921792951937\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6865019436644\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912529050439\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6937204789893\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883099148635\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907565048365\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928423115467\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898312142876\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6934440617452\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913935046993\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881244750651\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6851276694231\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6846088519661\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919841043026\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914168111987\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896675299701\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881622359808\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921171852704\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855280957355\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6874595174524\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919836080888\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870615008788\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6830242364058\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6797343070835\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887678808531\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.693903595256\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923943705364\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6931893864342\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832463085625\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855426513615\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914058829605\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943474439664\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924784078585\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921301430917\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888527313578\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901674152249\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923590812503\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950470410795\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864200385674\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6951995033436\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6869931091922\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6833594331141\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.688681381809\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847813814203\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6925431538376\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900351661635\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908694976499\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940509275589\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940895334182\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937840463692\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.952836807839\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943088123178\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945050314907\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948117240181\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942251616701\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94683632417\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941132696767\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947281913439\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950352216155\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936708370649\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947703102899\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949709467605\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942514207404\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939907331813\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947927861171\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.9422214244\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it9.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945362871523\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.0934, 'mean deviation': -0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.043801406802415316\n","Dataset 0 new STD: 0.043801406802415316\n","1\n","Epoch 0 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.043916  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043833  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043807  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Iteration 10 ======\n","Using symmetrizer  trace\n","1\n","1\n","Success!\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6843702813014\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6942932339516\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6868330127903\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921791228995\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6865017713702\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6912527327497\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6937203066951\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6883097425694\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6907563325424\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6928421392526\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6898310419934\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.693443889451\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6913933324052\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881243027709\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.685127497129\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6846086796719\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919839320084\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914166389045\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6896673576759\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6881620636866\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921170129762\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855279234413\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6874593451582\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6919834357946\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6870613285847\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6830240641116\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6797341347894\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6887677085589\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6939034229618\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923941982423\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.69318921414\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6832461362683\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6855424790674\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6914057106664\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6943472716723\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6924782355643\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6921299707975\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6888525590636\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6901672429308\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6923589089562\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6950468687853\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6864198662732\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6951993310495\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6869929368981\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.68335926082\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6886812095148\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6847812091261\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6925429815434\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6900349938693\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -79.6908693253557\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940456491952\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.940842550545\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.937787680055\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.952784024202\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.943035339542\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.94499753127\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.948064456544\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942198833064\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.946783540534\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.941079913131\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947229129802\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.950299432518\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.936655587012\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947650319262\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.949656683968\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942461423767\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.939854548176\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.947875077535\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.942168640763\n","NeuralXC: Loading model from /content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/sc/model_it10.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -118.945310087887\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","Warning (in get_preprocessor): Dataset not homogeneous\n","======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","workdir/3/pyscf.chkpt\n","workdir/4/pyscf.chkpt\n","workdir/5/pyscf.chkpt\n","workdir/6/pyscf.chkpt\n","workdir/7/pyscf.chkpt\n","workdir/8/pyscf.chkpt\n","workdir/9/pyscf.chkpt\n","workdir/10/pyscf.chkpt\n","workdir/11/pyscf.chkpt\n","workdir/12/pyscf.chkpt\n","workdir/13/pyscf.chkpt\n","workdir/14/pyscf.chkpt\n","workdir/15/pyscf.chkpt\n","workdir/16/pyscf.chkpt\n","workdir/17/pyscf.chkpt\n","workdir/18/pyscf.chkpt\n","workdir/19/pyscf.chkpt\n","workdir/20/pyscf.chkpt\n","workdir/21/pyscf.chkpt\n","workdir/22/pyscf.chkpt\n","workdir/23/pyscf.chkpt\n","workdir/24/pyscf.chkpt\n","workdir/25/pyscf.chkpt\n","workdir/26/pyscf.chkpt\n","workdir/27/pyscf.chkpt\n","workdir/28/pyscf.chkpt\n","workdir/29/pyscf.chkpt\n","workdir/30/pyscf.chkpt\n","workdir/31/pyscf.chkpt\n","workdir/32/pyscf.chkpt\n","workdir/33/pyscf.chkpt\n","workdir/34/pyscf.chkpt\n","workdir/35/pyscf.chkpt\n","workdir/36/pyscf.chkpt\n","workdir/37/pyscf.chkpt\n","workdir/38/pyscf.chkpt\n","workdir/39/pyscf.chkpt\n","workdir/40/pyscf.chkpt\n","workdir/41/pyscf.chkpt\n","workdir/42/pyscf.chkpt\n","workdir/43/pyscf.chkpt\n","workdir/44/pyscf.chkpt\n","workdir/45/pyscf.chkpt\n","workdir/46/pyscf.chkpt\n","workdir/47/pyscf.chkpt\n","workdir/48/pyscf.chkpt\n","workdir/49/pyscf.chkpt\n","workdir/50/pyscf.chkpt\n","workdir/51/pyscf.chkpt\n","workdir/52/pyscf.chkpt\n","workdir/53/pyscf.chkpt\n","workdir/54/pyscf.chkpt\n","workdir/55/pyscf.chkpt\n","workdir/56/pyscf.chkpt\n","workdir/57/pyscf.chkpt\n","workdir/58/pyscf.chkpt\n","workdir/59/pyscf.chkpt\n","workdir/60/pyscf.chkpt\n","workdir/61/pyscf.chkpt\n","workdir/62/pyscf.chkpt\n","workdir/63/pyscf.chkpt\n","workdir/64/pyscf.chkpt\n","workdir/65/pyscf.chkpt\n","workdir/66/pyscf.chkpt\n","workdir/67/pyscf.chkpt\n","workdir/68/pyscf.chkpt\n","workdir/69/pyscf.chkpt\n","70 systems found, adding 540fffd922b6f0cbee81e4c1c80c7dfa\n","70 systems found, adding energy\n","{'mae': 0.03386, 'max': 0.0934, 'mean deviation': 0.0, 'rmse': 0.0438}\n","{'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","Using symmetrizer  trace\n","1\n","Dataset 0 old STD: 0.04380140680248962\n","Dataset 0 new STD: 0.04380140680248962\n","1\n","Epoch 0 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 10 ||  Training loss : 0.043916  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 20 ||  Training loss : 0.043832  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 30 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 40 ||  Training loss : 0.043807  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 50 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 60 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 70 ||  Training loss : 0.043802  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 80 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 90 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 100 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.001\n","Epoch 120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 170 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 180 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 190 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 200 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 210 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 220 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 0.0001\n","Epoch 230 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 240 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 250 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 260 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 270 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 280 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 290 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 300 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 310 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 320 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 330 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1e-05\n","Epoch 340 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 350 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-06\n","Epoch 450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1010 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1020 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1030 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1040 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1050 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1060 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1070 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1080 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1090 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1100 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1110 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1120 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1130 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1140 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1150 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1160 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1170 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1180 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1190 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1200 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1210 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1220 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1230 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1240 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1250 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1260 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1270 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1280 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1290 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1300 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1310 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1320 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1330 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1340 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1350 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1360 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1370 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1380 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1390 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1400 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1410 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1420 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1430 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1440 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1450 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1460 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1470 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1480 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1490 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1500 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1510 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1520 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1530 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1540 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1550 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1560 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1570 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1580 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1590 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1600 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1610 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1620 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1630 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1640 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1650 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1660 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1670 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1680 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1690 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1700 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1710 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1720 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1730 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1740 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1750 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1760 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1770 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1780 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1790 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1800 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1810 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1820 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1830 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1840 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1850 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1860 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1870 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1880 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1890 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1900 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1910 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1920 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1930 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1940 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1950 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1960 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1970 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1980 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 1990 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","Epoch 2000 ||  Training loss : 0.043801  Validation loss : 0.000000  Learning rate: 1.0000000000000002e-07\n","1\n","====== Testing ======\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.214060080596\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.201961322321\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.190499683485\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199970777441\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196712592222\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198751579621\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.2021352214\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.201535957962\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.1927707477\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196713858838\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195122766934\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.188055378265\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.194467530015\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197082738283\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.1985486987\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199604305337\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.20106118844\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200436341545\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200102928732\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200674441064\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.202267617935\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199621639979\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.188864495744\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.204965502341\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.19799096975\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197121299826\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196378727668\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.203094035709\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199259908349\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.190173818813\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.202895895286\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196445593709\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196988555348\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197672343793\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.194405460877\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.191512180973\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197175724545\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.19587248455\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.202696468782\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196845348495\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196719336342\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198325608003\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.204940232235\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198523169831\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.201656802673\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.19235294212\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.1894645972\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197807546336\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198195640204\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.190547362328\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.183748922124\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.191637355666\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199996593098\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198680992352\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199138999001\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198369363862\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.1969793148\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.194687631836\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200683459305\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.188219599063\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.194501913404\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195110024966\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.201179321317\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196494742577\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196105000307\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196723499759\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.19708912353\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.1897750811\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195311419545\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.183687547957\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.189641061897\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.194235815082\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.192922428473\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.194327128069\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.18723657472\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196187317675\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.190723340199\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198066066666\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195153866524\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.191657581261\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195690640406\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.191825175017\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195896612778\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.192999750663\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.187944230277\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197909698137\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.202825155119\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.198708177025\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.196703717109\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200091348243\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.192433124356\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.202654065048\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.190257424619\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.199508362587\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.193011069803\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.194817062622\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200756838189\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.195307249485\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.19891288563\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.200764725996\n","NeuralXC: Loading model from /content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2/workdir/testing/nxc.jit\n","NeuralXC: Model successfully loaded\n","converged SCF energy = -158.197774329638\n","101 systems found, adding energy\n","101 systems found, adding energy\n","{'mae': 0.04258, 'max': 0.15613, 'mean deviation': 0.0, 'rmse': 0.05406}\n","/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_50_20_merge_2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hr50rTK3Jmno"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"33tiXCx3R30Q"},"source":["## Train on 20 propane and test on the same 20 propanes"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ua1sOAS6SDkd","executionInfo":{"status":"ok","timestamp":1625836901708,"user_tz":-60,"elapsed":10,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"225cddc9-938b-4dcb-a89e-989413fd2c9d"},"source":["cd examples/example_scripts"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[Errno 2] No such file or directory: 'examples/example_scripts'\n","/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qw9NU6r8Taj8","executionInfo":{"status":"ok","timestamp":1625836902126,"user_tz":-60,"elapsed":424,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"294918ff-06e9-4db8-ca17-4dbeb2444393"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["basis.json            results.traj         train_subset_ethane.csv\n","hyperparameters.json  test_subset.csv      train_subset_propane.csv\n","README.txt            train_20_propane.sh  \u001b[0m\u001b[01;34mworkdir\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aslsYBBYTlOZ","executionInfo":{"status":"ok","timestamp":1625836902129,"user_tz":-60,"elapsed":15,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"005799a7-683c-492a-f33a-ba8602216e2e"},"source":["cd train_model_20_propane/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[Errno 2] No such file or directory: 'train_model_20_propane/'\n","/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc/examples/example_scripts/train_model_20_propane\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-xQ772NiU0aO","executionInfo":{"status":"ok","timestamp":1625836907952,"user_tz":-60,"elapsed":5834,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"3a0bffb4-2d45-4063-8a8e-a277ccfe7758"},"source":["!sh train_20_propane.sh"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cp: -r not specified; omitting directory '../workdir'\n","using unit 0.04336410390059322\n","using unit 0.04336410390059322\n","FILEPATH basis_sgdml_benzene.json\n","Traceback (most recent call last):\n","  File \"../../../scripts/fix_paths.py\", line 12, in <module>\n","    basis = json.load(open(path, 'r'))\n","FileNotFoundError: [Errno 2] No such file or directory: 'basis_sgdml_benzene.json'\n","\n","====== Iteration 0 ======\n","\n","Running SCF calculations ...\n","-----------------------------\n","\n","Re-using results\n","Re-using results\n","Re-using results\n","Re-using results\n","Re-using results\n","Re-using results\n","Re-using results\n","Re-using results\n","Re-using results\n","Re-using results\n","Re-using results\n","Re-using results\n","Re-using results\n","Re-using results\n","Re-using results\n","Re-using results\n","Re-using results\n","Re-using results\n","Re-using results\n","Re-using results\n","\n","Projecting onto basis ...\n","-----------------------------\n","\n","workdir/0/pyscf.chkpt\n","workdir/1/pyscf.chkpt\n","workdir/2/pyscf.chkpt\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/neuralxc\", line 7, in <module>\n","    exec(compile(f.read(), __file__, 'exec'))\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/bin/neuralxc\", line 240, in <module>\n","    func(**args_dict)\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/drivers/model.py\", line 198, in sc_driver\n","    pre_driver(xyz, 'workdir', preprocessor='pre.json', dest='data.hdf5/system/it{}'.format(iteration))\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/drivers/other.py\", line 209, in pre_driver\n","    data = preprocessor.fit_transform(None)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 464, in fit_transform\n","    return self.fit(X, **fit_params).transform(X)\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/preprocessor/preprocessor.py\", line 38, in transform\n","    basis_rep = self.get_basis_rep()\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/preprocessor/preprocessor.py\", line 127, in get_basis_rep\n","    results = list(futures)\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/preprocessor/preprocessor.py\", line 148, in transform_one\n","    projector = DensityProjector(**density_dict, basis_instructions=basis_instructions)\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/projector/projector.py\", line 35, in DensityProjector\n","    return registry[projector_type](**kwargs)\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/projector/pyscf.py\", line 76, in __init__\n","    self.initialize(mol)\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/projector/pyscf.py\", line 101, in initialize\n","    self.eri3c = get_eri3c(mol, auxmol, self.op)\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/projector/pyscf.py\", line 31, in get_eri3c\n","    eri3c = pmol.intor('int3c1e_sph', shls_slice=(0, mol.nbas, 0, mol.nbas, mol.nbas, mol.nbas + auxmol.nbas))\n","  File \"/usr/local/lib/python3.7/dist-packages/pyscf/gto/mole.py\", line 3269, in intor\n","    shls_slice, comp, hermi, aosym, out=out)\n","  File \"/usr/local/lib/python3.7/dist-packages/pyscf/gto/moleintor.py\", line 237, in getints\n","    aosym, ao_loc, cintopt, out)\n","  File \"/usr/local/lib/python3.7/dist-packages/pyscf/gto/moleintor.py\", line 549, in getints3c\n","    env.ctypes.data_as(ctypes.c_void_p))\n","KeyboardInterrupt\n","^C\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MjnLCRpvVATK","executionInfo":{"status":"ok","timestamp":1625836910211,"user_tz":-60,"elapsed":2263,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"a5648d91-c9a1-4698-8b3d-4187c0fef172"},"source":["!neuralxc engine basis_sgdml_benzene.json workdir/testing.traj --workdir ./tmp"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/usr/local/bin/neuralxc\", line 7, in <module>\n","    exec(compile(f.read(), __file__, 'exec'))\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/bin/neuralxc\", line 240, in <module>\n","    func(**args_dict)\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/drivers/other.py\", line 97, in run_engine_driver\n","    pre = make_nested_absolute(ConfigFile(preprocessor))\n","  File \"/content/drive/MyDrive/MSc_Project/neural_xc_playground/neuralxc/neuralxc/utils/config.py\", line 151, in __init__\n","    content = json.loads(open(content, 'r').read())\n","FileNotFoundError: [Errno 2] No such file or directory: 'basis_sgdml_benzene.json'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CjZX2qB7-mSW"},"source":["## Task: get projection out"]},{"cell_type":"markdown","metadata":{"id":"IxIJKo55-rcJ"},"source":["### `neural xc pre ...` seems to do the job"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6aKa0V3GeWHq","executionInfo":{"status":"ok","timestamp":1624405750491,"user_tz":-60,"elapsed":9777,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"fcfef875-46dc-4e34-fcfe-8163581e072d"},"source":["!neuralxc pre basis_sgdml_benzene.json --srcdir ./tmp --dest ./tmp --xyz workdir/testing.traj"],"execution_count":null,"outputs":[{"output_type":"stream","text":["======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","./tmp/0/pyscf.chkpt\n","./tmp/1/pyscf.chkpt\n","./tmp/2/pyscf.chkpt\n","./tmp/3/pyscf.chkpt\n","./tmp/4/pyscf.chkpt\n","./tmp/5/pyscf.chkpt\n","./tmp/6/pyscf.chkpt\n","./tmp/7/pyscf.chkpt\n","./tmp/8/pyscf.chkpt\n","./tmp/9/pyscf.chkpt\n","./tmp/10/pyscf.chkpt\n","./tmp/11/pyscf.chkpt\n","./tmp/12/pyscf.chkpt\n","./tmp/13/pyscf.chkpt\n","./tmp/14/pyscf.chkpt\n","./tmp/15/pyscf.chkpt\n","./tmp/16/pyscf.chkpt\n","./tmp/17/pyscf.chkpt\n","./tmp/18/pyscf.chkpt\n","./tmp/19/pyscf.chkpt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mA-ULovFBkH7"},"source":["test_pre = np.load(\".tmp/540fffd922b6f0cbee81e4c1c80c7dfa.npy\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oMs4hRCDQFGl","executionInfo":{"status":"ok","timestamp":1624458178995,"user_tz":-60,"elapsed":279,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"36b075ea-42b6-4ea2-c0d2-45a342c61f00"},"source":["test_pre"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1.75093776e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","        -1.06849315e-04,  2.35582319e-03, -5.02244948e-02],\n","       [ 1.75063416e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","        -1.12710913e-02, -3.75379073e-02, -3.70123750e-02],\n","       [ 1.75087629e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","         3.32885384e-02,  8.95267630e-02,  5.28164416e-02],\n","       ...,\n","       [ 1.75051993e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","        -3.81710922e-02,  3.76717721e-02,  2.99026083e-02],\n","       [ 1.75092086e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","        -6.50714538e-02, -7.12436218e-03,  4.84704325e-02],\n","       [ 1.75051101e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","        -7.98489089e-02, -9.68339061e-02, -4.01309898e-02]])"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EyyPAaI7CD-5","executionInfo":{"status":"ok","timestamp":1624404156301,"user_tz":-60,"elapsed":380,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"36862813-8405-4b22-e47d-bd46181e7741"},"source":["test_pre.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(20, 1262)"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"4mhEHUGw-0-u"},"source":["#### TODO: figure out why 1262?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KtMr9aJZCbsK","executionInfo":{"status":"ok","timestamp":1624405414720,"user_tz":-60,"elapsed":260,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"89a15a95-a2de-4141-c56d-941a3705f4e0"},"source":["(1+4+9+16+25+25+25+25+25+25)*3 + (1+4+9+16)*8"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["780"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"markdown","metadata":{"id":"yatjk2zsCPqV"},"source":["propane C3H8 "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TM_rIalVCG5I","executionInfo":{"status":"ok","timestamp":1624404168844,"user_tz":-60,"elapsed":261,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"e40e8aa2-c0a7-4286-8f83-cd2bc5a39154"},"source":["test_pre[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 1.75093776e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","       -1.06849315e-04,  2.35582319e-03, -5.02244948e-02])"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"VWxhFwmI_Awv"},"source":["### import drivers from neuralxc.drivers\n","`pre_driver()` function does the same job as `nerualxc pre ...`"]},{"cell_type":"code","metadata":{"id":"biI5uUyUKdKI"},"source":["from neuralxc.drivers.other import pre_driver"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"No4h3jhFKze8","executionInfo":{"status":"ok","timestamp":1624406830483,"user_tz":-60,"elapsed":8111,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"468f77ec-e6bb-4032-98f4-82a8c895b943"},"source":["pre_driver(\"examples/example_scripts/train_model_20_propane/workdir/testing.traj\",\n","           \"examples/example_scripts/train_model_20_propane/tmp\",\n","           \"examples/example_scripts/train_model_20_propane/basis_sgdml_benzene.json\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n","examples/example_scripts/train_model_20_propane/tmp/0/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/1/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/2/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/3/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/4/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/5/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/6/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/7/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/8/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/9/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/10/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/11/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/12/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/13/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/14/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/15/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/16/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/17/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/18/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/19/pyscf.chkpt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZpSdG0GILXvW"},"source":["test_pre2 = np.load(\".tmp/540fffd922b6f0cbee81e4c1c80c7dfa.npy\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SewRHKSVMmG5","executionInfo":{"status":"ok","timestamp":1624407103461,"user_tz":-60,"elapsed":289,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"5d8bc7d0-d14c-45a3-d1d4-c8de3a425ba0"},"source":["np.array_equal(test_pre,test_pre2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":85}]},{"cell_type":"markdown","metadata":{"id":"ICDO3V3vBPU3"},"source":["### tried to change n and l \n","though it doesn't seem to work. After ruuning `pre_driver()`, \"pre.json\" get overwritten"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-dwlN9UHN3cU","executionInfo":{"status":"ok","timestamp":1624407795800,"user_tz":-60,"elapsed":8858,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"d0197ac0-cd65-4c44-c1b6-47b9a8744968"},"source":["#changed n=4,l=4 for C\n","pre_driver(\"examples/example_scripts/train_model_20_propane/workdir/testing.traj\",\n","           \"examples/example_scripts/train_model_20_propane/tmp\",\n","           \"../pre.json\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 4, 'l': 4}, 'H': {'n': 4, 'l': 4}}\n","examples/example_scripts/train_model_20_propane/tmp/0/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/1/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/2/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/3/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/4/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/5/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/6/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/7/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/8/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/9/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/10/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/11/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/12/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/13/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/14/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/15/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/16/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/17/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/18/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/19/pyscf.chkpt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ci5mrXVpMmJZ"},"source":["test_pre3 = np.load(\".tmp/540fffd922b6f0cbee81e4c1c80c7dfa.npy\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B6sqGAdRMmMA","executionInfo":{"status":"ok","timestamp":1624407812789,"user_tz":-60,"elapsed":4,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"8824c281-a9b8-4af9-eadb-1c8ba2d28462"},"source":["np.array_equal(test_pre,test_pre3)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"kneW23z1QuXc","executionInfo":{"status":"ok","timestamp":1624407994741,"user_tz":-60,"elapsed":273,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"ede31a85-45c6-4d70-e41b-e452a834532c"},"source":["pwd"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/MSc_Project/neural_xc_playground/neuralxc'"]},"metadata":{"tags":[]},"execution_count":101}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Bhla_pIPQP0","executionInfo":{"status":"ok","timestamp":1624408031890,"user_tz":-60,"elapsed":8744,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"443f3fed-ef15-41a0-8079-0e0b313ffc7a"},"source":["#changed n=1,l=1 for C and H\n","pre_driver(\"examples/example_scripts/train_model_20_propane/workdir/testing.traj\",\n","           \"examples/example_scripts/train_model_20_propane/tmp\",\n","           \"../pre.json\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["======Projecting onto basis sets======\n","BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 1, 'l': 1}, 'H': {'n': 1, 'l': 1}}\n","examples/example_scripts/train_model_20_propane/tmp/0/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/1/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/2/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/3/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/4/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/5/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/6/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/7/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/8/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/9/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/10/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/11/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/12/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/13/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/14/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/15/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/16/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/17/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/18/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/19/pyscf.chkpt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hwCMkIwbQ5ZS"},"source":["test_pre4 = np.load(\".tmp/540fffd922b6f0cbee81e4c1c80c7dfa.npy\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e_WKMgOdRARq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624408075987,"user_tz":-60,"elapsed":316,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"665d7bed-0808-43c1-e9f1-d43ea184619c"},"source":["np.array_equal(test_pre,test_pre3)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":104}]},{"cell_type":"markdown","metadata":{"id":"psyDamXdBifo"},"source":["#### go through `pre_driver()` line by line"]},{"cell_type":"code","metadata":{"id":"VAyuxQFDBfsf"},"source":["preprocessor_path = \"../pre.json\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y0g9LVDJCNne"},"source":["`make_nested_absolute()` is in neuralxc.formatter"]},{"cell_type":"code","metadata":{"id":"uVvIbyu8CJkP"},"source":["from neuralxc.formatter import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G8l_r7suCVZp"},"source":["import json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cRn9GtdGBfu_"},"source":["pre = make_nested_absolute(json.loads(open(\"../pre.json\", 'r').read()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PHpxd1QyBfxb","executionInfo":{"status":"ok","timestamp":1624454627565,"user_tz":-60,"elapsed":310,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"5b38f7f3-2dea-49bb-a76e-a8cc0f9dd3f7"},"source":["pre"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'engine_kwargs': {'basis': 'ccpvdz', 'xc': 'PBE'},\n"," 'n_workers': 1,\n"," 'preprocessor': {'C': {'l': 5, 'n': 10},\n","  'H': {'l': 4, 'n': 4},\n","  'application': 'pyscf',\n","  'basis': 'ccpvtz-jkfit',\n","  'extension': 'chkpt',\n","  'spec_agnostic': False}}"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"2C1FC2ApC3uA"},"source":["from ase.io import read"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UIGaa3lwBfzw"},"source":["atoms = read(\"examples/example_scripts/train_model_20_propane/workdir/testing.traj\", ':')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y8Tv3wSqBf2S","executionInfo":{"status":"ok","timestamp":1624454773387,"user_tz":-60,"elapsed":302,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"5b4e12fc-2959-417b-b251-6ad5d726e68f"},"source":["atoms"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...)),\n"," Atoms(symbols='C2H5CH3', pbc=False, calculator=SinglePointCalculator(...))]"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xz2G3NDMBf5I","executionInfo":{"status":"ok","timestamp":1624454763874,"user_tz":-60,"elapsed":290,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"dd3bb128-e033-45f7-82f8-697a99d40827"},"source":["atoms[0].positions"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1.25991742, -1.19954342,  1.41354126],\n","       [ 2.13037387, -0.57736755,  0.21461351],\n","       [ 0.86798119, -0.41870608,  2.04738686],\n","       [ 1.74387902, -1.90588774,  1.96012759],\n","       [ 0.3011863 , -1.61970725,  1.05809014],\n","       [ 2.90968884,  0.0927169 ,  0.59594325],\n","       [ 1.47191602,  0.10321607, -0.42808757],\n","       [ 2.72913521, -1.59525051, -0.75344542],\n","       [ 3.54174583, -2.16964612, -0.17017267],\n","       [ 3.23082184, -1.04979412, -1.52984522],\n","       [ 2.00499249, -2.2991183 , -1.25418812]])"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"ekhGuznZC_Om"},"source":["from neuralxc.ml.utils import get_preprocessor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"grxYNj35C_Q8"},"source":["preprocessor = get_preprocessor(pre, atoms, \"examples/example_scripts/train_model_20_propane/tmp\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WC_Wnjp0C_Ta","executionInfo":{"status":"ok","timestamp":1624454898014,"user_tz":-60,"elapsed":311,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"fb858789-7c15-4158-8e03-19fb4ad9bfbc"},"source":["preprocessor.basis_instructions"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'C': {'l': 5, 'n': 10},\n"," 'H': {'l': 4, 'n': 4},\n"," 'application': 'pyscf',\n"," 'basis': 'ccpvtz-jkfit',\n"," 'extension': 'chkpt',\n"," 'spec_agnostic': False}"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"zIF0rOoIC_Vo"},"source":["from neuralxc.ml.utils import get_basis_grid"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2JTYh9NaC_YN"},"source":["basis_grid = get_basis_grid(pre)['preprocessor__basis_instructions']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YtFTUkj4EE7f"},"source":["???  basis_grid is pretty much the same as preprocessor.basis_instructions "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QsjRnH_rC_am","executionInfo":{"status":"ok","timestamp":1624454985843,"user_tz":-60,"elapsed":4,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"f05d7240-880c-4bbf-fabf-eff77486ccfb"},"source":["basis_grid"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'C': {'l': 5, 'n': 10},\n","  'H': {'l': 4, 'n': 4},\n","  'application': 'pyscf',\n","  'basis': 'ccpvtz-jkfit',\n","  'extension': 'chkpt',\n","  'spec_agnostic': False}]"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"w_ElQhFwC_c0"},"source":["basis_instr = basis_grid[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pyHiwCzuE-_K"},"source":["preprocessor.basis_instructions = basis_instr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wYWqBWZME_Bs","executionInfo":{"status":"ok","timestamp":1624455334666,"user_tz":-60,"elapsed":307,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"b0c07178-83c6-4f1d-fd5f-35ce69a061f7"},"source":[" print('BI', basis_instr)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["BI {'basis': 'ccpvtz-jkfit', 'extension': 'chkpt', 'application': 'pyscf', 'spec_agnostic': False, 'C': {'n': 10, 'l': 5}, 'H': {'n': 4, 'l': 4}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"5GxFzpPVFSKq","executionInfo":{"status":"ok","timestamp":1624455356577,"user_tz":-60,"elapsed":291,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"07b3793f-d628-4689-f176-268ef4fd47e1"},"source":["basis_instr.get('application', 'siesta')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'pyscf'"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"eMOjDIwSFwfR"},"source":["from neuralxc.drivers.other import get_real_basis"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jizFN6NAFSNB"},"source":["real_basis = get_real_basis(atoms,basis_instr['basis'],spec_agnostic=basis_instr.get('spec_agnostic', False))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FgvCJEfKFSS_","executionInfo":{"status":"ok","timestamp":1624455502966,"user_tz":-60,"elapsed":403,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"49d7f566-d0ee-4ca9-ca27-66478549e8d4"},"source":["real_basis"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'C': {'l': 5, 'n': 10}, 'H': {'l': 4, 'n': 4}}"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3_r03NiHE_EJ","executionInfo":{"status":"ok","timestamp":1624455572144,"user_tz":-60,"elapsed":357,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"a15ad882-04f5-4b4c-e0fb-f7cffd4a6177"},"source":["real_basis.keys()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['C', 'H'])"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"markdown","metadata":{"id":"Wx8mpoBEGa93"},"source":["#### The line below overwrites `basis_instr` whic is read from user-provided input"]},{"cell_type":"code","metadata":{"id":"4957PFl2GPEd"},"source":["for key in real_basis:\n","    basis_instr[key] = real_basis[key]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tkjSe2jKGrWi"},"source":["pre.update({'preprocessor': basis_instr})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TEGze91RGrY5","executionInfo":{"status":"ok","timestamp":1624455745960,"user_tz":-60,"elapsed":262,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"213566a8-e99c-4b84-fc9f-1b7637152113"},"source":["pre"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'engine_kwargs': {'basis': 'ccpvdz', 'xc': 'PBE'},\n"," 'n_workers': 1,\n"," 'preprocessor': {'C': {'l': 5, 'n': 10},\n","  'H': {'l': 4, 'n': 4},\n","  'application': 'pyscf',\n","  'basis': 'ccpvtz-jkfit',\n","  'extension': 'chkpt',\n","  'spec_agnostic': False}}"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"markdown","metadata":{"id":"7TKBkpBEHTMA"},"source":["#### `preprocessor.fit_transform(None)` seems to be the method that we need to figure out next!!!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"klZuCaOMGra4","executionInfo":{"status":"ok","timestamp":1624455800787,"user_tz":-60,"elapsed":16882,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"daa8a7bf-e3d3-4225-8819-97350b88e642"},"source":["data = preprocessor.fit_transform(None)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["examples/example_scripts/train_model_20_propane/tmp/0/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/1/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/2/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/3/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/4/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/5/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/6/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/7/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/8/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/9/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/10/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/11/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/12/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/13/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/14/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/15/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/16/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/17/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/18/pyscf.chkpt\n","examples/example_scripts/train_model_20_propane/tmp/19/pyscf.chkpt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"46Jw0dIuHPqq","executionInfo":{"status":"ok","timestamp":1624455842442,"user_tz":-60,"elapsed":260,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"7d0e6177-1570-4187-8caa-89cceb4579e9"},"source":["data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1.75093776e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","        -1.06849315e-04,  2.35582319e-03, -5.02244948e-02],\n","       [ 1.75063416e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","        -1.12710913e-02, -3.75379073e-02, -3.70123750e-02],\n","       [ 1.75087629e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","         3.32885384e-02,  8.95267630e-02,  5.28164416e-02],\n","       ...,\n","       [ 1.75051993e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","        -3.81710922e-02,  3.76717721e-02,  2.99026083e-02],\n","       [ 1.75092086e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","        -6.50714538e-02, -7.12436218e-03,  4.84704325e-02],\n","       [ 1.75051101e+00,  0.00000000e+00,  0.00000000e+00, ...,\n","        -7.98489089e-02, -9.68339061e-02, -4.01309898e-02]])"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6sq8n1kuGPG-","executionInfo":{"status":"ok","timestamp":1624455833777,"user_tz":-60,"elapsed":270,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"221ee7f2-ab5d-4c13-cb8e-a83e96433c89"},"source":["data.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(20, 1262)"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"markdown","metadata":{"id":"s1DowF_zKnK5"},"source":["Investiaging `preprocessor` object leads to:"]},{"cell_type":"markdown","metadata":{"id":"MC_1T77QKOYK"},"source":["`get_preprocessor()` in `neuralxc/ml/utils.py`"]},{"cell_type":"markdown","metadata":{"id":"ZQ5KjAxiKrnO"},"source":["which then leads to "]},{"cell_type":"markdown","metadata":{"id":"v3Fv-sbXKYkB"},"source":["`Preprocessor` class in `neuralxc/preprocessor/preprocessor.py`"]},{"cell_type":"markdown","metadata":{"id":"OyNDVH4OKxMC"},"source":["which leads to\n"]},{"cell_type":"markdown","metadata":{"id":"qfuCC8_hKzak"},"source":["`DensityProjector()` in `neuralxc/projector/projector.py`"]},{"cell_type":"code","metadata":{"id":"8C7m4jnvKN5o"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BepKIudVKN8D"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tcFivjIWKN-v"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rH_TtnlBKOBK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PSEgqJXkGPJD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"se-6TNBTGPLU"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YVztqbgQGPNt"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LcZ4sas99ggj"},"source":["import h5py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XSLWLGaZ-ivf"},"source":["f = h5py.File(\"workdir/testing/data.hdf5\", \"r\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RWxNboTz-1vE","executionInfo":{"status":"ok","timestamp":1624403515514,"user_tz":-60,"elapsed":264,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"7e80dd46-1075-4ddd-8b4e-424aefe94971"},"source":["f.keys()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<KeysViewHDF5 ['system']>"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iVs_w1yC-2oV","executionInfo":{"status":"ok","timestamp":1624403537259,"user_tz":-60,"elapsed":266,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"0a5806ec-c14b-4b09-f374-aeda97ef3c12"},"source":["list(f['system'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['it0', 'ref', 'testing']"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4DICrl6qAeGG","executionInfo":{"status":"ok","timestamp":1624403743079,"user_tz":-60,"elapsed":262,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"c56e5899-e00b-4664-fd01-97ce53c1b943"},"source":["list(f['system']['it0']['density'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['540fffd922b6f0cbee81e4c1c80c7dfa']"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E4-JGMJm_5Ba","executionInfo":{"status":"ok","timestamp":1624403586498,"user_tz":-60,"elapsed":268,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"ecba242d-bf74-4984-e0ef-2cdcd167ebd3"},"source":["list(f['system']['it0']['energy'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.10226966514346714,\n"," 0.039626753764423484,\n"," 0.15792664630362196,\n"," -0.20108776160122943,\n"," 0.021907554097651882,\n"," -0.018725369080584642,\n"," -0.051604822480840085,\n"," 0.03901253773119606,\n"," -0.08694773404067746,\n"," 0.10049447554138169,\n"," -0.04182985434999864,\n"," -0.16976948681258364,\n"," 0.21459703824666576,\n"," -0.07165824658704878,\n"," -0.16000632735449472,\n"," 0.050760118499965756,\n"," 0.1796901350612643,\n"," -0.09989474409803734,\n"," 0.050183174743324344,\n"," -0.05494375272974139]"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cAxiKij2_vjn","executionInfo":{"status":"ok","timestamp":1624403565527,"user_tz":-60,"elapsed":310,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"07a70b36-967f-4d61-ae48-91d1a057c798"},"source":["list(f['system']['ref']['energy'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.10550690699938059,\n"," 0.006440664167257637,\n"," 0.14910719306271858,\n"," -0.17694527238290902,\n"," 0.025851356377643242,\n"," -0.0311490537451391,\n"," -0.016132461960751243,\n"," 0.018089817258442054,\n"," -0.1169276492832978,\n"," 0.11217311358223014,\n"," -0.011145042170483066,\n"," -0.18532556012951318,\n"," 0.21620274688575591,\n"," -0.06451687403523465,\n"," -0.17053774719624926,\n"," 0.04993273920445063,\n"," 0.2065744226656534,\n"," -0.0954033824173166,\n"," 0.041813624193309806,\n"," -0.06360954108640726]"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H2Sj9XDM-4B8","executionInfo":{"status":"ok","timestamp":1624403622853,"user_tz":-60,"elapsed":355,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"c205c896-a829-479e-96ea-1bc800ae63eb"},"source":["list(f['system']['testing']['nxc']['energy'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.10226966514346714,\n"," 0.039626753764423484,\n"," 0.15792664630362196,\n"," -0.20108776160122943,\n"," 0.021907554097651882,\n"," -0.018725369080584642,\n"," -0.051604822480840085,\n"," 0.03901253773119606,\n"," -0.08694773404067746,\n"," 0.10049447554138169,\n"," -0.04182985434999864,\n"," -0.16976948681258364,\n"," 0.21459703824666576,\n"," -0.07165824658704878,\n"," -0.16000632735449472,\n"," 0.050760118499965756,\n"," 0.1796901350612643,\n"," -0.09989474409803734,\n"," 0.050183174743324344,\n"," -0.05494375272974139]"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UafMhu4H_ZxX","executionInfo":{"status":"ok","timestamp":1624403693120,"user_tz":-60,"elapsed":411,"user":{"displayName":"cen Ellen","photoUrl":"","userId":"14213049525492468648"}},"outputId":"c815038a-8f23-4583-b589-f22e424375d7"},"source":["list(f['system']['testing']['ref']['energy'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.10550690699938059,\n"," 0.006440664167257637,\n"," 0.14910719306271858,\n"," -0.17694527238290902,\n"," 0.025851356377643242,\n"," -0.0311490537451391,\n"," -0.016132461960751243,\n"," 0.018089817258442054,\n"," -0.1169276492832978,\n"," 0.11217311358223014,\n"," -0.011145042170483066,\n"," -0.18532556012951318,\n"," 0.21620274688575591,\n"," -0.06451687403523465,\n"," -0.17053774719624926,\n"," 0.04993273920445063,\n"," 0.2065744226656534,\n"," -0.0954033824173166,\n"," 0.041813624193309806,\n"," -0.06360954108640726]"]},"metadata":{"tags":[]},"execution_count":41}]}]}